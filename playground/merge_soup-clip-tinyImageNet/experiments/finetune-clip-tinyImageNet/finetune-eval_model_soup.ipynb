{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook takes a pre-trained CLIP model, fine-tune on tiny imagenet. Fine-tunes uses 5 hyper-parameters, and then evaluate model soup performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vLaqZVWVR4tE"
   },
   "source": [
    "## Setup Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 18,
     "status": "ok",
     "timestamp": 1756051719474,
     "user": {
      "displayName": "Yang Zhou",
      "userId": "09717432835810465973"
     },
     "user_tz": 240
    },
    "id": "NxzIeVi39G-1"
   },
   "outputs": [],
   "source": [
    "LOCAL = False\n",
    "\n",
    "# if run locally:\n",
    "if LOCAL:\n",
    "    DATA_DIR = \"../dataset\"\n",
    "    CODE_DIR = \"../\"\n",
    "# on Colab\n",
    "else:\n",
    "    DATA_DIR = \"/content\"\n",
    "    CODE_DIR = \"./clip_TinyImageNet\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 39,
     "status": "ok",
     "timestamp": 1756051719516,
     "user": {
      "displayName": "Yang Zhou",
      "userId": "09717432835810465973"
     },
     "user_tz": 240
    },
    "id": "z5vVaNV99G-1"
   },
   "outputs": [],
   "source": [
    "import os, sys\n",
    "ROOT = os.path.abspath(CODE_DIR)\n",
    "if ROOT not in sys.path:\n",
    "    sys.path.insert(0, ROOT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HlsOOf4r9G-2"
   },
   "source": [
    "If you want to use Claude Code, uncomment the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1756051719529,
     "user": {
      "displayName": "Yang Zhou",
      "userId": "09717432835810465973"
     },
     "user_tz": 240
    },
    "id": "QnjsVSoia2CU"
   },
   "outputs": [],
   "source": [
    "# !npm install -g @anthropic-ai/claude-code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5O1BqBcz9G-2"
   },
   "source": [
    "If use Colab, you need to save output results to google drive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 19655,
     "status": "ok",
     "timestamp": 1756051739176,
     "user": {
      "displayName": "Yang Zhou",
      "userId": "09717432835810465973"
     },
     "user_tz": 240
    },
    "id": "-GlMz92MR4tG",
    "outputId": "bd24bf3e-9aa9-40dc-a620-ee017df8a62f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "storage_dir = \"drive/MyDrive/Colab Notebooks/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "orDTysBy9G-3"
   },
   "source": [
    "We will work under the same dir as this notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K23WdGmnfvFA"
   },
   "source": [
    "To copy the code to fine-tune clip on tinyImageNet, run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1542,
     "status": "ok",
     "timestamp": 1756051740719,
     "user": {
      "displayName": "Yang Zhou",
      "userId": "09717432835810465973"
     },
     "user_tz": 240
    },
    "id": "nTJAmIyofRi9",
    "outputId": "051dd08f-fb95-4c12-8308-3558216f8568"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'clip_TinyImageNet'...\n",
      "remote: Enumerating objects: 83, done.\u001b[K\n",
      "remote: Counting objects: 100% (83/83), done.\u001b[K\n",
      "remote: Compressing objects: 100% (45/45), done.\u001b[K\n",
      "remote: Total 83 (delta 42), reused 79 (delta 38), pack-reused 0 (from 0)\u001b[K\n",
      "Receiving objects: 100% (83/83), 2.48 MiB | 8.21 MiB/s, done.\n",
      "Resolving deltas: 100% (42/42), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/nbzy1995/clip_TinyImageNet.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AX1HKoALfFXS"
   },
   "source": [
    "To download tiny imagenet dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 100958,
     "status": "ok",
     "timestamp": 1756051841679,
     "user": {
      "displayName": "Yang Zhou",
      "userId": "09717432835810465973"
     },
     "user_tz": 240
    },
    "id": "3uz63y5IdKYb"
   },
   "outputs": [],
   "source": [
    "!wget -q http://cs231n.stanford.edu/tiny-imagenet-200.zip\n",
    "!unzip -q tiny-imagenet-200.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eRvGCTwr9G-3"
   },
   "source": [
    "Now we created a directory called \"tiny-imagenet-200\" containing the dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c3HW7EYqfgA8"
   },
   "source": [
    "We now copy pre-computed index for the train/ folder, 90% for training, 10% for validation. The val/ folder will be used as test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 33,
     "status": "ok",
     "timestamp": 1756051841715,
     "user": {
      "displayName": "Yang Zhou",
      "userId": "09717432835810465973"
     },
     "user_tz": 240
    },
    "id": "BOwhtbZZdOFf"
   },
   "outputs": [],
   "source": [
    "# if on Colab\n",
    "!cp $CODE_DIR/dataset/tiny_imagenet_train_val_indices.npy /content/tiny_imagenet_train_val_indices.npy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B8bVSnvX9G-4"
   },
   "source": [
    "Now we install the requirements for fine-tuning clip on tinyImageNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 18766,
     "status": "ok",
     "timestamp": 1756051860490,
     "user": {
      "displayName": "Yang Zhou",
      "userId": "09717432835810465973"
     },
     "user_tz": 240
    },
    "id": "LWGC1rvTR4tH",
    "outputId": "450146f1-9ce5-41cf-e11c-9fec4fd0771c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.8 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m77.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "\u001b[33m  DEPRECATION: Building 'clip' using the legacy setup.py bdist_wheel mechanism, which will be removed in a future version. pip 25.3 will enforce this behaviour change. A possible replacement is to use the standardized build interface by setting the `--use-pep517` option, (possibly combined with `--no-build-isolation`), or adding a `pyproject.toml` file to the source tree of 'clip'. Discussion can be found at https://github.com/pypa/pip/issues/6334\u001b[0m\u001b[33m\n",
      "\u001b[0m  Building wheel for clip (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "✅ Core packages installed!\n"
     ]
    }
   ],
   "source": [
    "!pip install --quiet --upgrade pip\n",
    "!pip install -q -r clip_TinyImageNet/requirements.txt\n",
    "print(\"✅ Core packages installed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5308,
     "status": "ok",
     "timestamp": 1756051865800,
     "user": {
      "displayName": "Yang Zhou",
      "userId": "09717432835810465973"
     },
     "user_tz": 240
    },
    "id": "oe6uZ6LzR4tH",
    "outputId": "63ef4191-13f6-4153-ed4c-cb7168fb7f86"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 System Information:\n",
      "Python version: Python 3.12.11\n",
      "PyTorch version: 2.8.0+cu126\n",
      "CUDA available: True\n",
      "GPU device: Tesla T4\n",
      "GPU memory: 15.8 GB\n",
      "CUDA version: 12.6\n"
     ]
    }
   ],
   "source": [
    "# Check GPU availability and system info\n",
    "import torch\n",
    "import subprocess\n",
    "\n",
    "print(\"🔍 System Information:\")\n",
    "print(f\"Python version: {subprocess.check_output(['python', '--version']).decode().strip()}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU device: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "    print(f\"CUDA version: {torch.version.cuda}\")\n",
    "else:\n",
    "    print(\"❌ No GPU available! Please enable GPU runtime in Colab.\")\n",
    "    print(\"Runtime > Change runtime type > Hardware accelerator > GPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1756051865820,
     "user": {
      "displayName": "Yang Zhou",
      "userId": "09717432835810465973"
     },
     "user_tz": 240
    },
    "id": "QBl3z7po9G-4"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M0Iv7ZHU9G-4"
   },
   "source": [
    "## TinyImageNet - View\n",
    "\n",
    "**First Time Setup:**\n",
    "If the indices file doesn't exist, run the generation script ONCE:\n",
    "```bash\n",
    "cd dataset\n",
    "python generate_train_val_indices.py\n",
    "```\n",
    "\n",
    "⚠️ **Do NOT run the generation script multiple times unless you want to change the split!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5541,
     "status": "ok",
     "timestamp": 1756051871368,
     "user": {
      "displayName": "Yang Zhou",
      "userId": "09717432835810465973"
     },
     "user_tz": 240
    },
    "id": "kRA9wRK19G-4",
    "outputId": "acb84de7-f35d-478a-b103-ef14dace02d1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Checking for persistent indices...\n",
      "✅ Found persistent indices: /content/tiny_imagenet_train_val_indices.npy\n"
     ]
    }
   ],
   "source": [
    "# Check if persistent indices exist, generate if needed\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "\n",
    "sys.path.append(CODE_DIR)\n",
    "\n",
    "INDICES_FILE = os.path.join(DATA_DIR, 'tiny_imagenet_train_val_indices.npy')\n",
    "\n",
    "print(\"🔍 Checking for persistent indices...\")\n",
    "\n",
    "if os.path.exists(INDICES_FILE):\n",
    "    print(f\"✅ Found persistent indices: {INDICES_FILE}\")\n",
    "else:\n",
    "    print(f\"❌ Persistent indices not found: {INDICES_FILE}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6934,
     "status": "ok",
     "timestamp": 1756051878301,
     "user": {
      "displayName": "Yang Zhou",
      "userId": "09717432835810465973"
     },
     "user_tz": 240
    },
    "id": "F2F7XOI59G-4",
    "outputId": "bb88f2c1-8636-4844-ba7b-1958f21250d3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating datasets...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████| 338M/338M [00:01<00:00, 262MiB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Datasets created:\n",
      "   Train split: 90000 samples\n",
      "   Val split: 10000 samples\n",
      "   Test split: 10000 samples\n"
     ]
    }
   ],
   "source": [
    "print(\"Creating datasets...\")\n",
    "\n",
    "from dataset.tiny_imagenet import TinyImageNet\n",
    "\n",
    "# Use CLIP's expected preprocessing for proper model evaluation\n",
    "import clip\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "_, clip_preprocess = clip.load('ViT-B/32', device, jit=False)\n",
    "\n",
    "data_tinyImageNet = TinyImageNet(\n",
    "    train_preprocess=clip_preprocess,  # Use CLIP preprocessing for training\n",
    "    eval_preprocess=clip_preprocess,   # Use CLIP preprocessing for evaluation\n",
    "    location=DATA_DIR,\n",
    "    batch_size=8,\n",
    "    num_workers=2,\n",
    "    distributed=False,\n",
    ")\n",
    "\n",
    "print(f\"✅ Datasets created:\")\n",
    "print(f\"   Train split: {len(data_tinyImageNet.train_sampler)} samples\") # Train split (90% of original train data)\n",
    "print(f\"   Val split: {len(data_tinyImageNet.val_sampler)} samples\")\n",
    "print(f\"   Test split: {len(data_tinyImageNet.test_loader.dataset)} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1756051878311,
     "user": {
      "displayName": "Yang Zhou",
      "userId": "09717432835810465973"
     },
     "user_tz": 240
    },
    "id": "qde30oyI9G-4"
   },
   "outputs": [],
   "source": [
    "from itertools import islice\n",
    "\n",
    "def show_split_samples(data, split_name, batch_index=0):\n",
    "    \"\"\"Show sample images with labels, indices, and paths\"\"\"\n",
    "\n",
    "    if split_name == \"train\":\n",
    "        loader = data.train_loader\n",
    "    elif split_name == \"validation\":\n",
    "        loader = data.val_loader\n",
    "    elif split_name == \"test\":\n",
    "        loader = data.test_loader\n",
    "\n",
    "    # get one batch\n",
    "    batch = next(islice(loader, batch_index, None))\n",
    "    images = batch['images']\n",
    "    labels = batch['labels']\n",
    "    paths = batch['image_paths']\n",
    "    print(f\"fetching {len(labels)} samples\")\n",
    "\n",
    "    idx_to_class = data.idx_to_class\n",
    "\n",
    "    # Create subplot\n",
    "    fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    for i in range(len(images)):\n",
    "        # Display image (denormalize from CLIP preprocessing)\n",
    "        img = images[i]\n",
    "        # CLIP normalization: mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711]\n",
    "        mean = torch.tensor([0.48145466, 0.4578275, 0.40821073]).view(3, 1, 1)\n",
    "        std = torch.tensor([0.26862954, 0.26130258, 0.27577711]).view(3, 1, 1)\n",
    "        img = img * std + mean  # denormalize\n",
    "        img = torch.clamp(img, 0, 1)  # clamp to [0,1]\n",
    "        img = img.permute(1, 2, 0)\n",
    "\n",
    "        axes[i].imshow(img)\n",
    "        axes[i].axis('off')\n",
    "\n",
    "        # Get label info\n",
    "        label_idx = int(labels[i])\n",
    "        class_id = idx_to_class[label_idx]\n",
    "        class_word = data.wordnet_map[class_id]\n",
    "\n",
    "        # Show title with index and class\n",
    "        axes[i].set_title(f\"Label: {label_idx} | {class_id} | {class_word[:30]} \\n Path: {os.path.basename(paths[i])}\", fontsize=9)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.suptitle(f'{split_name.upper()} Split - Images with Labels', fontsize=14, y=1.02)\n",
    "    plt.show()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "81IreCVU9G-4"
   },
   "source": [
    "Uncomment following cells to visualiza samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "executionInfo": {
     "elapsed": 87,
     "status": "ok",
     "timestamp": 1756051878399,
     "user": {
      "displayName": "Yang Zhou",
      "userId": "09717432835810465973"
     },
     "user_tz": 240
    },
    "id": "SZ-xK6ZG9G-5"
   },
   "outputs": [],
   "source": [
    "# show_split_samples(data_tinyImageNet, \"train\",0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1756051878401,
     "user": {
      "displayName": "Yang Zhou",
      "userId": "09717432835810465973"
     },
     "user_tz": 240
    },
    "id": "9z5qcNJR9G-5"
   },
   "outputs": [],
   "source": [
    "# show_split_samples(data_tinyImageNet, \"validation\",6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1756051878401,
     "user": {
      "displayName": "Yang Zhou",
      "userId": "09717432835810465973"
     },
     "user_tz": 240
    },
    "id": "7q4_rQ009G-5"
   },
   "outputs": [],
   "source": [
    "# show_split_samples(data_tinyImageNet, \"test\",0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1756051878402,
     "user": {
      "displayName": "Yang Zhou",
      "userId": "09717432835810465973"
     },
     "user_tz": 240
    },
    "id": "p9_Vhcng9G-5"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b_-KAJBYR4tH"
   },
   "source": [
    "## Zero-shot classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q9STFxQdR4tH"
   },
   "source": [
    "measure performance of CLIP zero-shot classifier on TinyImageNet dataset. use test split (val/ folder)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 52185,
     "status": "ok",
     "timestamp": 1755919705212,
     "user": {
      "displayName": "Yang Zhou",
      "userId": "09717432835810465973"
     },
     "user_tz": 240
    },
    "id": "mvfXPQGTR4tI",
    "outputId": "d43fe068-68de-4bb4-9319-501511c5fae1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(\n",
      "Building zero-shot classifier.\n",
      "100% 200/200 [00:10<00:00, 18.24it/s]\n",
      "[0% 0/313]\tAcc: 53.12\tData (t) 0.818\tBatch (t) 1.768\n",
      "[6% 20/313]\tAcc: 57.59\tData (t) 0.007\tBatch (t) 0.097\n",
      "[13% 40/313]\tAcc: 59.83\tData (t) 0.007\tBatch (t) 0.096\n",
      "[19% 60/313]\tAcc: 59.84\tData (t) 0.006\tBatch (t) 0.095\n",
      "[26% 80/313]\tAcc: 60.53\tData (t) 0.010\tBatch (t) 0.088\n",
      "[32% 100/313]\tAcc: 60.61\tData (t) 0.009\tBatch (t) 0.087\n",
      "[38% 120/313]\tAcc: 61.13\tData (t) 0.002\tBatch (t) 0.083\n",
      "[45% 140/313]\tAcc: 61.39\tData (t) 0.002\tBatch (t) 0.081\n",
      "[51% 160/313]\tAcc: 61.55\tData (t) 0.012\tBatch (t) 0.087\n",
      "[58% 180/313]\tAcc: 61.64\tData (t) 0.002\tBatch (t) 0.083\n",
      "[64% 200/313]\tAcc: 61.63\tData (t) 0.002\tBatch (t) 0.092\n",
      "[70% 220/313]\tAcc: 61.45\tData (t) 0.163\tBatch (t) 0.238\n",
      "[77% 240/313]\tAcc: 61.51\tData (t) 0.009\tBatch (t) 0.090\n",
      "[83% 260/313]\tAcc: 61.31\tData (t) 0.002\tBatch (t) 0.082\n",
      "[89% 280/313]\tAcc: 61.31\tData (t) 0.002\tBatch (t) 0.084\n",
      "[96% 300/313]\tAcc: 61.29\tData (t) 0.002\tBatch (t) 0.084\n",
      "Accuracy is 61.41.\n"
     ]
    }
   ],
   "source": [
    "!python $CODE_DIR/zeroshot.py --dataset TinyImageNet --batch-size 32 --data-location $DATA_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7mpIX5bR9G-5"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TmkIYFQuR4tI"
   },
   "source": [
    "## Fine-tuning\n",
    "We start with pretrained CLIP ViT-B/32, then fine-tune it on TinyImageNet. The training set is 90% of the train/, and 10% of them are used as validation set. We sweep for different hyperparameters to create 5 diverse models for model soups.\n",
    "\n",
    "### Hyperparameter Configurations:\n",
    "1. **Config 1**: lr=3e-5, wd=0.1, epochs=10, batch_size=256, timm_aug=False\n",
    "2. **Config 2**: lr=1e-5, wd=0.1, epochs=10, batch_size=256, timm_aug=False  \n",
    "3. **Config 3**: lr=3e-6, wd=0.1, epochs=10, batch_size=256, timm_aug=False\n",
    "4. **Config 4**: lr=2e-5, wd=1e-3, epochs=10, batch_size=256, timm_aug=True\n",
    "5. **Config 5**: lr=1e-6, wd=1e-4, epochs=10, batch_size=256, timm_aug=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7996557,
     "status": "ok",
     "timestamp": 1755927931673,
     "user": {
      "displayName": "Yang Zhou",
      "userId": "09717432835810465973"
     },
     "user_tz": 240
    },
    "id": "fmT6QvCNR4tI",
    "outputId": "f1ddc423-6b26-4567-84ca-be3d0868b525"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(\n",
      "Building zero-shot classifier.\n",
      "100% 200/200 [00:10<00:00, 18.22it/s]\n",
      "Saving model to ./config1_0.pt\n",
      "Train Epoch: 0 [0% 0/352]\tLoss: 1.442926\tData (t) 11.733\tBatch (t) 15.286\n",
      "Train Epoch: 0 [6% 20/352]\tLoss: 1.261179\tData (t) 2.040\tBatch (t) 2.307\n",
      "Train Epoch: 0 [11% 40/352]\tLoss: 0.963566\tData (t) 1.796\tBatch (t) 2.055\n",
      "Train Epoch: 0 [17% 60/352]\tLoss: 1.013636\tData (t) 2.045\tBatch (t) 2.076\n",
      "Train Epoch: 0 [23% 80/352]\tLoss: 0.916063\tData (t) 2.007\tBatch (t) 2.073\n",
      "Train Epoch: 0 [28% 100/352]\tLoss: 0.836356\tData (t) 1.816\tBatch (t) 2.080\n",
      "Train Epoch: 0 [34% 120/352]\tLoss: 1.102863\tData (t) 2.034\tBatch (t) 2.066\n",
      "Train Epoch: 0 [40% 140/352]\tLoss: 0.925082\tData (t) 2.032\tBatch (t) 2.065\n",
      "Train Epoch: 0 [45% 160/352]\tLoss: 1.065236\tData (t) 2.039\tBatch (t) 2.070\n",
      "Train Epoch: 0 [51% 180/352]\tLoss: 0.848087\tData (t) 2.037\tBatch (t) 2.069\n",
      "Train Epoch: 0 [57% 200/352]\tLoss: 0.978953\tData (t) 1.815\tBatch (t) 1.847\n",
      "Train Epoch: 0 [62% 220/352]\tLoss: 0.807766\tData (t) 1.821\tBatch (t) 1.855\n",
      "Train Epoch: 0 [68% 240/352]\tLoss: 0.923739\tData (t) 1.823\tBatch (t) 1.855\n",
      "Train Epoch: 0 [74% 260/352]\tLoss: 0.964957\tData (t) 2.036\tBatch (t) 2.068\n",
      "Train Epoch: 0 [80% 280/352]\tLoss: 0.844073\tData (t) 2.034\tBatch (t) 2.065\n",
      "Train Epoch: 0 [85% 300/352]\tLoss: 1.044112\tData (t) 2.032\tBatch (t) 2.064\n",
      "Train Epoch: 0 [91% 320/352]\tLoss: 0.882609\tData (t) 2.031\tBatch (t) 2.291\n",
      "Train Epoch: 0 [97% 340/352]\tLoss: 0.833859\tData (t) 1.805\tBatch (t) 1.836\n",
      "********************************************************************************\n",
      "Starting eval on validation split\n",
      "Val loss: 1.2707   Acc: 76.22: 100% 40/40 [00:38<00:00,  1.05it/s]\n",
      "Val acc at epoch 0: 76.22\n",
      "Saving model to ./config1_1.pt\n",
      "Train Epoch: 1 [0% 0/352]\tLoss: 0.714028\tData (t) 7.058\tBatch (t) 7.929\n",
      "Train Epoch: 1 [6% 20/352]\tLoss: 0.507759\tData (t) 2.060\tBatch (t) 2.092\n",
      "Train Epoch: 1 [11% 40/352]\tLoss: 0.748859\tData (t) 2.018\tBatch (t) 2.050\n",
      "Train Epoch: 1 [17% 60/352]\tLoss: 0.869590\tData (t) 2.047\tBatch (t) 2.199\n",
      "Train Epoch: 1 [23% 80/352]\tLoss: 0.740386\tData (t) 1.976\tBatch (t) 2.008\n",
      "Train Epoch: 1 [28% 100/352]\tLoss: 0.563878\tData (t) 2.015\tBatch (t) 2.056\n",
      "Train Epoch: 1 [34% 120/352]\tLoss: 0.764934\tData (t) 2.021\tBatch (t) 2.076\n",
      "Train Epoch: 1 [40% 140/352]\tLoss: 0.773874\tData (t) 2.034\tBatch (t) 2.077\n",
      "Train Epoch: 1 [45% 160/352]\tLoss: 0.673568\tData (t) 2.036\tBatch (t) 2.079\n",
      "Train Epoch: 1 [51% 180/352]\tLoss: 0.758510\tData (t) 2.034\tBatch (t) 2.100\n",
      "Train Epoch: 1 [57% 200/352]\tLoss: 0.709141\tData (t) 1.928\tBatch (t) 2.142\n",
      "Train Epoch: 1 [62% 220/352]\tLoss: 0.764736\tData (t) 1.831\tBatch (t) 1.863\n",
      "Train Epoch: 1 [68% 240/352]\tLoss: 0.682794\tData (t) 2.034\tBatch (t) 2.066\n",
      "Train Epoch: 1 [74% 260/352]\tLoss: 0.684872\tData (t) 2.039\tBatch (t) 2.072\n",
      "Train Epoch: 1 [80% 280/352]\tLoss: 0.704220\tData (t) 2.038\tBatch (t) 2.069\n",
      "Train Epoch: 1 [85% 300/352]\tLoss: 0.671935\tData (t) 2.037\tBatch (t) 2.095\n",
      "Train Epoch: 1 [91% 320/352]\tLoss: 0.613728\tData (t) 2.019\tBatch (t) 2.050\n",
      "Train Epoch: 1 [97% 340/352]\tLoss: 0.743365\tData (t) 2.041\tBatch (t) 2.072\n",
      "********************************************************************************\n",
      "Starting eval on validation split\n",
      "Val loss: 1.1198   Acc: 75.04: 100% 40/40 [00:34<00:00,  1.15it/s]\n",
      "Val acc at epoch 1: 75.04\n",
      "Saving model to ./config1_2.pt\n",
      "Train Epoch: 2 [0% 0/352]\tLoss: 0.496453\tData (t) 6.567\tBatch (t) 6.931\n",
      "Train Epoch: 2 [6% 20/352]\tLoss: 0.396248\tData (t) 2.074\tBatch (t) 2.121\n",
      "Train Epoch: 2 [11% 40/352]\tLoss: 0.413617\tData (t) 2.017\tBatch (t) 2.059\n",
      "Train Epoch: 2 [17% 60/352]\tLoss: 0.457554\tData (t) 2.051\tBatch (t) 2.223\n",
      "Train Epoch: 2 [23% 80/352]\tLoss: 0.552157\tData (t) 2.032\tBatch (t) 2.077\n",
      "Train Epoch: 2 [28% 100/352]\tLoss: 0.398652\tData (t) 2.040\tBatch (t) 2.071\n",
      "Train Epoch: 2 [34% 120/352]\tLoss: 0.404064\tData (t) 2.042\tBatch (t) 2.073\n",
      "Train Epoch: 2 [40% 140/352]\tLoss: 0.401470\tData (t) 2.041\tBatch (t) 2.072\n",
      "Train Epoch: 2 [45% 160/352]\tLoss: 0.463397\tData (t) 2.030\tBatch (t) 2.069\n",
      "Train Epoch: 2 [51% 180/352]\tLoss: 0.514588\tData (t) 2.034\tBatch (t) 2.065\n",
      "Train Epoch: 2 [57% 200/352]\tLoss: 0.454806\tData (t) 2.033\tBatch (t) 2.066\n",
      "Train Epoch: 2 [62% 220/352]\tLoss: 0.375516\tData (t) 2.034\tBatch (t) 2.081\n",
      "Train Epoch: 2 [68% 240/352]\tLoss: 0.522849\tData (t) 2.035\tBatch (t) 2.072\n",
      "Train Epoch: 2 [74% 260/352]\tLoss: 0.368076\tData (t) 2.038\tBatch (t) 2.221\n",
      "Train Epoch: 2 [80% 280/352]\tLoss: 0.333889\tData (t) 1.864\tBatch (t) 1.896\n",
      "Train Epoch: 2 [85% 300/352]\tLoss: 0.344610\tData (t) 2.037\tBatch (t) 2.068\n",
      "Train Epoch: 2 [91% 320/352]\tLoss: 0.497864\tData (t) 2.042\tBatch (t) 2.074\n",
      "Train Epoch: 2 [97% 340/352]\tLoss: 0.402224\tData (t) 1.873\tBatch (t) 1.916\n",
      "********************************************************************************\n",
      "Starting eval on validation split\n",
      "Val loss: 0.8136   Acc: 76.80: 100% 40/40 [00:36<00:00,  1.10it/s]\n",
      "Val acc at epoch 2: 76.80\n",
      "Saving model to ./config1_3.pt\n",
      "Train Epoch: 3 [0% 0/352]\tLoss: 0.185093\tData (t) 6.628\tBatch (t) 6.914\n",
      "Train Epoch: 3 [6% 20/352]\tLoss: 0.137882\tData (t) 2.063\tBatch (t) 2.099\n",
      "Train Epoch: 3 [11% 40/352]\tLoss: 0.238177\tData (t) 2.004\tBatch (t) 2.044\n",
      "Train Epoch: 3 [17% 60/352]\tLoss: 0.279114\tData (t) 2.033\tBatch (t) 2.081\n",
      "Train Epoch: 3 [23% 80/352]\tLoss: 0.269158\tData (t) 2.017\tBatch (t) 2.048\n",
      "Train Epoch: 3 [28% 100/352]\tLoss: 0.328763\tData (t) 1.998\tBatch (t) 2.047\n",
      "Train Epoch: 3 [34% 120/352]\tLoss: 0.246874\tData (t) 2.023\tBatch (t) 2.075\n",
      "Train Epoch: 3 [40% 140/352]\tLoss: 0.256330\tData (t) 2.012\tBatch (t) 2.058\n",
      "Train Epoch: 3 [45% 160/352]\tLoss: 0.218323\tData (t) 1.855\tBatch (t) 2.088\n",
      "Train Epoch: 3 [51% 180/352]\tLoss: 0.238956\tData (t) 1.840\tBatch (t) 2.072\n",
      "Train Epoch: 3 [57% 200/352]\tLoss: 0.258622\tData (t) 2.034\tBatch (t) 2.083\n",
      "Train Epoch: 3 [62% 220/352]\tLoss: 0.220502\tData (t) 1.963\tBatch (t) 2.009\n",
      "Train Epoch: 3 [68% 240/352]\tLoss: 0.230731\tData (t) 2.021\tBatch (t) 2.074\n",
      "Train Epoch: 3 [74% 260/352]\tLoss: 0.263673\tData (t) 2.024\tBatch (t) 2.070\n",
      "Train Epoch: 3 [80% 280/352]\tLoss: 0.267900\tData (t) 2.022\tBatch (t) 2.063\n",
      "Train Epoch: 3 [85% 300/352]\tLoss: 0.198898\tData (t) 2.017\tBatch (t) 2.069\n",
      "Train Epoch: 3 [91% 320/352]\tLoss: 0.197539\tData (t) 2.032\tBatch (t) 2.064\n",
      "Train Epoch: 3 [97% 340/352]\tLoss: 0.334714\tData (t) 2.026\tBatch (t) 2.058\n",
      "********************************************************************************\n",
      "Starting eval on validation split\n",
      "Val loss: 1.0995   Acc: 77.14: 100% 40/40 [00:34<00:00,  1.16it/s]\n",
      "Val acc at epoch 3: 77.14\n",
      "Saving model to ./config1_4.pt\n",
      "Train Epoch: 4 [0% 0/352]\tLoss: 0.058337\tData (t) 6.106\tBatch (t) 6.500\n",
      "Train Epoch: 4 [6% 20/352]\tLoss: 0.114853\tData (t) 2.049\tBatch (t) 2.092\n",
      "Train Epoch: 4 [11% 40/352]\tLoss: 0.096688\tData (t) 2.017\tBatch (t) 2.049\n",
      "Train Epoch: 4 [17% 60/352]\tLoss: 0.152249\tData (t) 2.038\tBatch (t) 2.069\n",
      "Train Epoch: 4 [23% 80/352]\tLoss: 0.252231\tData (t) 2.034\tBatch (t) 2.065\n",
      "Train Epoch: 4 [28% 100/352]\tLoss: 0.076533\tData (t) 2.035\tBatch (t) 2.068\n",
      "Train Epoch: 4 [34% 120/352]\tLoss: 0.132971\tData (t) 2.032\tBatch (t) 2.064\n",
      "Train Epoch: 4 [40% 140/352]\tLoss: 0.092438\tData (t) 2.041\tBatch (t) 2.072\n",
      "Train Epoch: 4 [45% 160/352]\tLoss: 0.125334\tData (t) 2.038\tBatch (t) 2.075\n",
      "Train Epoch: 4 [51% 180/352]\tLoss: 0.060519\tData (t) 2.034\tBatch (t) 2.225\n",
      "Train Epoch: 4 [57% 200/352]\tLoss: 0.086647\tData (t) 1.878\tBatch (t) 2.191\n",
      "Train Epoch: 4 [62% 220/352]\tLoss: 0.104377\tData (t) 1.900\tBatch (t) 2.196\n",
      "Train Epoch: 4 [68% 240/352]\tLoss: 0.100532\tData (t) 1.907\tBatch (t) 1.939\n",
      "Train Epoch: 4 [74% 260/352]\tLoss: 0.177138\tData (t) 1.976\tBatch (t) 2.011\n",
      "Train Epoch: 4 [80% 280/352]\tLoss: 0.149915\tData (t) 1.980\tBatch (t) 2.012\n",
      "Train Epoch: 4 [85% 300/352]\tLoss: 0.191410\tData (t) 2.025\tBatch (t) 2.056\n",
      "Train Epoch: 4 [91% 320/352]\tLoss: 0.138626\tData (t) 2.016\tBatch (t) 2.052\n",
      "Train Epoch: 4 [97% 340/352]\tLoss: 0.097233\tData (t) 2.025\tBatch (t) 2.056\n",
      "********************************************************************************\n",
      "Starting eval on validation split\n",
      "Val loss: 0.3845   Acc: 76.99: 100% 40/40 [00:42<00:00,  1.06s/it]\n",
      "Val acc at epoch 4: 76.99\n",
      "Saving model to ./config1_5.pt\n",
      "Train Epoch: 5 [0% 0/352]\tLoss: 0.047342\tData (t) 5.888\tBatch (t) 6.208\n",
      "Train Epoch: 5 [6% 20/352]\tLoss: 0.045724\tData (t) 2.045\tBatch (t) 2.076\n",
      "Train Epoch: 5 [11% 40/352]\tLoss: 0.036104\tData (t) 2.012\tBatch (t) 2.043\n",
      "Train Epoch: 5 [17% 60/352]\tLoss: 0.092604\tData (t) 2.044\tBatch (t) 2.104\n",
      "Train Epoch: 5 [23% 80/352]\tLoss: 0.075031\tData (t) 2.032\tBatch (t) 2.065\n",
      "Train Epoch: 5 [28% 100/352]\tLoss: 0.059714\tData (t) 2.038\tBatch (t) 2.075\n",
      "Train Epoch: 5 [34% 120/352]\tLoss: 0.033275\tData (t) 2.034\tBatch (t) 2.066\n",
      "Train Epoch: 5 [40% 140/352]\tLoss: 0.085656\tData (t) 2.036\tBatch (t) 2.068\n",
      "Train Epoch: 5 [45% 160/352]\tLoss: 0.072312\tData (t) 1.986\tBatch (t) 2.020\n",
      "Train Epoch: 5 [51% 180/352]\tLoss: 0.082576\tData (t) 2.042\tBatch (t) 2.072\n",
      "Train Epoch: 5 [57% 200/352]\tLoss: 0.071426\tData (t) 2.038\tBatch (t) 2.071\n",
      "Train Epoch: 5 [62% 220/352]\tLoss: 0.085166\tData (t) 2.036\tBatch (t) 2.069\n",
      "Train Epoch: 5 [68% 240/352]\tLoss: 0.064636\tData (t) 2.038\tBatch (t) 2.070\n",
      "Train Epoch: 5 [74% 260/352]\tLoss: 0.050800\tData (t) 2.027\tBatch (t) 2.066\n",
      "Train Epoch: 5 [80% 280/352]\tLoss: 0.077679\tData (t) 2.046\tBatch (t) 2.077\n",
      "Train Epoch: 5 [85% 300/352]\tLoss: 0.092337\tData (t) 2.042\tBatch (t) 2.073\n",
      "Train Epoch: 5 [91% 320/352]\tLoss: 0.061193\tData (t) 2.034\tBatch (t) 2.066\n",
      "Train Epoch: 5 [97% 340/352]\tLoss: 0.042397\tData (t) 2.027\tBatch (t) 2.058\n",
      "********************************************************************************\n",
      "Starting eval on validation split\n",
      "Val loss: 0.6287   Acc: 77.96: 100% 40/40 [00:37<00:00,  1.08it/s]\n",
      "Val acc at epoch 5: 77.96\n",
      "Saving model to ./config1_6.pt\n",
      "Train Epoch: 6 [0% 0/352]\tLoss: 0.061105\tData (t) 8.624\tBatch (t) 8.914\n",
      "Train Epoch: 6 [6% 20/352]\tLoss: 0.014794\tData (t) 1.919\tBatch (t) 1.951\n",
      "Train Epoch: 6 [11% 40/352]\tLoss: 0.024344\tData (t) 1.793\tBatch (t) 1.824\n",
      "Train Epoch: 6 [17% 60/352]\tLoss: 0.028481\tData (t) 1.858\tBatch (t) 1.889\n",
      "Train Epoch: 6 [23% 80/352]\tLoss: 0.020336\tData (t) 1.788\tBatch (t) 1.829\n",
      "Train Epoch: 6 [28% 100/352]\tLoss: 0.066957\tData (t) 2.017\tBatch (t) 2.048\n",
      "Train Epoch: 6 [34% 120/352]\tLoss: 0.033540\tData (t) 1.963\tBatch (t) 1.994\n",
      "Train Epoch: 6 [40% 140/352]\tLoss: 0.043989\tData (t) 2.041\tBatch (t) 2.073\n",
      "Train Epoch: 6 [45% 160/352]\tLoss: 0.015840\tData (t) 2.036\tBatch (t) 2.068\n",
      "Train Epoch: 6 [51% 180/352]\tLoss: 0.010376\tData (t) 2.040\tBatch (t) 2.072\n",
      "Train Epoch: 6 [57% 200/352]\tLoss: 0.029828\tData (t) 2.039\tBatch (t) 2.076\n",
      "Train Epoch: 6 [62% 220/352]\tLoss: 0.019020\tData (t) 2.028\tBatch (t) 2.070\n",
      "Train Epoch: 6 [68% 240/352]\tLoss: 0.011406\tData (t) 2.041\tBatch (t) 2.072\n",
      "Train Epoch: 6 [74% 260/352]\tLoss: 0.019642\tData (t) 2.030\tBatch (t) 2.241\n",
      "Train Epoch: 6 [80% 280/352]\tLoss: 0.016148\tData (t) 1.878\tBatch (t) 1.910\n",
      "Train Epoch: 6 [85% 300/352]\tLoss: 0.027405\tData (t) 2.040\tBatch (t) 2.071\n",
      "Train Epoch: 6 [91% 320/352]\tLoss: 0.024107\tData (t) 2.038\tBatch (t) 2.069\n",
      "Train Epoch: 6 [97% 340/352]\tLoss: 0.047117\tData (t) 2.039\tBatch (t) 2.071\n",
      "********************************************************************************\n",
      "Starting eval on validation split\n",
      "Val loss: 0.1376   Acc: 79.21: 100% 40/40 [00:39<00:00,  1.02it/s]\n",
      "Val acc at epoch 6: 79.21\n",
      "Saving model to ./config1_7.pt\n",
      "Train Epoch: 7 [0% 0/352]\tLoss: 0.008159\tData (t) 8.975\tBatch (t) 9.266\n",
      "Train Epoch: 7 [6% 20/352]\tLoss: 0.002781\tData (t) 2.047\tBatch (t) 2.080\n",
      "Train Epoch: 7 [11% 40/352]\tLoss: 0.004646\tData (t) 2.030\tBatch (t) 2.061\n",
      "Train Epoch: 7 [17% 60/352]\tLoss: 0.008357\tData (t) 2.037\tBatch (t) 2.068\n",
      "Train Epoch: 7 [23% 80/352]\tLoss: 0.006591\tData (t) 2.032\tBatch (t) 2.063\n",
      "Train Epoch: 7 [28% 100/352]\tLoss: 0.005889\tData (t) 2.033\tBatch (t) 2.064\n",
      "Train Epoch: 7 [34% 120/352]\tLoss: 0.013710\tData (t) 1.864\tBatch (t) 1.896\n",
      "Train Epoch: 7 [40% 140/352]\tLoss: 0.011587\tData (t) 2.035\tBatch (t) 2.066\n",
      "Train Epoch: 7 [45% 160/352]\tLoss: 0.031828\tData (t) 2.040\tBatch (t) 2.072\n",
      "Train Epoch: 7 [51% 180/352]\tLoss: 0.005371\tData (t) 2.034\tBatch (t) 2.065\n",
      "Train Epoch: 7 [57% 200/352]\tLoss: 0.006831\tData (t) 2.032\tBatch (t) 2.063\n",
      "Train Epoch: 7 [62% 220/352]\tLoss: 0.001469\tData (t) 2.037\tBatch (t) 2.069\n",
      "Train Epoch: 7 [68% 240/352]\tLoss: 0.004138\tData (t) 1.921\tBatch (t) 1.952\n",
      "Train Epoch: 7 [74% 260/352]\tLoss: 0.002297\tData (t) 2.036\tBatch (t) 2.069\n",
      "Train Epoch: 7 [80% 280/352]\tLoss: 0.001525\tData (t) 2.035\tBatch (t) 2.071\n",
      "Train Epoch: 7 [85% 300/352]\tLoss: 0.003119\tData (t) 2.031\tBatch (t) 2.063\n",
      "Train Epoch: 7 [91% 320/352]\tLoss: 0.001227\tData (t) 2.038\tBatch (t) 2.070\n",
      "Train Epoch: 7 [97% 340/352]\tLoss: 0.004459\tData (t) 2.035\tBatch (t) 2.154\n",
      "********************************************************************************\n",
      "Starting eval on validation split\n",
      "Val loss: 0.2098   Acc: 80.45: 100% 40/40 [00:36<00:00,  1.09it/s]\n",
      "Val acc at epoch 7: 80.45\n",
      "Saving model to ./config1_8.pt\n",
      "Train Epoch: 8 [0% 0/352]\tLoss: 0.000716\tData (t) 7.575\tBatch (t) 8.187\n",
      "Train Epoch: 8 [6% 20/352]\tLoss: 0.000996\tData (t) 1.741\tBatch (t) 1.805\n",
      "Train Epoch: 8 [11% 40/352]\tLoss: 0.000775\tData (t) 1.752\tBatch (t) 1.845\n",
      "Train Epoch: 8 [17% 60/352]\tLoss: 0.000873\tData (t) 1.931\tBatch (t) 1.973\n",
      "Train Epoch: 8 [23% 80/352]\tLoss: 0.001172\tData (t) 2.020\tBatch (t) 2.070\n",
      "Train Epoch: 8 [28% 100/352]\tLoss: 0.001098\tData (t) 2.020\tBatch (t) 2.051\n",
      "Train Epoch: 8 [34% 120/352]\tLoss: 0.000632\tData (t) 2.036\tBatch (t) 2.067\n",
      "Train Epoch: 8 [40% 140/352]\tLoss: 0.000564\tData (t) 2.031\tBatch (t) 2.063\n",
      "Train Epoch: 8 [45% 160/352]\tLoss: 0.000364\tData (t) 2.040\tBatch (t) 2.072\n",
      "Train Epoch: 8 [51% 180/352]\tLoss: 0.000722\tData (t) 2.035\tBatch (t) 2.066\n",
      "Train Epoch: 8 [57% 200/352]\tLoss: 0.000501\tData (t) 2.037\tBatch (t) 2.068\n",
      "Train Epoch: 8 [62% 220/352]\tLoss: 0.000525\tData (t) 2.035\tBatch (t) 2.066\n",
      "Train Epoch: 8 [68% 240/352]\tLoss: 0.001946\tData (t) 2.037\tBatch (t) 2.068\n",
      "Train Epoch: 8 [74% 260/352]\tLoss: 0.000951\tData (t) 2.038\tBatch (t) 2.069\n",
      "Train Epoch: 8 [80% 280/352]\tLoss: 0.000752\tData (t) 2.026\tBatch (t) 2.064\n",
      "Train Epoch: 8 [85% 300/352]\tLoss: 0.000764\tData (t) 2.035\tBatch (t) 2.065\n",
      "Train Epoch: 8 [91% 320/352]\tLoss: 0.000533\tData (t) 2.036\tBatch (t) 2.067\n",
      "Train Epoch: 8 [97% 340/352]\tLoss: 0.000545\tData (t) 2.034\tBatch (t) 2.065\n",
      "********************************************************************************\n",
      "Starting eval on validation split\n",
      "Val loss: 0.1265   Acc: 81.25: 100% 40/40 [00:36<00:00,  1.10it/s]\n",
      "Val acc at epoch 8: 81.25\n",
      "Saving model to ./config1_9.pt\n",
      "Train Epoch: 9 [0% 0/352]\tLoss: 0.000439\tData (t) 6.750\tBatch (t) 7.117\n",
      "Train Epoch: 9 [6% 20/352]\tLoss: 0.000270\tData (t) 2.056\tBatch (t) 2.088\n",
      "Train Epoch: 9 [11% 40/352]\tLoss: 0.000328\tData (t) 2.013\tBatch (t) 2.046\n",
      "Train Epoch: 9 [17% 60/352]\tLoss: 0.000288\tData (t) 2.044\tBatch (t) 2.075\n",
      "Train Epoch: 9 [23% 80/352]\tLoss: 0.000402\tData (t) 2.037\tBatch (t) 2.094\n",
      "Train Epoch: 9 [28% 100/352]\tLoss: 0.000474\tData (t) 2.028\tBatch (t) 2.076\n",
      "Train Epoch: 9 [34% 120/352]\tLoss: 0.000287\tData (t) 2.026\tBatch (t) 2.057\n",
      "Train Epoch: 9 [40% 140/352]\tLoss: 0.000230\tData (t) 2.029\tBatch (t) 2.061\n",
      "Train Epoch: 9 [45% 160/352]\tLoss: 0.000326\tData (t) 2.025\tBatch (t) 2.056\n",
      "Train Epoch: 9 [51% 180/352]\tLoss: 0.000268\tData (t) 2.022\tBatch (t) 2.058\n",
      "Train Epoch: 9 [57% 200/352]\tLoss: 0.000349\tData (t) 1.715\tBatch (t) 2.000\n",
      "Train Epoch: 9 [62% 220/352]\tLoss: 0.000367\tData (t) 1.827\tBatch (t) 1.880\n",
      "Train Epoch: 9 [68% 240/352]\tLoss: 0.000300\tData (t) 2.012\tBatch (t) 2.043\n",
      "Train Epoch: 9 [74% 260/352]\tLoss: 0.000256\tData (t) 2.028\tBatch (t) 2.061\n",
      "Train Epoch: 9 [80% 280/352]\tLoss: 0.000301\tData (t) 2.036\tBatch (t) 2.070\n",
      "Train Epoch: 9 [85% 300/352]\tLoss: 0.000332\tData (t) 2.038\tBatch (t) 2.069\n",
      "Train Epoch: 9 [91% 320/352]\tLoss: 0.000384\tData (t) 2.018\tBatch (t) 2.065\n",
      "Train Epoch: 9 [97% 340/352]\tLoss: 0.000284\tData (t) 2.035\tBatch (t) 2.066\n",
      "********************************************************************************\n",
      "Starting eval on validation split\n",
      "Val loss: 0.1076   Acc: 81.26: 100% 40/40 [00:34<00:00,  1.15it/s]\n",
      "Val acc at epoch 9: 81.26\n",
      "Saving model to ./config1_10.pt\n",
      "✅ Configuration 1 completed and backed up to Drive!\n"
     ]
    }
   ],
   "source": [
    "# Configuration 1: lr=3e-5, wd=0.1, epochs=10, batch_size=256, timm_aug=False\n",
    "!python $CODE_DIR/finetune.py --lr 3e-5 --wd 0.1 --epochs 10 --batch-size 256 --data-location $DATA_DIR --name \"config1\"\n",
    "\n",
    "# Backup model to Google Drive\n",
    "!cp config1_*.pt \"/content/drive/MyDrive/Colab Notebooks/\"\n",
    "print(\"✅ Configuration 1 completed and backed up to Drive!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8097860,
     "status": "ok",
     "timestamp": 1755936029540,
     "user": {
      "displayName": "Yang Zhou",
      "userId": "09717432835810465973"
     },
     "user_tz": 240
    },
    "id": "4GPdoKxdvedb",
    "outputId": "762602a7-ff3c-4c93-df26-d1000771a561"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(\n",
      "Building zero-shot classifier.\n",
      "100% 200/200 [00:11<00:00, 17.68it/s]\n",
      "Saving model to ./config2_0.pt\n",
      "Train Epoch: 0 [0% 0/352]\tLoss: 1.667826\tData (t) 14.185\tBatch (t) 20.316\n",
      "Train Epoch: 0 [6% 20/352]\tLoss: 1.226475\tData (t) 1.959\tBatch (t) 2.005\n",
      "Train Epoch: 0 [11% 40/352]\tLoss: 1.347273\tData (t) 2.024\tBatch (t) 2.088\n",
      "Train Epoch: 0 [17% 60/352]\tLoss: 1.134597\tData (t) 2.014\tBatch (t) 2.060\n",
      "Train Epoch: 0 [23% 80/352]\tLoss: 0.934149\tData (t) 2.039\tBatch (t) 2.070\n",
      "Train Epoch: 0 [28% 100/352]\tLoss: 0.970417\tData (t) 2.030\tBatch (t) 2.078\n",
      "Train Epoch: 0 [34% 120/352]\tLoss: 1.104056\tData (t) 2.029\tBatch (t) 2.062\n",
      "Train Epoch: 0 [40% 140/352]\tLoss: 1.082346\tData (t) 2.034\tBatch (t) 2.065\n",
      "Train Epoch: 0 [45% 160/352]\tLoss: 0.980819\tData (t) 2.032\tBatch (t) 2.063\n",
      "Train Epoch: 0 [51% 180/352]\tLoss: 0.824228\tData (t) 2.032\tBatch (t) 2.063\n",
      "Train Epoch: 0 [57% 200/352]\tLoss: 1.084069\tData (t) 2.031\tBatch (t) 2.061\n",
      "Train Epoch: 0 [62% 220/352]\tLoss: 1.013503\tData (t) 2.032\tBatch (t) 2.063\n",
      "Train Epoch: 0 [68% 240/352]\tLoss: 0.845630\tData (t) 2.029\tBatch (t) 2.060\n",
      "Train Epoch: 0 [74% 260/352]\tLoss: 0.688177\tData (t) 2.041\tBatch (t) 2.072\n",
      "Train Epoch: 0 [80% 280/352]\tLoss: 0.853603\tData (t) 2.038\tBatch (t) 2.069\n",
      "Train Epoch: 0 [85% 300/352]\tLoss: 0.732890\tData (t) 2.024\tBatch (t) 2.064\n",
      "Train Epoch: 0 [91% 320/352]\tLoss: 0.906724\tData (t) 2.028\tBatch (t) 2.074\n",
      "Train Epoch: 0 [97% 340/352]\tLoss: 0.755277\tData (t) 2.018\tBatch (t) 2.181\n",
      "********************************************************************************\n",
      "Starting eval on validation split\n",
      "Val loss: 1.4452   Acc: 77.66: 100% 40/40 [00:37<00:00,  1.08it/s]\n",
      "Val acc at epoch 0: 77.66\n",
      "Saving model to ./config2_1.pt\n",
      "Train Epoch: 1 [0% 0/352]\tLoss: 0.676741\tData (t) 10.397\tBatch (t) 10.759\n",
      "Train Epoch: 1 [6% 20/352]\tLoss: 0.618062\tData (t) 2.011\tBatch (t) 2.044\n",
      "Train Epoch: 1 [11% 40/352]\tLoss: 0.593301\tData (t) 2.021\tBatch (t) 2.058\n",
      "Train Epoch: 1 [17% 60/352]\tLoss: 0.638656\tData (t) 2.044\tBatch (t) 2.076\n",
      "Train Epoch: 1 [23% 80/352]\tLoss: 0.661722\tData (t) 2.043\tBatch (t) 2.075\n",
      "Train Epoch: 1 [28% 100/352]\tLoss: 0.487982\tData (t) 2.011\tBatch (t) 2.042\n",
      "Train Epoch: 1 [34% 120/352]\tLoss: 0.665891\tData (t) 2.032\tBatch (t) 2.064\n",
      "Train Epoch: 1 [40% 140/352]\tLoss: 0.654861\tData (t) 2.030\tBatch (t) 2.062\n",
      "Train Epoch: 1 [45% 160/352]\tLoss: 0.617651\tData (t) 2.033\tBatch (t) 2.065\n",
      "Train Epoch: 1 [51% 180/352]\tLoss: 0.712417\tData (t) 1.885\tBatch (t) 2.126\n",
      "Train Epoch: 1 [57% 200/352]\tLoss: 0.619495\tData (t) 1.812\tBatch (t) 2.069\n",
      "Train Epoch: 1 [62% 220/352]\tLoss: 0.604688\tData (t) 1.934\tBatch (t) 1.966\n",
      "Train Epoch: 1 [68% 240/352]\tLoss: 0.830658\tData (t) 2.040\tBatch (t) 2.071\n",
      "Train Epoch: 1 [74% 260/352]\tLoss: 0.537400\tData (t) 2.030\tBatch (t) 2.062\n",
      "Train Epoch: 1 [80% 280/352]\tLoss: 0.524023\tData (t) 2.027\tBatch (t) 2.059\n",
      "Train Epoch: 1 [85% 300/352]\tLoss: 0.751732\tData (t) 2.041\tBatch (t) 2.073\n",
      "Train Epoch: 1 [91% 320/352]\tLoss: 0.612888\tData (t) 2.024\tBatch (t) 2.184\n",
      "Train Epoch: 1 [97% 340/352]\tLoss: 0.724967\tData (t) 1.871\tBatch (t) 1.903\n",
      "********************************************************************************\n",
      "Starting eval on validation split\n",
      "Val loss: 0.9015   Acc: 79.52: 100% 40/40 [00:46<00:00,  1.17s/it]\n",
      "Val acc at epoch 1: 79.52\n",
      "Saving model to ./config2_2.pt\n",
      "Train Epoch: 2 [0% 0/352]\tLoss: 0.333616\tData (t) 12.899\tBatch (t) 13.088\n",
      "Train Epoch: 2 [6% 20/352]\tLoss: 0.301759\tData (t) 2.075\tBatch (t) 2.122\n",
      "Train Epoch: 2 [11% 40/352]\tLoss: 0.426270\tData (t) 2.023\tBatch (t) 2.055\n",
      "Train Epoch: 2 [17% 60/352]\tLoss: 0.343622\tData (t) 2.046\tBatch (t) 2.100\n",
      "Train Epoch: 2 [23% 80/352]\tLoss: 0.380778\tData (t) 1.859\tBatch (t) 2.099\n",
      "Train Epoch: 2 [28% 100/352]\tLoss: 0.334388\tData (t) 1.831\tBatch (t) 1.862\n",
      "Train Epoch: 2 [34% 120/352]\tLoss: 0.337016\tData (t) 2.029\tBatch (t) 2.061\n",
      "Train Epoch: 2 [40% 140/352]\tLoss: 0.313427\tData (t) 2.034\tBatch (t) 2.086\n",
      "Train Epoch: 2 [45% 160/352]\tLoss: 0.382062\tData (t) 2.037\tBatch (t) 2.068\n",
      "Train Epoch: 2 [51% 180/352]\tLoss: 0.260750\tData (t) 2.029\tBatch (t) 2.071\n",
      "Train Epoch: 2 [57% 200/352]\tLoss: 0.359604\tData (t) 2.034\tBatch (t) 2.076\n",
      "Train Epoch: 2 [62% 220/352]\tLoss: 0.327629\tData (t) 2.014\tBatch (t) 2.056\n",
      "Train Epoch: 2 [68% 240/352]\tLoss: 0.335867\tData (t) 2.018\tBatch (t) 2.062\n",
      "Train Epoch: 2 [74% 260/352]\tLoss: 0.236344\tData (t) 2.024\tBatch (t) 2.186\n",
      "Train Epoch: 2 [80% 280/352]\tLoss: 0.220993\tData (t) 2.010\tBatch (t) 2.052\n",
      "Train Epoch: 2 [85% 300/352]\tLoss: 0.330630\tData (t) 2.008\tBatch (t) 2.041\n",
      "Train Epoch: 2 [91% 320/352]\tLoss: 0.391981\tData (t) 2.036\tBatch (t) 2.068\n",
      "Train Epoch: 2 [97% 340/352]\tLoss: 0.246701\tData (t) 2.037\tBatch (t) 2.069\n",
      "********************************************************************************\n",
      "Starting eval on validation split\n",
      "Val loss: 0.7784   Acc: 79.82: 100% 40/40 [00:34<00:00,  1.14it/s]\n",
      "Val acc at epoch 2: 79.82\n",
      "Saving model to ./config2_3.pt\n",
      "Train Epoch: 3 [0% 0/352]\tLoss: 0.156245\tData (t) 5.439\tBatch (t) 5.723\n",
      "Train Epoch: 3 [6% 20/352]\tLoss: 0.089623\tData (t) 2.079\tBatch (t) 2.111\n",
      "Train Epoch: 3 [11% 40/352]\tLoss: 0.157543\tData (t) 2.013\tBatch (t) 2.045\n",
      "Train Epoch: 3 [17% 60/352]\tLoss: 0.158901\tData (t) 2.048\tBatch (t) 2.079\n",
      "Train Epoch: 3 [23% 80/352]\tLoss: 0.137364\tData (t) 2.035\tBatch (t) 2.067\n",
      "Train Epoch: 3 [28% 100/352]\tLoss: 0.180458\tData (t) 2.011\tBatch (t) 2.043\n",
      "Train Epoch: 3 [34% 120/352]\tLoss: 0.148695\tData (t) 2.026\tBatch (t) 2.057\n",
      "Train Epoch: 3 [40% 140/352]\tLoss: 0.149966\tData (t) 2.030\tBatch (t) 2.074\n",
      "Train Epoch: 3 [45% 160/352]\tLoss: 0.167727\tData (t) 2.041\tBatch (t) 2.072\n",
      "Train Epoch: 3 [51% 180/352]\tLoss: 0.125779\tData (t) 2.037\tBatch (t) 2.068\n",
      "Train Epoch: 3 [57% 200/352]\tLoss: 0.102279\tData (t) 2.030\tBatch (t) 2.183\n",
      "Train Epoch: 3 [62% 220/352]\tLoss: 0.098721\tData (t) 1.871\tBatch (t) 1.901\n",
      "Train Epoch: 3 [68% 240/352]\tLoss: 0.218179\tData (t) 2.032\tBatch (t) 2.070\n",
      "Train Epoch: 3 [74% 260/352]\tLoss: 0.128042\tData (t) 2.035\tBatch (t) 2.066\n",
      "Train Epoch: 3 [80% 280/352]\tLoss: 0.130421\tData (t) 2.036\tBatch (t) 2.067\n",
      "Train Epoch: 3 [85% 300/352]\tLoss: 0.137891\tData (t) 1.825\tBatch (t) 1.858\n",
      "Train Epoch: 3 [91% 320/352]\tLoss: 0.187173\tData (t) 1.876\tBatch (t) 2.101\n",
      "Train Epoch: 3 [97% 340/352]\tLoss: 0.171679\tData (t) 2.038\tBatch (t) 2.070\n",
      "********************************************************************************\n",
      "Starting eval on validation split\n",
      "Val loss: 0.6679   Acc: 80.09: 100% 40/40 [00:34<00:00,  1.15it/s]\n",
      "Val acc at epoch 3: 80.09\n",
      "Saving model to ./config2_4.pt\n",
      "Train Epoch: 4 [0% 0/352]\tLoss: 0.027313\tData (t) 7.738\tBatch (t) 7.974\n",
      "Train Epoch: 4 [6% 20/352]\tLoss: 0.067287\tData (t) 2.049\tBatch (t) 2.097\n",
      "Train Epoch: 4 [11% 40/352]\tLoss: 0.075789\tData (t) 2.000\tBatch (t) 2.031\n",
      "Train Epoch: 4 [17% 60/352]\tLoss: 0.080053\tData (t) 2.031\tBatch (t) 2.084\n",
      "Train Epoch: 4 [23% 80/352]\tLoss: 0.071117\tData (t) 2.029\tBatch (t) 2.079\n",
      "Train Epoch: 4 [28% 100/352]\tLoss: 0.047882\tData (t) 2.024\tBatch (t) 2.064\n",
      "Train Epoch: 4 [34% 120/352]\tLoss: 0.039832\tData (t) 2.021\tBatch (t) 2.059\n",
      "Train Epoch: 4 [40% 140/352]\tLoss: 0.067026\tData (t) 1.823\tBatch (t) 2.044\n",
      "Train Epoch: 4 [45% 160/352]\tLoss: 0.058693\tData (t) 1.945\tBatch (t) 1.997\n",
      "Train Epoch: 4 [51% 180/352]\tLoss: 0.100054\tData (t) 2.032\tBatch (t) 2.063\n",
      "Train Epoch: 4 [57% 200/352]\tLoss: 0.074906\tData (t) 2.042\tBatch (t) 2.073\n",
      "Train Epoch: 4 [62% 220/352]\tLoss: 0.051781\tData (t) 2.043\tBatch (t) 2.075\n",
      "Train Epoch: 4 [68% 240/352]\tLoss: 0.059039\tData (t) 2.039\tBatch (t) 2.071\n",
      "Train Epoch: 4 [74% 260/352]\tLoss: 0.049112\tData (t) 2.034\tBatch (t) 2.066\n",
      "Train Epoch: 4 [80% 280/352]\tLoss: 0.095064\tData (t) 2.036\tBatch (t) 2.067\n",
      "Train Epoch: 4 [85% 300/352]\tLoss: 0.042868\tData (t) 2.040\tBatch (t) 2.071\n",
      "Train Epoch: 4 [91% 320/352]\tLoss: 0.038207\tData (t) 2.040\tBatch (t) 2.072\n",
      "Train Epoch: 4 [97% 340/352]\tLoss: 0.042428\tData (t) 2.043\tBatch (t) 2.074\n",
      "********************************************************************************\n",
      "Starting eval on validation split\n",
      "Val loss: 0.5973   Acc: 81.02: 100% 40/40 [00:33<00:00,  1.20it/s]\n",
      "Val acc at epoch 4: 81.02\n",
      "Saving model to ./config2_5.pt\n",
      "Train Epoch: 5 [0% 0/352]\tLoss: 0.064929\tData (t) 7.615\tBatch (t) 7.874\n",
      "Train Epoch: 5 [6% 20/352]\tLoss: 0.011683\tData (t) 2.089\tBatch (t) 2.120\n",
      "Train Epoch: 5 [11% 40/352]\tLoss: 0.024168\tData (t) 2.014\tBatch (t) 2.050\n",
      "Train Epoch: 5 [17% 60/352]\tLoss: 0.026901\tData (t) 2.054\tBatch (t) 2.084\n",
      "Train Epoch: 5 [23% 80/352]\tLoss: 0.027791\tData (t) 2.034\tBatch (t) 2.065\n",
      "Train Epoch: 5 [28% 100/352]\tLoss: 0.019660\tData (t) 2.043\tBatch (t) 2.074\n",
      "Train Epoch: 5 [34% 120/352]\tLoss: 0.023394\tData (t) 2.040\tBatch (t) 2.071\n",
      "Train Epoch: 5 [40% 140/352]\tLoss: 0.040711\tData (t) 2.035\tBatch (t) 2.074\n",
      "Train Epoch: 5 [45% 160/352]\tLoss: 0.023937\tData (t) 2.041\tBatch (t) 2.072\n",
      "Train Epoch: 5 [51% 180/352]\tLoss: 0.032513\tData (t) 2.033\tBatch (t) 2.064\n",
      "Train Epoch: 5 [57% 200/352]\tLoss: 0.031272\tData (t) 2.034\tBatch (t) 2.065\n",
      "Train Epoch: 5 [62% 220/352]\tLoss: 0.015104\tData (t) 2.036\tBatch (t) 2.068\n",
      "Train Epoch: 5 [68% 240/352]\tLoss: 0.019935\tData (t) 2.041\tBatch (t) 2.072\n",
      "Train Epoch: 5 [74% 260/352]\tLoss: 0.031458\tData (t) 1.822\tBatch (t) 1.975\n",
      "Train Epoch: 5 [80% 280/352]\tLoss: 0.020223\tData (t) 1.894\tBatch (t) 2.149\n",
      "Train Epoch: 5 [85% 300/352]\tLoss: 0.011479\tData (t) 2.037\tBatch (t) 2.068\n",
      "Train Epoch: 5 [91% 320/352]\tLoss: 0.040380\tData (t) 1.802\tBatch (t) 2.099\n",
      "Train Epoch: 5 [97% 340/352]\tLoss: 0.010471\tData (t) 2.036\tBatch (t) 2.067\n",
      "********************************************************************************\n",
      "Starting eval on validation split\n",
      "Val loss: 0.7309   Acc: 81.12: 100% 40/40 [00:41<00:00,  1.04s/it]\n",
      "Val acc at epoch 5: 81.12\n",
      "Saving model to ./config2_6.pt\n",
      "Train Epoch: 6 [0% 0/352]\tLoss: 0.009487\tData (t) 7.405\tBatch (t) 8.620\n",
      "Train Epoch: 6 [6% 20/352]\tLoss: 0.006453\tData (t) 2.063\tBatch (t) 2.096\n",
      "Train Epoch: 6 [11% 40/352]\tLoss: 0.008782\tData (t) 2.027\tBatch (t) 2.058\n",
      "Train Epoch: 6 [17% 60/352]\tLoss: 0.008520\tData (t) 2.048\tBatch (t) 2.080\n",
      "Train Epoch: 6 [23% 80/352]\tLoss: 0.004801\tData (t) 2.036\tBatch (t) 2.068\n",
      "Train Epoch: 6 [28% 100/352]\tLoss: 0.011488\tData (t) 2.022\tBatch (t) 2.061\n",
      "Train Epoch: 6 [34% 120/352]\tLoss: 0.010120\tData (t) 2.027\tBatch (t) 2.059\n",
      "Train Epoch: 6 [40% 140/352]\tLoss: 0.013234\tData (t) 2.042\tBatch (t) 2.074\n",
      "Train Epoch: 6 [45% 160/352]\tLoss: 0.007728\tData (t) 2.043\tBatch (t) 2.075\n",
      "Train Epoch: 6 [51% 180/352]\tLoss: 0.011475\tData (t) 2.039\tBatch (t) 2.072\n",
      "Train Epoch: 6 [57% 200/352]\tLoss: 0.006148\tData (t) 2.038\tBatch (t) 2.069\n",
      "Train Epoch: 6 [62% 220/352]\tLoss: 0.010155\tData (t) 2.035\tBatch (t) 2.067\n",
      "Train Epoch: 6 [68% 240/352]\tLoss: 0.011376\tData (t) 1.878\tBatch (t) 2.073\n",
      "Train Epoch: 6 [74% 260/352]\tLoss: 0.006646\tData (t) 2.036\tBatch (t) 2.067\n",
      "Train Epoch: 6 [80% 280/352]\tLoss: 0.007011\tData (t) 2.038\tBatch (t) 2.070\n",
      "Train Epoch: 6 [85% 300/352]\tLoss: 0.005530\tData (t) 2.035\tBatch (t) 2.085\n",
      "Train Epoch: 6 [91% 320/352]\tLoss: 0.007036\tData (t) 2.021\tBatch (t) 2.069\n",
      "Train Epoch: 6 [97% 340/352]\tLoss: 0.005386\tData (t) 2.030\tBatch (t) 2.061\n",
      "********************************************************************************\n",
      "Starting eval on validation split\n",
      "Val loss: 0.6725   Acc: 81.87: 100% 40/40 [00:37<00:00,  1.07it/s]\n",
      "Val acc at epoch 6: 81.87\n",
      "Saving model to ./config2_7.pt\n",
      "Train Epoch: 7 [0% 0/352]\tLoss: 0.001847\tData (t) 6.603\tBatch (t) 6.938\n",
      "Train Epoch: 7 [6% 20/352]\tLoss: 0.002920\tData (t) 2.079\tBatch (t) 2.110\n",
      "Train Epoch: 7 [11% 40/352]\tLoss: 0.001780\tData (t) 2.012\tBatch (t) 2.043\n",
      "Train Epoch: 7 [17% 60/352]\tLoss: 0.001361\tData (t) 2.048\tBatch (t) 2.080\n",
      "Train Epoch: 7 [23% 80/352]\tLoss: 0.002550\tData (t) 2.036\tBatch (t) 2.067\n",
      "Train Epoch: 7 [28% 100/352]\tLoss: 0.002099\tData (t) 2.002\tBatch (t) 2.033\n",
      "Train Epoch: 7 [34% 120/352]\tLoss: 0.002073\tData (t) 2.013\tBatch (t) 2.044\n",
      "Train Epoch: 7 [40% 140/352]\tLoss: 0.001184\tData (t) 2.025\tBatch (t) 2.056\n",
      "Train Epoch: 7 [45% 160/352]\tLoss: 0.002236\tData (t) 2.034\tBatch (t) 2.066\n",
      "Train Epoch: 7 [51% 180/352]\tLoss: 0.001135\tData (t) 2.009\tBatch (t) 2.071\n",
      "Train Epoch: 7 [57% 200/352]\tLoss: 0.024861\tData (t) 2.037\tBatch (t) 2.083\n",
      "Train Epoch: 7 [62% 220/352]\tLoss: 0.029999\tData (t) 2.028\tBatch (t) 2.069\n",
      "Train Epoch: 7 [68% 240/352]\tLoss: 0.001408\tData (t) 1.795\tBatch (t) 2.082\n",
      "Train Epoch: 7 [74% 260/352]\tLoss: 0.001177\tData (t) 1.783\tBatch (t) 2.070\n",
      "Train Epoch: 7 [80% 280/352]\tLoss: 0.002928\tData (t) 1.881\tBatch (t) 2.160\n",
      "Train Epoch: 7 [85% 300/352]\tLoss: 0.002807\tData (t) 1.762\tBatch (t) 2.071\n",
      "Train Epoch: 7 [91% 320/352]\tLoss: 0.001812\tData (t) 2.039\tBatch (t) 2.070\n",
      "Train Epoch: 7 [97% 340/352]\tLoss: 0.002144\tData (t) 2.037\tBatch (t) 2.231\n",
      "********************************************************************************\n",
      "Starting eval on validation split\n",
      "Val loss: 0.4866   Acc: 82.28: 100% 40/40 [00:37<00:00,  1.07it/s]\n",
      "Val acc at epoch 7: 82.28\n",
      "Saving model to ./config2_8.pt\n",
      "Train Epoch: 8 [0% 0/352]\tLoss: 0.000661\tData (t) 4.658\tBatch (t) 4.794\n",
      "Train Epoch: 8 [6% 20/352]\tLoss: 0.000775\tData (t) 2.079\tBatch (t) 2.110\n",
      "Train Epoch: 8 [11% 40/352]\tLoss: 0.000645\tData (t) 2.015\tBatch (t) 2.048\n",
      "Train Epoch: 8 [17% 60/352]\tLoss: 0.000680\tData (t) 2.050\tBatch (t) 2.083\n",
      "Train Epoch: 8 [23% 80/352]\tLoss: 0.000674\tData (t) 1.831\tBatch (t) 1.863\n",
      "Train Epoch: 8 [28% 100/352]\tLoss: 0.000493\tData (t) 1.869\tBatch (t) 1.905\n",
      "Train Epoch: 8 [34% 120/352]\tLoss: 0.000482\tData (t) 2.036\tBatch (t) 2.070\n",
      "Train Epoch: 8 [40% 140/352]\tLoss: 0.000669\tData (t) 2.039\tBatch (t) 2.074\n",
      "Train Epoch: 8 [45% 160/352]\tLoss: 0.000686\tData (t) 2.041\tBatch (t) 2.072\n",
      "Train Epoch: 8 [51% 180/352]\tLoss: 0.000463\tData (t) 2.037\tBatch (t) 2.068\n",
      "Train Epoch: 8 [57% 200/352]\tLoss: 0.000487\tData (t) 1.859\tBatch (t) 2.066\n",
      "Train Epoch: 8 [62% 220/352]\tLoss: 0.000578\tData (t) 1.898\tBatch (t) 2.080\n",
      "Train Epoch: 8 [68% 240/352]\tLoss: 0.000505\tData (t) 1.875\tBatch (t) 2.066\n",
      "Train Epoch: 8 [74% 260/352]\tLoss: 0.015455\tData (t) 1.883\tBatch (t) 2.069\n",
      "Train Epoch: 8 [80% 280/352]\tLoss: 0.000692\tData (t) 2.018\tBatch (t) 2.049\n",
      "Train Epoch: 8 [85% 300/352]\tLoss: 0.000498\tData (t) 2.036\tBatch (t) 2.067\n",
      "Train Epoch: 8 [91% 320/352]\tLoss: 0.000403\tData (t) 2.037\tBatch (t) 2.069\n",
      "Train Epoch: 8 [97% 340/352]\tLoss: 0.000839\tData (t) 2.021\tBatch (t) 2.216\n",
      "********************************************************************************\n",
      "Starting eval on validation split\n",
      "Val loss: 0.4982   Acc: 82.53: 100% 40/40 [00:40<00:00,  1.02s/it]\n",
      "Val acc at epoch 8: 82.53\n",
      "Saving model to ./config2_9.pt\n",
      "Train Epoch: 9 [0% 0/352]\tLoss: 0.000526\tData (t) 10.155\tBatch (t) 11.574\n",
      "Train Epoch: 9 [6% 20/352]\tLoss: 0.000433\tData (t) 1.839\tBatch (t) 2.170\n",
      "Train Epoch: 9 [11% 40/352]\tLoss: 0.000427\tData (t) 1.739\tBatch (t) 2.090\n",
      "Train Epoch: 9 [17% 60/352]\tLoss: 0.000503\tData (t) 1.879\tBatch (t) 2.202\n",
      "Train Epoch: 9 [23% 80/352]\tLoss: 0.000438\tData (t) 1.879\tBatch (t) 2.174\n",
      "Train Epoch: 9 [28% 100/352]\tLoss: 0.000533\tData (t) 1.875\tBatch (t) 2.086\n",
      "Train Epoch: 9 [34% 120/352]\tLoss: 0.000447\tData (t) 2.032\tBatch (t) 2.226\n",
      "Train Epoch: 9 [40% 140/352]\tLoss: 0.000523\tData (t) 2.040\tBatch (t) 2.071\n",
      "Train Epoch: 9 [45% 160/352]\tLoss: 0.000362\tData (t) 2.036\tBatch (t) 2.068\n",
      "Train Epoch: 9 [51% 180/352]\tLoss: 0.000464\tData (t) 2.036\tBatch (t) 2.068\n",
      "Train Epoch: 9 [57% 200/352]\tLoss: 0.000426\tData (t) 2.035\tBatch (t) 2.075\n",
      "Train Epoch: 9 [62% 220/352]\tLoss: 0.000664\tData (t) 1.999\tBatch (t) 2.032\n",
      "Train Epoch: 9 [68% 240/352]\tLoss: 0.000505\tData (t) 2.015\tBatch (t) 2.048\n",
      "Train Epoch: 9 [74% 260/352]\tLoss: 0.000504\tData (t) 2.013\tBatch (t) 2.044\n",
      "Train Epoch: 9 [80% 280/352]\tLoss: 0.000466\tData (t) 2.025\tBatch (t) 2.056\n",
      "Train Epoch: 9 [85% 300/352]\tLoss: 0.000559\tData (t) 2.013\tBatch (t) 2.044\n",
      "Train Epoch: 9 [91% 320/352]\tLoss: 0.000442\tData (t) 2.020\tBatch (t) 2.066\n",
      "Train Epoch: 9 [97% 340/352]\tLoss: 0.000584\tData (t) 2.020\tBatch (t) 2.067\n",
      "********************************************************************************\n",
      "Starting eval on validation split\n",
      "Val loss: 0.4952   Acc: 82.58: 100% 40/40 [00:34<00:00,  1.15it/s]\n",
      "Val acc at epoch 9: 82.58\n",
      "Saving model to ./config2_10.pt\n",
      "✅ Configuration 2 completed and backed up to Drive!\n"
     ]
    }
   ],
   "source": [
    "# Configuration 2: lr=1e-5, wd=0.1, epochs=10, batch_size=256, timm_aug=False\n",
    "!python $CODE_DIR/finetune.py --lr 1e-5 --wd 0.1 --epochs 10 --batch-size 256 --data-location $DATA_DIR --name \"config2\"\n",
    "\n",
    "# Backup model to Google Drive\n",
    "!cp config2_*.pt \"/content/drive/MyDrive/Colab Notebooks/\"\n",
    "print(\"✅ Configuration 2 completed and backed up to Drive!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8116508,
     "status": "ok",
     "timestamp": 1755944146032,
     "user": {
      "displayName": "Yang Zhou",
      "userId": "09717432835810465973"
     },
     "user_tz": 240
    },
    "id": "Z8Qw_Tl0kQFE",
    "outputId": "159f3a80-a697-4829-84ec-0fca8d37dc18"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(\n",
      "Building zero-shot classifier.\n",
      "100% 200/200 [00:11<00:00, 17.26it/s]\n",
      "Saving model to ./config3_0.pt\n",
      "Train Epoch: 0 [0% 0/352]\tLoss: 1.600170\tData (t) 10.423\tBatch (t) 17.322\n",
      "Train Epoch: 0 [6% 20/352]\tLoss: 1.583610\tData (t) 2.006\tBatch (t) 2.053\n",
      "Train Epoch: 0 [11% 40/352]\tLoss: 1.211902\tData (t) 2.009\tBatch (t) 2.065\n",
      "Train Epoch: 0 [17% 60/352]\tLoss: 1.175887\tData (t) 2.009\tBatch (t) 2.053\n",
      "Train Epoch: 0 [23% 80/352]\tLoss: 1.156675\tData (t) 2.027\tBatch (t) 2.253\n",
      "Train Epoch: 0 [28% 100/352]\tLoss: 1.083810\tData (t) 2.030\tBatch (t) 2.061\n",
      "Train Epoch: 0 [34% 120/352]\tLoss: 1.012528\tData (t) 2.031\tBatch (t) 2.299\n",
      "Train Epoch: 0 [40% 140/352]\tLoss: 1.187462\tData (t) 2.035\tBatch (t) 2.067\n",
      "Train Epoch: 0 [45% 160/352]\tLoss: 0.988308\tData (t) 2.035\tBatch (t) 2.067\n",
      "Train Epoch: 0 [51% 180/352]\tLoss: 1.009607\tData (t) 2.036\tBatch (t) 2.069\n",
      "Train Epoch: 0 [57% 200/352]\tLoss: 0.782348\tData (t) 2.033\tBatch (t) 2.064\n",
      "Train Epoch: 0 [62% 220/352]\tLoss: 1.010485\tData (t) 2.034\tBatch (t) 2.070\n",
      "Train Epoch: 0 [68% 240/352]\tLoss: 1.001990\tData (t) 2.030\tBatch (t) 2.061\n",
      "Train Epoch: 0 [74% 260/352]\tLoss: 1.088537\tData (t) 2.030\tBatch (t) 2.061\n",
      "Train Epoch: 0 [80% 280/352]\tLoss: 0.921718\tData (t) 2.036\tBatch (t) 2.080\n",
      "Train Epoch: 0 [85% 300/352]\tLoss: 0.892968\tData (t) 2.033\tBatch (t) 2.083\n",
      "Train Epoch: 0 [91% 320/352]\tLoss: 1.035184\tData (t) 2.019\tBatch (t) 2.060\n",
      "Train Epoch: 0 [97% 340/352]\tLoss: 0.890812\tData (t) 2.030\tBatch (t) 2.085\n",
      "********************************************************************************\n",
      "Starting eval on validation split\n",
      "Val loss: 1.6565   Acc: 75.94: 100% 40/40 [00:41<00:00,  1.04s/it]\n",
      "Val acc at epoch 0: 75.94\n",
      "Saving model to ./config3_1.pt\n",
      "Train Epoch: 1 [0% 0/352]\tLoss: 0.662339\tData (t) 6.326\tBatch (t) 8.210\n",
      "Train Epoch: 1 [6% 20/352]\tLoss: 0.876097\tData (t) 1.776\tBatch (t) 2.036\n",
      "Train Epoch: 1 [11% 40/352]\tLoss: 0.792482\tData (t) 1.829\tBatch (t) 2.075\n",
      "Train Epoch: 1 [17% 60/352]\tLoss: 0.698151\tData (t) 2.021\tBatch (t) 2.053\n",
      "Train Epoch: 1 [23% 80/352]\tLoss: 0.829791\tData (t) 2.035\tBatch (t) 2.270\n",
      "Train Epoch: 1 [28% 100/352]\tLoss: 0.805837\tData (t) 1.808\tBatch (t) 1.840\n",
      "Train Epoch: 1 [34% 120/352]\tLoss: 0.722317\tData (t) 2.033\tBatch (t) 2.064\n",
      "Train Epoch: 1 [40% 140/352]\tLoss: 0.606036\tData (t) 2.032\tBatch (t) 2.063\n",
      "Train Epoch: 1 [45% 160/352]\tLoss: 0.829132\tData (t) 2.030\tBatch (t) 2.064\n",
      "Train Epoch: 1 [51% 180/352]\tLoss: 0.668275\tData (t) 2.035\tBatch (t) 2.069\n",
      "Train Epoch: 1 [57% 200/352]\tLoss: 0.805247\tData (t) 2.040\tBatch (t) 2.071\n",
      "Train Epoch: 1 [62% 220/352]\tLoss: 0.734003\tData (t) 1.944\tBatch (t) 2.138\n",
      "Train Epoch: 1 [68% 240/352]\tLoss: 0.647663\tData (t) 1.873\tBatch (t) 2.081\n",
      "Train Epoch: 1 [74% 260/352]\tLoss: 0.741500\tData (t) 1.865\tBatch (t) 2.063\n",
      "Train Epoch: 1 [80% 280/352]\tLoss: 0.784309\tData (t) 2.036\tBatch (t) 2.246\n",
      "Train Epoch: 1 [85% 300/352]\tLoss: 0.762425\tData (t) 2.031\tBatch (t) 2.062\n",
      "Train Epoch: 1 [91% 320/352]\tLoss: 0.716615\tData (t) 2.029\tBatch (t) 2.061\n",
      "Train Epoch: 1 [97% 340/352]\tLoss: 0.665906\tData (t) 2.038\tBatch (t) 2.080\n",
      "********************************************************************************\n",
      "Starting eval on validation split\n",
      "Val loss: 1.3520   Acc: 79.38: 100% 40/40 [00:35<00:00,  1.12it/s]\n",
      "Val acc at epoch 1: 79.38\n",
      "Saving model to ./config3_2.pt\n",
      "Train Epoch: 2 [0% 0/352]\tLoss: 0.500475\tData (t) 5.105\tBatch (t) 6.620\n",
      "Train Epoch: 2 [6% 20/352]\tLoss: 0.474187\tData (t) 2.046\tBatch (t) 2.077\n",
      "Train Epoch: 2 [11% 40/352]\tLoss: 0.465886\tData (t) 2.027\tBatch (t) 2.059\n",
      "Train Epoch: 2 [17% 60/352]\tLoss: 0.514993\tData (t) 2.040\tBatch (t) 2.075\n",
      "Train Epoch: 2 [23% 80/352]\tLoss: 0.456437\tData (t) 2.016\tBatch (t) 2.051\n",
      "Train Epoch: 2 [28% 100/352]\tLoss: 0.582185\tData (t) 2.032\tBatch (t) 2.230\n",
      "Train Epoch: 2 [34% 120/352]\tLoss: 0.607842\tData (t) 1.883\tBatch (t) 1.915\n",
      "Train Epoch: 2 [40% 140/352]\tLoss: 0.470312\tData (t) 2.037\tBatch (t) 2.069\n",
      "Train Epoch: 2 [45% 160/352]\tLoss: 0.590207\tData (t) 2.035\tBatch (t) 2.067\n",
      "Train Epoch: 2 [51% 180/352]\tLoss: 0.493021\tData (t) 2.035\tBatch (t) 2.067\n",
      "Train Epoch: 2 [57% 200/352]\tLoss: 0.451906\tData (t) 2.033\tBatch (t) 2.065\n",
      "Train Epoch: 2 [62% 220/352]\tLoss: 0.444617\tData (t) 2.031\tBatch (t) 2.228\n",
      "Train Epoch: 2 [68% 240/352]\tLoss: 0.466867\tData (t) 2.032\tBatch (t) 2.063\n",
      "Train Epoch: 2 [74% 260/352]\tLoss: 0.373290\tData (t) 2.033\tBatch (t) 2.066\n",
      "Train Epoch: 2 [80% 280/352]\tLoss: 0.384304\tData (t) 2.026\tBatch (t) 2.057\n",
      "Train Epoch: 2 [85% 300/352]\tLoss: 0.410113\tData (t) 2.031\tBatch (t) 2.067\n",
      "Train Epoch: 2 [91% 320/352]\tLoss: 0.537159\tData (t) 2.037\tBatch (t) 2.069\n",
      "Train Epoch: 2 [97% 340/352]\tLoss: 0.390819\tData (t) 2.033\tBatch (t) 2.065\n",
      "********************************************************************************\n",
      "Starting eval on validation split\n",
      "Val loss: 1.2622   Acc: 80.01: 100% 40/40 [00:36<00:00,  1.10it/s]\n",
      "Val acc at epoch 2: 80.01\n",
      "Saving model to ./config3_3.pt\n",
      "Train Epoch: 3 [0% 0/352]\tLoss: 0.241235\tData (t) 7.409\tBatch (t) 7.734\n",
      "Train Epoch: 3 [6% 20/352]\tLoss: 0.179509\tData (t) 1.890\tBatch (t) 1.922\n",
      "Train Epoch: 3 [11% 40/352]\tLoss: 0.203018\tData (t) 2.013\tBatch (t) 2.044\n",
      "Train Epoch: 3 [17% 60/352]\tLoss: 0.234291\tData (t) 2.042\tBatch (t) 2.073\n",
      "Train Epoch: 3 [23% 80/352]\tLoss: 0.268600\tData (t) 1.780\tBatch (t) 1.968\n",
      "Train Epoch: 3 [28% 100/352]\tLoss: 0.246853\tData (t) 1.784\tBatch (t) 2.110\n",
      "Train Epoch: 3 [34% 120/352]\tLoss: 0.251016\tData (t) 1.692\tBatch (t) 1.760\n",
      "Train Epoch: 3 [40% 140/352]\tLoss: 0.296930\tData (t) 1.788\tBatch (t) 2.233\n",
      "Train Epoch: 3 [45% 160/352]\tLoss: 0.252995\tData (t) 1.658\tBatch (t) 2.033\n",
      "Train Epoch: 3 [51% 180/352]\tLoss: 0.305707\tData (t) 1.760\tBatch (t) 1.813\n",
      "Train Epoch: 3 [57% 200/352]\tLoss: 0.306189\tData (t) 1.788\tBatch (t) 2.074\n",
      "Train Epoch: 3 [62% 220/352]\tLoss: 0.350857\tData (t) 1.843\tBatch (t) 1.993\n",
      "Train Epoch: 3 [68% 240/352]\tLoss: 0.191904\tData (t) 2.026\tBatch (t) 2.058\n",
      "Train Epoch: 3 [74% 260/352]\tLoss: 0.233573\tData (t) 1.820\tBatch (t) 2.031\n",
      "Train Epoch: 3 [80% 280/352]\tLoss: 0.167820\tData (t) 1.847\tBatch (t) 1.878\n",
      "Train Epoch: 3 [85% 300/352]\tLoss: 0.250414\tData (t) 2.030\tBatch (t) 2.062\n",
      "Train Epoch: 3 [91% 320/352]\tLoss: 0.258587\tData (t) 2.028\tBatch (t) 2.221\n",
      "Train Epoch: 3 [97% 340/352]\tLoss: 0.209613\tData (t) 2.037\tBatch (t) 2.071\n",
      "********************************************************************************\n",
      "Starting eval on validation split\n",
      "Val loss: 1.0777   Acc: 80.33: 100% 40/40 [00:38<00:00,  1.04it/s]\n",
      "Val acc at epoch 3: 80.33\n",
      "Saving model to ./config3_4.pt\n",
      "Train Epoch: 4 [0% 0/352]\tLoss: 0.124026\tData (t) 8.745\tBatch (t) 9.094\n",
      "Train Epoch: 4 [6% 20/352]\tLoss: 0.144066\tData (t) 1.898\tBatch (t) 2.076\n",
      "Train Epoch: 4 [11% 40/352]\tLoss: 0.145167\tData (t) 1.813\tBatch (t) 1.914\n",
      "Train Epoch: 4 [17% 60/352]\tLoss: 0.089334\tData (t) 2.042\tBatch (t) 2.073\n",
      "Train Epoch: 4 [23% 80/352]\tLoss: 0.151503\tData (t) 2.033\tBatch (t) 2.067\n",
      "Train Epoch: 4 [28% 100/352]\tLoss: 0.146599\tData (t) 1.736\tBatch (t) 2.061\n",
      "Train Epoch: 4 [34% 120/352]\tLoss: 0.071482\tData (t) 2.012\tBatch (t) 2.045\n",
      "Train Epoch: 4 [40% 140/352]\tLoss: 0.127730\tData (t) 1.997\tBatch (t) 2.039\n",
      "Train Epoch: 4 [45% 160/352]\tLoss: 0.157282\tData (t) 2.036\tBatch (t) 2.068\n",
      "Train Epoch: 4 [51% 180/352]\tLoss: 0.104406\tData (t) 2.039\tBatch (t) 2.071\n",
      "Train Epoch: 4 [57% 200/352]\tLoss: 0.126441\tData (t) 2.036\tBatch (t) 2.068\n",
      "Train Epoch: 4 [62% 220/352]\tLoss: 0.109649\tData (t) 2.035\tBatch (t) 2.067\n",
      "Train Epoch: 4 [68% 240/352]\tLoss: 0.104584\tData (t) 2.035\tBatch (t) 2.066\n",
      "Train Epoch: 4 [74% 260/352]\tLoss: 0.135807\tData (t) 2.033\tBatch (t) 2.231\n",
      "Train Epoch: 4 [80% 280/352]\tLoss: 0.091511\tData (t) 2.028\tBatch (t) 2.061\n",
      "Train Epoch: 4 [85% 300/352]\tLoss: 0.100324\tData (t) 2.033\tBatch (t) 2.066\n",
      "Train Epoch: 4 [91% 320/352]\tLoss: 0.113708\tData (t) 2.042\tBatch (t) 2.073\n",
      "Train Epoch: 4 [97% 340/352]\tLoss: 0.115919\tData (t) 2.035\tBatch (t) 2.065\n",
      "********************************************************************************\n",
      "Starting eval on validation split\n",
      "Val loss: 1.1246   Acc: 80.18: 100% 40/40 [00:43<00:00,  1.08s/it]\n",
      "Val acc at epoch 4: 80.18\n",
      "Saving model to ./config3_5.pt\n",
      "Train Epoch: 5 [0% 0/352]\tLoss: 0.104155\tData (t) 7.850\tBatch (t) 8.153\n",
      "Train Epoch: 5 [6% 20/352]\tLoss: 0.043968\tData (t) 1.870\tBatch (t) 1.901\n",
      "Train Epoch: 5 [11% 40/352]\tLoss: 0.076776\tData (t) 1.759\tBatch (t) 1.808\n",
      "Train Epoch: 5 [17% 60/352]\tLoss: 0.040138\tData (t) 1.865\tBatch (t) 2.067\n",
      "Train Epoch: 5 [23% 80/352]\tLoss: 0.046903\tData (t) 1.844\tBatch (t) 2.068\n",
      "Train Epoch: 5 [28% 100/352]\tLoss: 0.055030\tData (t) 1.834\tBatch (t) 2.140\n",
      "Train Epoch: 5 [34% 120/352]\tLoss: 0.042777\tData (t) 1.852\tBatch (t) 1.990\n",
      "Train Epoch: 5 [40% 140/352]\tLoss: 0.050542\tData (t) 1.803\tBatch (t) 1.923\n",
      "Train Epoch: 5 [45% 160/352]\tLoss: 0.029099\tData (t) 2.036\tBatch (t) 2.068\n",
      "Train Epoch: 5 [51% 180/352]\tLoss: 0.064202\tData (t) 2.034\tBatch (t) 2.234\n",
      "Train Epoch: 5 [57% 200/352]\tLoss: 0.065058\tData (t) 2.033\tBatch (t) 2.064\n",
      "Train Epoch: 5 [62% 220/352]\tLoss: 0.063958\tData (t) 2.033\tBatch (t) 2.064\n",
      "Train Epoch: 5 [68% 240/352]\tLoss: 0.050617\tData (t) 2.036\tBatch (t) 2.069\n",
      "Train Epoch: 5 [74% 260/352]\tLoss: 0.079222\tData (t) 1.881\tBatch (t) 2.082\n",
      "Train Epoch: 5 [80% 280/352]\tLoss: 0.037005\tData (t) 2.039\tBatch (t) 2.070\n",
      "Train Epoch: 5 [85% 300/352]\tLoss: 0.069549\tData (t) 2.038\tBatch (t) 2.232\n",
      "Train Epoch: 5 [91% 320/352]\tLoss: 0.036139\tData (t) 2.037\tBatch (t) 2.068\n",
      "Train Epoch: 5 [97% 340/352]\tLoss: 0.037980\tData (t) 2.042\tBatch (t) 2.073\n",
      "********************************************************************************\n",
      "Starting eval on validation split\n",
      "Val loss: 1.2523   Acc: 79.90: 100% 40/40 [00:40<00:00,  1.01s/it]\n",
      "Val acc at epoch 5: 79.90\n",
      "Saving model to ./config3_6.pt\n",
      "Train Epoch: 6 [0% 0/352]\tLoss: 0.017797\tData (t) 7.258\tBatch (t) 9.325\n",
      "Train Epoch: 6 [6% 20/352]\tLoss: 0.019833\tData (t) 1.919\tBatch (t) 2.001\n",
      "Train Epoch: 6 [11% 40/352]\tLoss: 0.020508\tData (t) 1.684\tBatch (t) 1.886\n",
      "Train Epoch: 6 [17% 60/352]\tLoss: 0.016267\tData (t) 1.851\tBatch (t) 2.110\n",
      "Train Epoch: 6 [23% 80/352]\tLoss: 0.031979\tData (t) 2.031\tBatch (t) 2.062\n",
      "Train Epoch: 6 [28% 100/352]\tLoss: 0.017781\tData (t) 2.035\tBatch (t) 2.066\n",
      "Train Epoch: 6 [34% 120/352]\tLoss: 0.014045\tData (t) 2.034\tBatch (t) 2.066\n",
      "Train Epoch: 6 [40% 140/352]\tLoss: 0.020653\tData (t) 2.040\tBatch (t) 2.073\n",
      "Train Epoch: 6 [45% 160/352]\tLoss: 0.019573\tData (t) 2.028\tBatch (t) 2.059\n",
      "Train Epoch: 6 [51% 180/352]\tLoss: 0.032023\tData (t) 1.842\tBatch (t) 2.067\n",
      "Train Epoch: 6 [57% 200/352]\tLoss: 0.019799\tData (t) 1.892\tBatch (t) 2.117\n",
      "Train Epoch: 6 [62% 220/352]\tLoss: 0.041664\tData (t) 1.863\tBatch (t) 1.894\n",
      "Train Epoch: 6 [68% 240/352]\tLoss: 0.014838\tData (t) 2.040\tBatch (t) 2.071\n",
      "Train Epoch: 6 [74% 260/352]\tLoss: 0.023943\tData (t) 2.032\tBatch (t) 2.065\n",
      "Train Epoch: 6 [80% 280/352]\tLoss: 0.030169\tData (t) 2.032\tBatch (t) 2.063\n",
      "Train Epoch: 6 [85% 300/352]\tLoss: 0.017972\tData (t) 2.037\tBatch (t) 2.068\n",
      "Train Epoch: 6 [91% 320/352]\tLoss: 0.030007\tData (t) 2.035\tBatch (t) 2.067\n",
      "Train Epoch: 6 [97% 340/352]\tLoss: 0.018398\tData (t) 2.031\tBatch (t) 2.063\n",
      "********************************************************************************\n",
      "Starting eval on validation split\n",
      "Val loss: 1.1844   Acc: 80.05: 100% 40/40 [00:35<00:00,  1.13it/s]\n",
      "Val acc at epoch 6: 80.05\n",
      "Saving model to ./config3_7.pt\n",
      "Train Epoch: 7 [0% 0/352]\tLoss: 0.008856\tData (t) 5.699\tBatch (t) 7.239\n",
      "Train Epoch: 7 [6% 20/352]\tLoss: 0.014491\tData (t) 1.848\tBatch (t) 2.105\n",
      "Train Epoch: 7 [11% 40/352]\tLoss: 0.017048\tData (t) 2.006\tBatch (t) 2.263\n",
      "Train Epoch: 7 [17% 60/352]\tLoss: 0.011247\tData (t) 2.051\tBatch (t) 2.092\n",
      "Train Epoch: 7 [23% 80/352]\tLoss: 0.007947\tData (t) 2.024\tBatch (t) 2.068\n",
      "Train Epoch: 7 [28% 100/352]\tLoss: 0.008474\tData (t) 1.875\tBatch (t) 2.018\n",
      "Train Epoch: 7 [34% 120/352]\tLoss: 0.008753\tData (t) 2.014\tBatch (t) 2.078\n",
      "Train Epoch: 7 [40% 140/352]\tLoss: 0.017390\tData (t) 2.016\tBatch (t) 2.051\n",
      "Train Epoch: 7 [45% 160/352]\tLoss: 0.009840\tData (t) 2.013\tBatch (t) 2.044\n",
      "Train Epoch: 7 [51% 180/352]\tLoss: 0.012144\tData (t) 2.024\tBatch (t) 2.055\n",
      "Train Epoch: 7 [57% 200/352]\tLoss: 0.015943\tData (t) 2.036\tBatch (t) 2.069\n",
      "Train Epoch: 7 [62% 220/352]\tLoss: 0.011105\tData (t) 2.013\tBatch (t) 2.065\n",
      "Train Epoch: 7 [68% 240/352]\tLoss: 0.007119\tData (t) 2.034\tBatch (t) 2.067\n",
      "Train Epoch: 7 [74% 260/352]\tLoss: 0.008718\tData (t) 2.037\tBatch (t) 2.068\n",
      "Train Epoch: 7 [80% 280/352]\tLoss: 0.027433\tData (t) 1.875\tBatch (t) 1.907\n",
      "Train Epoch: 7 [85% 300/352]\tLoss: 0.005524\tData (t) 2.043\tBatch (t) 2.074\n",
      "Train Epoch: 7 [91% 320/352]\tLoss: 0.006635\tData (t) 2.020\tBatch (t) 2.051\n",
      "Train Epoch: 7 [97% 340/352]\tLoss: 0.006588\tData (t) 2.038\tBatch (t) 2.069\n",
      "********************************************************************************\n",
      "Starting eval on validation split\n",
      "Val loss: 1.3090   Acc: 79.67: 100% 40/40 [00:39<00:00,  1.01it/s]\n",
      "Val acc at epoch 7: 79.67\n",
      "Saving model to ./config3_8.pt\n",
      "Train Epoch: 8 [0% 0/352]\tLoss: 0.004926\tData (t) 9.231\tBatch (t) 9.485\n",
      "Train Epoch: 8 [6% 20/352]\tLoss: 0.005028\tData (t) 2.073\tBatch (t) 2.109\n",
      "Train Epoch: 8 [11% 40/352]\tLoss: 0.005412\tData (t) 1.945\tBatch (t) 1.987\n",
      "Train Epoch: 8 [17% 60/352]\tLoss: 0.004831\tData (t) 1.768\tBatch (t) 1.800\n",
      "Train Epoch: 8 [23% 80/352]\tLoss: 0.003969\tData (t) 1.730\tBatch (t) 1.774\n",
      "Train Epoch: 8 [28% 100/352]\tLoss: 0.005127\tData (t) 1.705\tBatch (t) 1.840\n",
      "Train Epoch: 8 [34% 120/352]\tLoss: 0.004951\tData (t) 1.744\tBatch (t) 1.962\n",
      "Train Epoch: 8 [40% 140/352]\tLoss: 0.008145\tData (t) 1.827\tBatch (t) 2.037\n",
      "Train Epoch: 8 [45% 160/352]\tLoss: 0.004193\tData (t) 1.879\tBatch (t) 2.115\n",
      "Train Epoch: 8 [51% 180/352]\tLoss: 0.004646\tData (t) 1.834\tBatch (t) 2.169\n",
      "Train Epoch: 8 [57% 200/352]\tLoss: 0.009706\tData (t) 1.851\tBatch (t) 2.104\n",
      "Train Epoch: 8 [62% 220/352]\tLoss: 0.005624\tData (t) 2.036\tBatch (t) 2.085\n",
      "Train Epoch: 8 [68% 240/352]\tLoss: 0.004888\tData (t) 2.038\tBatch (t) 2.096\n",
      "Train Epoch: 8 [74% 260/352]\tLoss: 0.006764\tData (t) 2.037\tBatch (t) 2.083\n",
      "Train Epoch: 8 [80% 280/352]\tLoss: 0.004942\tData (t) 2.037\tBatch (t) 2.092\n",
      "Train Epoch: 8 [85% 300/352]\tLoss: 0.004289\tData (t) 1.876\tBatch (t) 2.116\n",
      "Train Epoch: 8 [91% 320/352]\tLoss: 0.005942\tData (t) 1.781\tBatch (t) 1.841\n",
      "Train Epoch: 8 [97% 340/352]\tLoss: 0.013199\tData (t) 2.022\tBatch (t) 2.055\n",
      "********************************************************************************\n",
      "Starting eval on validation split\n",
      "Val loss: 1.3488   Acc: 79.62: 100% 40/40 [00:39<00:00,  1.01it/s]\n",
      "Val acc at epoch 8: 79.62\n",
      "Saving model to ./config3_9.pt\n",
      "Train Epoch: 9 [0% 0/352]\tLoss: 0.004960\tData (t) 5.250\tBatch (t) 5.552\n",
      "Train Epoch: 9 [6% 20/352]\tLoss: 0.004220\tData (t) 2.083\tBatch (t) 2.114\n",
      "Train Epoch: 9 [11% 40/352]\tLoss: 0.005347\tData (t) 2.000\tBatch (t) 2.032\n",
      "Train Epoch: 9 [17% 60/352]\tLoss: 0.005048\tData (t) 1.990\tBatch (t) 2.077\n",
      "Train Epoch: 9 [23% 80/352]\tLoss: 0.007481\tData (t) 1.984\tBatch (t) 2.015\n",
      "Train Epoch: 9 [28% 100/352]\tLoss: 0.003823\tData (t) 1.804\tBatch (t) 1.845\n",
      "Train Epoch: 9 [34% 120/352]\tLoss: 0.004383\tData (t) 1.850\tBatch (t) 1.937\n",
      "Train Epoch: 9 [40% 140/352]\tLoss: 0.004129\tData (t) 1.844\tBatch (t) 2.108\n",
      "Train Epoch: 9 [45% 160/352]\tLoss: 0.003237\tData (t) 1.793\tBatch (t) 2.114\n",
      "Train Epoch: 9 [51% 180/352]\tLoss: 0.003658\tData (t) 1.830\tBatch (t) 2.180\n",
      "Train Epoch: 9 [57% 200/352]\tLoss: 0.004207\tData (t) 1.873\tBatch (t) 2.122\n",
      "Train Epoch: 9 [62% 220/352]\tLoss: 0.004571\tData (t) 1.881\tBatch (t) 2.132\n",
      "Train Epoch: 9 [68% 240/352]\tLoss: 0.004306\tData (t) 2.038\tBatch (t) 2.069\n",
      "Train Epoch: 9 [74% 260/352]\tLoss: 0.003941\tData (t) 2.033\tBatch (t) 2.095\n",
      "Train Epoch: 9 [80% 280/352]\tLoss: 0.002747\tData (t) 2.023\tBatch (t) 2.064\n",
      "Train Epoch: 9 [85% 300/352]\tLoss: 0.004346\tData (t) 2.038\tBatch (t) 2.080\n",
      "Train Epoch: 9 [91% 320/352]\tLoss: 0.004871\tData (t) 2.037\tBatch (t) 2.262\n",
      "Train Epoch: 9 [97% 340/352]\tLoss: 0.004017\tData (t) 2.037\tBatch (t) 2.082\n",
      "********************************************************************************\n",
      "Starting eval on validation split\n",
      "Val loss: 1.3658   Acc: 79.52: 100% 40/40 [00:35<00:00,  1.14it/s]\n",
      "Val acc at epoch 9: 79.52\n",
      "Saving model to ./config3_10.pt\n",
      "✅ Configuration 3 completed and backed up to Drive!\n"
     ]
    }
   ],
   "source": [
    "# Configuration 3: lr=3e-6, wd=0.1, epochs=10, batch_size=256, timm_aug=False\n",
    "!python $CODE_DIR/finetune.py --lr 3e-6 --wd 0.1 --epochs 10 --batch-size 256 --data-location $DATA_DIR --name \"config3\"\n",
    "\n",
    "# Backup model to Google Drive\n",
    "!cp config3_*.pt \"/content/drive/MyDrive/Colab Notebooks/\"\n",
    "print(\"✅ Configuration 3 completed and backed up to Drive!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8273909,
     "status": "ok",
     "timestamp": 1755952419943,
     "user": {
      "displayName": "Yang Zhou",
      "userId": "09717432835810465973"
     },
     "user_tz": 240
    },
    "id": "QOoQJlw0tmwX",
    "outputId": "3ee05ee5-126e-4b11-c498-5409fc26d580"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(\n",
      "Building zero-shot classifier.\n",
      "100% 200/200 [00:12<00:00, 16.02it/s]\n",
      "Saving model to ./config4_0.pt\n",
      "Train Epoch: 0 [0% 0/352]\tLoss: 3.308727\tData (t) 15.836\tBatch (t) 23.652\n",
      "Train Epoch: 0 [6% 20/352]\tLoss: 2.737835\tData (t) 1.325\tBatch (t) 1.400\n",
      "Train Epoch: 0 [11% 40/352]\tLoss: 2.778366\tData (t) 1.858\tBatch (t) 2.029\n",
      "Train Epoch: 0 [17% 60/352]\tLoss: 2.296328\tData (t) 1.971\tBatch (t) 2.070\n",
      "Train Epoch: 0 [23% 80/352]\tLoss: 2.271421\tData (t) 1.982\tBatch (t) 2.036\n",
      "Train Epoch: 0 [28% 100/352]\tLoss: 2.170868\tData (t) 2.007\tBatch (t) 2.054\n",
      "Train Epoch: 0 [34% 120/352]\tLoss: 2.146047\tData (t) 1.986\tBatch (t) 2.038\n",
      "Train Epoch: 0 [40% 140/352]\tLoss: 1.899604\tData (t) 1.807\tBatch (t) 1.855\n",
      "Train Epoch: 0 [45% 160/352]\tLoss: 2.142196\tData (t) 1.999\tBatch (t) 2.049\n",
      "Train Epoch: 0 [51% 180/352]\tLoss: 2.240858\tData (t) 1.990\tBatch (t) 2.039\n",
      "Train Epoch: 0 [57% 200/352]\tLoss: 2.136600\tData (t) 1.904\tBatch (t) 1.955\n",
      "Train Epoch: 0 [62% 220/352]\tLoss: 2.052347\tData (t) 1.987\tBatch (t) 2.036\n",
      "Train Epoch: 0 [68% 240/352]\tLoss: 1.901188\tData (t) 1.763\tBatch (t) 2.169\n",
      "Train Epoch: 0 [74% 260/352]\tLoss: 2.136467\tData (t) 1.975\tBatch (t) 2.050\n",
      "Train Epoch: 0 [80% 280/352]\tLoss: 1.842118\tData (t) 1.984\tBatch (t) 2.053\n",
      "Train Epoch: 0 [85% 300/352]\tLoss: 2.115879\tData (t) 1.965\tBatch (t) 2.015\n",
      "Train Epoch: 0 [91% 320/352]\tLoss: 2.066776\tData (t) 1.974\tBatch (t) 2.022\n",
      "Train Epoch: 0 [97% 340/352]\tLoss: 1.872259\tData (t) 2.027\tBatch (t) 2.310\n",
      "********************************************************************************\n",
      "Starting eval on validation split\n",
      "Val loss: 1.9575   Acc: 53.97: 100% 40/40 [00:50<00:00,  1.26s/it]\n",
      "Val acc at epoch 0: 53.97\n",
      "Saving model to ./config4_1.pt\n",
      "Train Epoch: 1 [0% 0/352]\tLoss: 1.956558\tData (t) 10.727\tBatch (t) 11.194\n",
      "Train Epoch: 1 [6% 20/352]\tLoss: 2.178433\tData (t) 2.043\tBatch (t) 2.116\n",
      "Train Epoch: 1 [11% 40/352]\tLoss: 1.847307\tData (t) 2.014\tBatch (t) 2.082\n",
      "Train Epoch: 1 [17% 60/352]\tLoss: 1.907821\tData (t) 2.007\tBatch (t) 2.107\n",
      "Train Epoch: 1 [23% 80/352]\tLoss: 1.891371\tData (t) 2.023\tBatch (t) 2.093\n",
      "Train Epoch: 1 [28% 100/352]\tLoss: 1.887058\tData (t) 1.979\tBatch (t) 2.337\n",
      "Train Epoch: 1 [34% 120/352]\tLoss: 2.008329\tData (t) 2.035\tBatch (t) 2.067\n",
      "Train Epoch: 1 [40% 140/352]\tLoss: 1.864542\tData (t) 2.040\tBatch (t) 2.071\n",
      "Train Epoch: 1 [45% 160/352]\tLoss: 1.818202\tData (t) 2.043\tBatch (t) 2.074\n",
      "Train Epoch: 1 [51% 180/352]\tLoss: 1.858009\tData (t) 2.036\tBatch (t) 2.077\n",
      "Train Epoch: 1 [57% 200/352]\tLoss: 1.899758\tData (t) 2.020\tBatch (t) 2.052\n",
      "Train Epoch: 1 [62% 220/352]\tLoss: 1.651940\tData (t) 1.989\tBatch (t) 2.022\n",
      "Train Epoch: 1 [68% 240/352]\tLoss: 1.796330\tData (t) 1.992\tBatch (t) 2.026\n",
      "Train Epoch: 1 [74% 260/352]\tLoss: 1.735942\tData (t) 1.975\tBatch (t) 2.029\n",
      "Train Epoch: 1 [80% 280/352]\tLoss: 1.754004\tData (t) 2.022\tBatch (t) 2.281\n",
      "Train Epoch: 1 [85% 300/352]\tLoss: 1.626352\tData (t) 1.702\tBatch (t) 2.187\n",
      "Train Epoch: 1 [91% 320/352]\tLoss: 1.884002\tData (t) 1.781\tBatch (t) 2.117\n",
      "Train Epoch: 1 [97% 340/352]\tLoss: 1.685472\tData (t) 2.019\tBatch (t) 2.067\n",
      "********************************************************************************\n",
      "Starting eval on validation split\n",
      "Val loss: 1.3504   Acc: 57.07: 100% 40/40 [00:57<00:00,  1.43s/it]\n",
      "Val acc at epoch 1: 57.07\n",
      "Saving model to ./config4_2.pt\n",
      "Train Epoch: 2 [0% 0/352]\tLoss: 1.444750\tData (t) 7.478\tBatch (t) 7.925\n",
      "Train Epoch: 2 [6% 20/352]\tLoss: 1.481389\tData (t) 1.289\tBatch (t) 1.986\n",
      "Train Epoch: 2 [11% 40/352]\tLoss: 1.772088\tData (t) 1.557\tBatch (t) 2.048\n",
      "Train Epoch: 2 [17% 60/352]\tLoss: 1.659999\tData (t) 2.054\tBatch (t) 2.085\n",
      "Train Epoch: 2 [23% 80/352]\tLoss: 1.681903\tData (t) 2.037\tBatch (t) 2.286\n",
      "Train Epoch: 2 [28% 100/352]\tLoss: 1.679308\tData (t) 1.734\tBatch (t) 1.791\n",
      "Train Epoch: 2 [34% 120/352]\tLoss: 1.922280\tData (t) 1.967\tBatch (t) 2.015\n",
      "Train Epoch: 2 [40% 140/352]\tLoss: 1.659079\tData (t) 2.018\tBatch (t) 2.049\n",
      "Train Epoch: 2 [45% 160/352]\tLoss: 1.697760\tData (t) 2.025\tBatch (t) 2.058\n",
      "Train Epoch: 2 [51% 180/352]\tLoss: 1.678767\tData (t) 2.026\tBatch (t) 2.058\n",
      "Train Epoch: 2 [57% 200/352]\tLoss: 1.446963\tData (t) 2.026\tBatch (t) 2.066\n",
      "Train Epoch: 2 [62% 220/352]\tLoss: 1.560400\tData (t) 1.969\tBatch (t) 2.013\n",
      "Train Epoch: 2 [68% 240/352]\tLoss: 1.698651\tData (t) 2.004\tBatch (t) 2.052\n",
      "Train Epoch: 2 [74% 260/352]\tLoss: 1.746687\tData (t) 1.969\tBatch (t) 2.029\n",
      "Train Epoch: 2 [80% 280/352]\tLoss: 1.438398\tData (t) 1.996\tBatch (t) 2.050\n",
      "Train Epoch: 2 [85% 300/352]\tLoss: 1.293065\tData (t) 2.005\tBatch (t) 2.053\n",
      "Train Epoch: 2 [91% 320/352]\tLoss: 1.542468\tData (t) 1.975\tBatch (t) 2.041\n",
      "Train Epoch: 2 [97% 340/352]\tLoss: 1.778425\tData (t) 2.018\tBatch (t) 2.232\n",
      "********************************************************************************\n",
      "Starting eval on validation split\n",
      "Val loss: 1.3550   Acc: 59.94: 100% 40/40 [00:51<00:00,  1.30s/it]\n",
      "Val acc at epoch 2: 59.94\n",
      "Saving model to ./config4_3.pt\n",
      "Train Epoch: 3 [0% 0/352]\tLoss: 1.441822\tData (t) 11.329\tBatch (t) 11.633\n",
      "Train Epoch: 3 [6% 20/352]\tLoss: 1.489492\tData (t) 2.071\tBatch (t) 2.121\n",
      "Train Epoch: 3 [11% 40/352]\tLoss: 1.481040\tData (t) 1.992\tBatch (t) 2.024\n",
      "Train Epoch: 3 [17% 60/352]\tLoss: 1.406862\tData (t) 2.037\tBatch (t) 2.084\n",
      "Train Epoch: 3 [23% 80/352]\tLoss: 1.347749\tData (t) 1.655\tBatch (t) 2.227\n",
      "Train Epoch: 3 [28% 100/352]\tLoss: 1.500801\tData (t) 1.653\tBatch (t) 2.090\n",
      "Train Epoch: 3 [34% 120/352]\tLoss: 1.531621\tData (t) 1.842\tBatch (t) 2.084\n",
      "Train Epoch: 3 [40% 140/352]\tLoss: 1.461180\tData (t) 1.916\tBatch (t) 2.181\n",
      "Train Epoch: 3 [45% 160/352]\tLoss: 1.281556\tData (t) 2.044\tBatch (t) 2.354\n",
      "Train Epoch: 3 [51% 180/352]\tLoss: 1.694699\tData (t) 2.027\tBatch (t) 2.079\n",
      "Train Epoch: 3 [57% 200/352]\tLoss: 1.542373\tData (t) 2.030\tBatch (t) 2.062\n",
      "Train Epoch: 3 [62% 220/352]\tLoss: 1.625910\tData (t) 2.018\tBatch (t) 2.051\n",
      "Train Epoch: 3 [68% 240/352]\tLoss: 1.479071\tData (t) 2.024\tBatch (t) 2.055\n",
      "Train Epoch: 3 [74% 260/352]\tLoss: 1.236661\tData (t) 2.020\tBatch (t) 2.423\n",
      "Train Epoch: 3 [80% 280/352]\tLoss: 1.550960\tData (t) 1.778\tBatch (t) 2.102\n",
      "Train Epoch: 3 [85% 300/352]\tLoss: 1.617999\tData (t) 2.041\tBatch (t) 2.076\n",
      "Train Epoch: 3 [91% 320/352]\tLoss: 1.399518\tData (t) 1.450\tBatch (t) 2.185\n",
      "Train Epoch: 3 [97% 340/352]\tLoss: 1.434320\tData (t) 2.034\tBatch (t) 2.065\n",
      "********************************************************************************\n",
      "Starting eval on validation split\n",
      "Val loss: 1.1451   Acc: 60.94: 100% 40/40 [00:53<00:00,  1.34s/it]\n",
      "Val acc at epoch 3: 60.94\n",
      "Saving model to ./config4_4.pt\n",
      "Train Epoch: 4 [0% 0/352]\tLoss: 1.127196\tData (t) 9.255\tBatch (t) 9.540\n",
      "Train Epoch: 4 [6% 20/352]\tLoss: 1.339646\tData (t) 2.036\tBatch (t) 2.068\n",
      "Train Epoch: 4 [11% 40/352]\tLoss: 1.363666\tData (t) 2.002\tBatch (t) 2.033\n",
      "Train Epoch: 4 [17% 60/352]\tLoss: 1.348864\tData (t) 2.030\tBatch (t) 2.062\n",
      "Train Epoch: 4 [23% 80/352]\tLoss: 1.283372\tData (t) 2.016\tBatch (t) 2.068\n",
      "Train Epoch: 4 [28% 100/352]\tLoss: 1.349254\tData (t) 2.015\tBatch (t) 2.063\n",
      "Train Epoch: 4 [34% 120/352]\tLoss: 1.171003\tData (t) 2.017\tBatch (t) 2.067\n",
      "Train Epoch: 4 [40% 140/352]\tLoss: 1.341178\tData (t) 1.998\tBatch (t) 2.046\n",
      "Train Epoch: 4 [45% 160/352]\tLoss: 1.428798\tData (t) 2.003\tBatch (t) 2.052\n",
      "Train Epoch: 4 [51% 180/352]\tLoss: 1.264622\tData (t) 1.981\tBatch (t) 2.028\n",
      "Train Epoch: 4 [57% 200/352]\tLoss: 1.226815\tData (t) 1.965\tBatch (t) 2.014\n",
      "Train Epoch: 4 [62% 220/352]\tLoss: 1.585469\tData (t) 1.767\tBatch (t) 1.950\n",
      "Train Epoch: 4 [68% 240/352]\tLoss: 1.328398\tData (t) 1.951\tBatch (t) 2.101\n",
      "Train Epoch: 4 [74% 260/352]\tLoss: 1.315127\tData (t) 1.977\tBatch (t) 2.020\n",
      "Train Epoch: 4 [80% 280/352]\tLoss: 1.152453\tData (t) 1.935\tBatch (t) 2.105\n",
      "Train Epoch: 4 [85% 300/352]\tLoss: 1.219202\tData (t) 1.701\tBatch (t) 2.285\n",
      "Train Epoch: 4 [91% 320/352]\tLoss: 1.295661\tData (t) 2.033\tBatch (t) 2.109\n",
      "Train Epoch: 4 [97% 340/352]\tLoss: 1.408458\tData (t) 1.850\tBatch (t) 2.046\n",
      "********************************************************************************\n",
      "Starting eval on validation split\n",
      "Val loss: 1.7125   Acc: 63.50: 100% 40/40 [00:50<00:00,  1.27s/it]\n",
      "Val acc at epoch 4: 63.50\n",
      "Saving model to ./config4_5.pt\n",
      "Train Epoch: 5 [0% 0/352]\tLoss: 1.287405\tData (t) 8.793\tBatch (t) 9.279\n",
      "Train Epoch: 5 [6% 20/352]\tLoss: 0.995261\tData (t) 2.078\tBatch (t) 2.155\n",
      "Train Epoch: 5 [11% 40/352]\tLoss: 1.334487\tData (t) 2.013\tBatch (t) 2.064\n",
      "Train Epoch: 5 [17% 60/352]\tLoss: 1.190423\tData (t) 2.047\tBatch (t) 2.079\n",
      "Train Epoch: 5 [23% 80/352]\tLoss: 1.226589\tData (t) 2.013\tBatch (t) 2.065\n",
      "Train Epoch: 5 [28% 100/352]\tLoss: 1.194247\tData (t) 1.506\tBatch (t) 2.065\n",
      "Train Epoch: 5 [34% 120/352]\tLoss: 1.223211\tData (t) 2.040\tBatch (t) 2.355\n",
      "Train Epoch: 5 [40% 140/352]\tLoss: 1.161648\tData (t) 1.416\tBatch (t) 2.327\n",
      "Train Epoch: 5 [45% 160/352]\tLoss: 1.320092\tData (t) 1.943\tBatch (t) 1.990\n",
      "Train Epoch: 5 [51% 180/352]\tLoss: 1.401924\tData (t) 1.924\tBatch (t) 1.993\n",
      "Train Epoch: 5 [57% 200/352]\tLoss: 1.151597\tData (t) 1.921\tBatch (t) 2.034\n",
      "Train Epoch: 5 [62% 220/352]\tLoss: 1.009274\tData (t) 1.718\tBatch (t) 1.891\n",
      "Train Epoch: 5 [68% 240/352]\tLoss: 1.144514\tData (t) 1.984\tBatch (t) 2.032\n",
      "Train Epoch: 5 [74% 260/352]\tLoss: 1.070089\tData (t) 1.747\tBatch (t) 1.891\n",
      "Train Epoch: 5 [80% 280/352]\tLoss: 1.102438\tData (t) 2.001\tBatch (t) 2.054\n",
      "Train Epoch: 5 [85% 300/352]\tLoss: 1.127967\tData (t) 2.028\tBatch (t) 2.076\n",
      "Train Epoch: 5 [91% 320/352]\tLoss: 1.105364\tData (t) 1.734\tBatch (t) 2.040\n",
      "Train Epoch: 5 [97% 340/352]\tLoss: 1.204669\tData (t) 1.971\tBatch (t) 2.107\n",
      "********************************************************************************\n",
      "Starting eval on validation split\n",
      "Val loss: 1.1996   Acc: 64.81: 100% 40/40 [00:48<00:00,  1.22s/it]\n",
      "Val acc at epoch 5: 64.81\n",
      "Saving model to ./config4_6.pt\n",
      "Train Epoch: 6 [0% 0/352]\tLoss: 0.871250\tData (t) 9.583\tBatch (t) 10.209\n",
      "Train Epoch: 6 [6% 20/352]\tLoss: 1.195513\tData (t) 1.884\tBatch (t) 2.202\n",
      "Train Epoch: 6 [11% 40/352]\tLoss: 1.197906\tData (t) 1.570\tBatch (t) 1.674\n",
      "Train Epoch: 6 [17% 60/352]\tLoss: 1.014491\tData (t) 2.037\tBatch (t) 2.092\n",
      "Train Epoch: 6 [23% 80/352]\tLoss: 0.958766\tData (t) 2.010\tBatch (t) 2.101\n",
      "Train Epoch: 6 [28% 100/352]\tLoss: 1.082414\tData (t) 2.026\tBatch (t) 2.113\n",
      "Train Epoch: 6 [34% 120/352]\tLoss: 0.966647\tData (t) 1.540\tBatch (t) 1.925\n",
      "Train Epoch: 6 [40% 140/352]\tLoss: 0.992274\tData (t) 1.715\tBatch (t) 1.818\n",
      "Train Epoch: 6 [45% 160/352]\tLoss: 1.128404\tData (t) 2.025\tBatch (t) 2.092\n",
      "Train Epoch: 6 [51% 180/352]\tLoss: 1.124886\tData (t) 2.039\tBatch (t) 2.105\n",
      "Train Epoch: 6 [57% 200/352]\tLoss: 1.059141\tData (t) 2.033\tBatch (t) 2.075\n",
      "Train Epoch: 6 [62% 220/352]\tLoss: 1.014565\tData (t) 2.043\tBatch (t) 2.074\n",
      "Train Epoch: 6 [68% 240/352]\tLoss: 0.920213\tData (t) 2.043\tBatch (t) 2.075\n",
      "Train Epoch: 6 [74% 260/352]\tLoss: 1.144994\tData (t) 1.424\tBatch (t) 1.693\n",
      "Train Epoch: 6 [80% 280/352]\tLoss: 1.293874\tData (t) 1.690\tBatch (t) 1.980\n",
      "Train Epoch: 6 [85% 300/352]\tLoss: 0.915213\tData (t) 1.677\tBatch (t) 1.740\n",
      "Train Epoch: 6 [91% 320/352]\tLoss: 1.080406\tData (t) 1.436\tBatch (t) 2.244\n",
      "Train Epoch: 6 [97% 340/352]\tLoss: 1.027332\tData (t) 2.026\tBatch (t) 2.057\n",
      "********************************************************************************\n",
      "Starting eval on validation split\n",
      "Val loss: 0.7815   Acc: 65.85: 100% 40/40 [00:55<00:00,  1.39s/it]\n",
      "Val acc at epoch 6: 65.85\n",
      "Saving model to ./config4_7.pt\n",
      "Train Epoch: 7 [0% 0/352]\tLoss: 0.807386\tData (t) 14.085\tBatch (t) 15.128\n",
      "Train Epoch: 7 [6% 20/352]\tLoss: 1.099274\tData (t) 1.791\tBatch (t) 1.905\n",
      "Train Epoch: 7 [11% 40/352]\tLoss: 0.961717\tData (t) 1.953\tBatch (t) 2.075\n",
      "Train Epoch: 7 [17% 60/352]\tLoss: 0.977368\tData (t) 1.792\tBatch (t) 2.205\n",
      "Train Epoch: 7 [23% 80/352]\tLoss: 0.888976\tData (t) 2.032\tBatch (t) 2.134\n",
      "Train Epoch: 7 [28% 100/352]\tLoss: 1.155100\tData (t) 2.031\tBatch (t) 2.086\n",
      "Train Epoch: 7 [34% 120/352]\tLoss: 0.938048\tData (t) 2.035\tBatch (t) 2.067\n",
      "Train Epoch: 7 [40% 140/352]\tLoss: 0.768538\tData (t) 1.595\tBatch (t) 1.899\n",
      "Train Epoch: 7 [45% 160/352]\tLoss: 1.150117\tData (t) 1.889\tBatch (t) 2.241\n",
      "Train Epoch: 7 [51% 180/352]\tLoss: 1.050574\tData (t) 1.645\tBatch (t) 2.152\n",
      "Train Epoch: 7 [57% 200/352]\tLoss: 0.966412\tData (t) 1.568\tBatch (t) 2.281\n",
      "Train Epoch: 7 [62% 220/352]\tLoss: 1.029784\tData (t) 1.734\tBatch (t) 2.030\n",
      "Train Epoch: 7 [68% 240/352]\tLoss: 0.722690\tData (t) 1.677\tBatch (t) 1.853\n",
      "Train Epoch: 7 [74% 260/352]\tLoss: 0.792969\tData (t) 2.020\tBatch (t) 2.075\n",
      "Train Epoch: 7 [80% 280/352]\tLoss: 0.871943\tData (t) 1.575\tBatch (t) 2.022\n",
      "Train Epoch: 7 [85% 300/352]\tLoss: 0.851016\tData (t) 1.980\tBatch (t) 2.030\n",
      "Train Epoch: 7 [91% 320/352]\tLoss: 0.986487\tData (t) 1.965\tBatch (t) 2.016\n",
      "Train Epoch: 7 [97% 340/352]\tLoss: 1.025280\tData (t) 2.023\tBatch (t) 2.072\n",
      "********************************************************************************\n",
      "Starting eval on validation split\n",
      "Val loss: 1.0353   Acc: 67.00: 100% 40/40 [00:55<00:00,  1.38s/it]\n",
      "Val acc at epoch 7: 67.00\n",
      "Saving model to ./config4_8.pt\n",
      "Train Epoch: 8 [0% 0/352]\tLoss: 0.908187\tData (t) 10.147\tBatch (t) 11.712\n",
      "Train Epoch: 8 [6% 20/352]\tLoss: 0.897846\tData (t) 1.725\tBatch (t) 2.164\n",
      "Train Epoch: 8 [11% 40/352]\tLoss: 0.931017\tData (t) 1.819\tBatch (t) 2.193\n",
      "Train Epoch: 8 [17% 60/352]\tLoss: 0.921625\tData (t) 1.899\tBatch (t) 2.206\n",
      "Train Epoch: 8 [23% 80/352]\tLoss: 0.744111\tData (t) 2.038\tBatch (t) 2.360\n",
      "Train Epoch: 8 [28% 100/352]\tLoss: 0.589181\tData (t) 1.834\tBatch (t) 2.024\n",
      "Train Epoch: 8 [34% 120/352]\tLoss: 1.035614\tData (t) 1.696\tBatch (t) 2.078\n",
      "Train Epoch: 8 [40% 140/352]\tLoss: 0.997585\tData (t) 1.673\tBatch (t) 2.280\n",
      "Train Epoch: 8 [45% 160/352]\tLoss: 0.866436\tData (t) 1.656\tBatch (t) 2.240\n",
      "Train Epoch: 8 [51% 180/352]\tLoss: 0.879002\tData (t) 1.658\tBatch (t) 1.707\n",
      "Train Epoch: 8 [57% 200/352]\tLoss: 0.952779\tData (t) 1.501\tBatch (t) 2.040\n",
      "Train Epoch: 8 [62% 220/352]\tLoss: 0.883705\tData (t) 1.740\tBatch (t) 2.218\n",
      "Train Epoch: 8 [68% 240/352]\tLoss: 0.827332\tData (t) 1.477\tBatch (t) 1.775\n",
      "Train Epoch: 8 [74% 260/352]\tLoss: 0.905580\tData (t) 2.024\tBatch (t) 2.055\n",
      "Train Epoch: 8 [80% 280/352]\tLoss: 0.888685\tData (t) 1.894\tBatch (t) 2.217\n",
      "Train Epoch: 8 [85% 300/352]\tLoss: 0.956931\tData (t) 1.779\tBatch (t) 2.163\n",
      "Train Epoch: 8 [91% 320/352]\tLoss: 0.984040\tData (t) 1.194\tBatch (t) 1.901\n",
      "Train Epoch: 8 [97% 340/352]\tLoss: 0.974589\tData (t) 2.037\tBatch (t) 2.068\n",
      "********************************************************************************\n",
      "Starting eval on validation split\n",
      "Val loss: 0.9735   Acc: 67.86: 100% 40/40 [00:51<00:00,  1.29s/it]\n",
      "Val acc at epoch 8: 67.86\n",
      "Saving model to ./config4_9.pt\n",
      "Train Epoch: 9 [0% 0/352]\tLoss: 1.043026\tData (t) 13.462\tBatch (t) 14.231\n",
      "Train Epoch: 9 [6% 20/352]\tLoss: 1.120823\tData (t) 2.010\tBatch (t) 2.096\n",
      "Train Epoch: 9 [11% 40/352]\tLoss: 0.731964\tData (t) 1.948\tBatch (t) 1.995\n",
      "Train Epoch: 9 [17% 60/352]\tLoss: 0.755099\tData (t) 2.018\tBatch (t) 2.069\n",
      "Train Epoch: 9 [23% 80/352]\tLoss: 1.034172\tData (t) 2.017\tBatch (t) 2.063\n",
      "Train Epoch: 9 [28% 100/352]\tLoss: 0.860669\tData (t) 2.016\tBatch (t) 2.063\n",
      "Train Epoch: 9 [34% 120/352]\tLoss: 0.900929\tData (t) 1.988\tBatch (t) 2.040\n",
      "Train Epoch: 9 [40% 140/352]\tLoss: 0.752061\tData (t) 1.833\tBatch (t) 2.194\n",
      "Train Epoch: 9 [45% 160/352]\tLoss: 1.017275\tData (t) 2.043\tBatch (t) 2.119\n",
      "Train Epoch: 9 [51% 180/352]\tLoss: 0.831608\tData (t) 2.038\tBatch (t) 2.144\n",
      "Train Epoch: 9 [57% 200/352]\tLoss: 0.802452\tData (t) 2.042\tBatch (t) 2.092\n",
      "Train Epoch: 9 [62% 220/352]\tLoss: 0.819054\tData (t) 1.726\tBatch (t) 1.931\n",
      "Train Epoch: 9 [68% 240/352]\tLoss: 0.706940\tData (t) 2.039\tBatch (t) 2.070\n",
      "Train Epoch: 9 [74% 260/352]\tLoss: 0.825359\tData (t) 2.022\tBatch (t) 2.570\n",
      "Train Epoch: 9 [80% 280/352]\tLoss: 0.866080\tData (t) 2.038\tBatch (t) 2.070\n",
      "Train Epoch: 9 [85% 300/352]\tLoss: 0.950668\tData (t) 2.035\tBatch (t) 2.066\n",
      "Train Epoch: 9 [91% 320/352]\tLoss: 0.943301\tData (t) 2.032\tBatch (t) 2.063\n",
      "Train Epoch: 9 [97% 340/352]\tLoss: 0.966110\tData (t) 2.036\tBatch (t) 2.068\n",
      "********************************************************************************\n",
      "Starting eval on validation split\n",
      "Val loss: 0.8298   Acc: 67.90: 100% 40/40 [00:49<00:00,  1.24s/it]\n",
      "Val acc at epoch 9: 67.90\n",
      "Saving model to ./config4_10.pt\n",
      "✅ Configuration 4 completed and backed up to Drive!\n"
     ]
    }
   ],
   "source": [
    "# Configuration 4: lr=2e-5, wd=1e-3, epochs=10, batch_size=256, timm_aug=True\n",
    "!python $CODE_DIR/finetune.py --lr 2e-5 --wd 1e-3 --epochs 10 --batch-size 256 --timm-aug --data-location $DATA_DIR --name \"config4\"\n",
    "\n",
    "# Backup model to Google Drive\n",
    "!cp config4_*.pt \"/content/drive/MyDrive/Colab Notebooks/\"\n",
    "print(\"✅ Configuration 4 completed and backed up to Drive!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8145479,
     "status": "ok",
     "timestamp": 1755960565429,
     "user": {
      "displayName": "Yang Zhou",
      "userId": "09717432835810465973"
     },
     "user_tz": 240
    },
    "id": "MOn2lXyxuByF",
    "outputId": "358884ec-c69d-4547-f4c6-d3428412f663"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(\n",
      "Building zero-shot classifier.\n",
      "100% 200/200 [00:12<00:00, 16.65it/s]\n",
      "Saving model to ./config5_0.pt\n",
      "Train Epoch: 0 [0% 0/352]\tLoss: 1.430876\tData (t) 13.988\tBatch (t) 19.614\n",
      "Train Epoch: 0 [6% 20/352]\tLoss: 1.671773\tData (t) 2.032\tBatch (t) 2.335\n",
      "Train Epoch: 0 [11% 40/352]\tLoss: 1.435908\tData (t) 2.023\tBatch (t) 2.070\n",
      "Train Epoch: 0 [17% 60/352]\tLoss: 1.195043\tData (t) 2.033\tBatch (t) 2.081\n",
      "Train Epoch: 0 [23% 80/352]\tLoss: 1.295531\tData (t) 2.033\tBatch (t) 2.066\n",
      "Train Epoch: 0 [28% 100/352]\tLoss: 1.165548\tData (t) 1.821\tBatch (t) 1.852\n",
      "Train Epoch: 0 [34% 120/352]\tLoss: 1.359950\tData (t) 1.803\tBatch (t) 1.834\n",
      "Train Epoch: 0 [40% 140/352]\tLoss: 1.352595\tData (t) 1.812\tBatch (t) 1.844\n",
      "Train Epoch: 0 [45% 160/352]\tLoss: 1.147962\tData (t) 1.799\tBatch (t) 1.831\n",
      "Train Epoch: 0 [51% 180/352]\tLoss: 1.303125\tData (t) 2.039\tBatch (t) 2.071\n",
      "Train Epoch: 0 [57% 200/352]\tLoss: 0.973293\tData (t) 1.796\tBatch (t) 1.830\n",
      "Train Epoch: 0 [62% 220/352]\tLoss: 1.075627\tData (t) 2.036\tBatch (t) 2.067\n",
      "Train Epoch: 0 [68% 240/352]\tLoss: 1.115972\tData (t) 1.789\tBatch (t) 1.821\n",
      "Train Epoch: 0 [74% 260/352]\tLoss: 1.143139\tData (t) 2.030\tBatch (t) 2.067\n",
      "Train Epoch: 0 [80% 280/352]\tLoss: 0.995153\tData (t) 2.033\tBatch (t) 2.064\n",
      "Train Epoch: 0 [85% 300/352]\tLoss: 1.041163\tData (t) 2.038\tBatch (t) 2.311\n",
      "Train Epoch: 0 [91% 320/352]\tLoss: 1.082978\tData (t) 2.029\tBatch (t) 2.061\n",
      "Train Epoch: 0 [97% 340/352]\tLoss: 0.975382\tData (t) 2.039\tBatch (t) 2.193\n",
      "********************************************************************************\n",
      "Starting eval on validation split\n",
      "Val loss: 1.7508   Acc: 73.35: 100% 40/40 [00:39<00:00,  1.02it/s]\n",
      "Val acc at epoch 0: 73.35\n",
      "Saving model to ./config5_1.pt\n",
      "Train Epoch: 1 [0% 0/352]\tLoss: 1.012269\tData (t) 9.627\tBatch (t) 11.156\n",
      "Train Epoch: 1 [6% 20/352]\tLoss: 1.199017\tData (t) 2.056\tBatch (t) 2.105\n",
      "Train Epoch: 1 [11% 40/352]\tLoss: 0.701922\tData (t) 1.987\tBatch (t) 2.041\n",
      "Train Epoch: 1 [17% 60/352]\tLoss: 0.923861\tData (t) 2.033\tBatch (t) 2.065\n",
      "Train Epoch: 1 [23% 80/352]\tLoss: 0.869257\tData (t) 2.016\tBatch (t) 2.048\n",
      "Train Epoch: 1 [28% 100/352]\tLoss: 0.887115\tData (t) 2.015\tBatch (t) 2.046\n",
      "Train Epoch: 1 [34% 120/352]\tLoss: 0.983095\tData (t) 2.013\tBatch (t) 2.045\n",
      "Train Epoch: 1 [40% 140/352]\tLoss: 0.853483\tData (t) 2.007\tBatch (t) 2.040\n",
      "Train Epoch: 1 [45% 160/352]\tLoss: 0.811381\tData (t) 2.040\tBatch (t) 2.072\n",
      "Train Epoch: 1 [51% 180/352]\tLoss: 0.907304\tData (t) 2.034\tBatch (t) 2.066\n",
      "Train Epoch: 1 [57% 200/352]\tLoss: 0.899913\tData (t) 2.036\tBatch (t) 2.275\n",
      "Train Epoch: 1 [62% 220/352]\tLoss: 0.965076\tData (t) 2.033\tBatch (t) 2.237\n",
      "Train Epoch: 1 [68% 240/352]\tLoss: 0.770086\tData (t) 2.030\tBatch (t) 2.237\n",
      "Train Epoch: 1 [74% 260/352]\tLoss: 0.831856\tData (t) 1.866\tBatch (t) 2.187\n",
      "Train Epoch: 1 [80% 280/352]\tLoss: 0.768492\tData (t) 1.831\tBatch (t) 2.106\n",
      "Train Epoch: 1 [85% 300/352]\tLoss: 0.761004\tData (t) 1.801\tBatch (t) 2.057\n",
      "Train Epoch: 1 [91% 320/352]\tLoss: 0.801376\tData (t) 1.842\tBatch (t) 2.062\n",
      "Train Epoch: 1 [97% 340/352]\tLoss: 0.717733\tData (t) 2.040\tBatch (t) 2.073\n",
      "********************************************************************************\n",
      "Starting eval on validation split\n",
      "Val loss: 1.3466   Acc: 77.10: 100% 40/40 [00:35<00:00,  1.13it/s]\n",
      "Val acc at epoch 1: 77.10\n",
      "Saving model to ./config5_2.pt\n",
      "Train Epoch: 2 [0% 0/352]\tLoss: 0.777507\tData (t) 6.565\tBatch (t) 6.841\n",
      "Train Epoch: 2 [6% 20/352]\tLoss: 0.754846\tData (t) 2.062\tBatch (t) 2.105\n",
      "Train Epoch: 2 [11% 40/352]\tLoss: 0.666979\tData (t) 2.011\tBatch (t) 2.051\n",
      "Train Epoch: 2 [17% 60/352]\tLoss: 0.678192\tData (t) 2.047\tBatch (t) 2.079\n",
      "Train Epoch: 2 [23% 80/352]\tLoss: 0.824914\tData (t) 2.033\tBatch (t) 2.066\n",
      "Train Epoch: 2 [28% 100/352]\tLoss: 0.641474\tData (t) 2.038\tBatch (t) 2.069\n",
      "Train Epoch: 2 [34% 120/352]\tLoss: 0.729982\tData (t) 1.867\tBatch (t) 2.109\n",
      "Train Epoch: 2 [40% 140/352]\tLoss: 0.675222\tData (t) 2.036\tBatch (t) 2.067\n",
      "Train Epoch: 2 [45% 160/352]\tLoss: 0.681803\tData (t) 2.037\tBatch (t) 2.069\n",
      "Train Epoch: 2 [51% 180/352]\tLoss: 0.711249\tData (t) 2.038\tBatch (t) 2.080\n",
      "Train Epoch: 2 [57% 200/352]\tLoss: 0.690822\tData (t) 2.033\tBatch (t) 2.079\n",
      "Train Epoch: 2 [62% 220/352]\tLoss: 0.641407\tData (t) 2.024\tBatch (t) 2.071\n",
      "Train Epoch: 2 [68% 240/352]\tLoss: 0.722968\tData (t) 2.025\tBatch (t) 2.067\n",
      "Train Epoch: 2 [74% 260/352]\tLoss: 0.585187\tData (t) 2.005\tBatch (t) 2.036\n",
      "Train Epoch: 2 [80% 280/352]\tLoss: 0.632622\tData (t) 1.831\tBatch (t) 1.865\n",
      "Train Epoch: 2 [85% 300/352]\tLoss: 0.642740\tData (t) 2.021\tBatch (t) 2.056\n",
      "Train Epoch: 2 [91% 320/352]\tLoss: 0.836868\tData (t) 2.031\tBatch (t) 2.063\n",
      "Train Epoch: 2 [97% 340/352]\tLoss: 0.682153\tData (t) 2.029\tBatch (t) 2.060\n",
      "********************************************************************************\n",
      "Starting eval on validation split\n",
      "Val loss: 1.6620   Acc: 78.31: 100% 40/40 [00:38<00:00,  1.04it/s]\n",
      "Val acc at epoch 2: 78.31\n",
      "Saving model to ./config5_3.pt\n",
      "Train Epoch: 3 [0% 0/352]\tLoss: 0.494101\tData (t) 8.803\tBatch (t) 9.077\n",
      "Train Epoch: 3 [6% 20/352]\tLoss: 0.635498\tData (t) 2.072\tBatch (t) 2.104\n",
      "Train Epoch: 3 [11% 40/352]\tLoss: 0.449384\tData (t) 2.012\tBatch (t) 2.044\n",
      "Train Epoch: 3 [17% 60/352]\tLoss: 0.605136\tData (t) 2.050\tBatch (t) 2.083\n",
      "Train Epoch: 3 [23% 80/352]\tLoss: 0.574960\tData (t) 2.032\tBatch (t) 2.063\n",
      "Train Epoch: 3 [28% 100/352]\tLoss: 0.487305\tData (t) 2.026\tBatch (t) 2.072\n",
      "Train Epoch: 3 [34% 120/352]\tLoss: 0.541073\tData (t) 1.874\tBatch (t) 2.071\n",
      "Train Epoch: 3 [40% 140/352]\tLoss: 0.530483\tData (t) 2.042\tBatch (t) 2.074\n",
      "Train Epoch: 3 [45% 160/352]\tLoss: 0.562935\tData (t) 2.032\tBatch (t) 2.063\n",
      "Train Epoch: 3 [51% 180/352]\tLoss: 0.609307\tData (t) 2.030\tBatch (t) 2.065\n",
      "Train Epoch: 3 [57% 200/352]\tLoss: 0.535473\tData (t) 2.030\tBatch (t) 2.079\n",
      "Train Epoch: 3 [62% 220/352]\tLoss: 0.577255\tData (t) 2.033\tBatch (t) 2.075\n",
      "Train Epoch: 3 [68% 240/352]\tLoss: 0.529458\tData (t) 2.015\tBatch (t) 2.060\n",
      "Train Epoch: 3 [74% 260/352]\tLoss: 0.476237\tData (t) 2.020\tBatch (t) 2.272\n",
      "Train Epoch: 3 [80% 280/352]\tLoss: 0.580042\tData (t) 1.855\tBatch (t) 1.949\n",
      "Train Epoch: 3 [85% 300/352]\tLoss: 0.559105\tData (t) 2.017\tBatch (t) 2.055\n",
      "Train Epoch: 3 [91% 320/352]\tLoss: 0.707017\tData (t) 2.033\tBatch (t) 2.064\n",
      "Train Epoch: 3 [97% 340/352]\tLoss: 0.670715\tData (t) 2.035\tBatch (t) 2.067\n",
      "********************************************************************************\n",
      "Starting eval on validation split\n",
      "Val loss: 1.4092   Acc: 79.00: 100% 40/40 [00:35<00:00,  1.12it/s]\n",
      "Val acc at epoch 3: 79.00\n",
      "Saving model to ./config5_4.pt\n",
      "Train Epoch: 4 [0% 0/352]\tLoss: 0.389467\tData (t) 6.472\tBatch (t) 6.741\n",
      "Train Epoch: 4 [6% 20/352]\tLoss: 0.440723\tData (t) 2.081\tBatch (t) 2.117\n",
      "Train Epoch: 4 [11% 40/352]\tLoss: 0.426568\tData (t) 2.012\tBatch (t) 2.045\n",
      "Train Epoch: 4 [17% 60/352]\tLoss: 0.426283\tData (t) 2.050\tBatch (t) 2.081\n",
      "Train Epoch: 4 [23% 80/352]\tLoss: 0.486069\tData (t) 2.031\tBatch (t) 2.062\n",
      "Train Epoch: 4 [28% 100/352]\tLoss: 0.477017\tData (t) 2.033\tBatch (t) 2.065\n",
      "Train Epoch: 4 [34% 120/352]\tLoss: 0.504004\tData (t) 2.034\tBatch (t) 2.065\n",
      "Train Epoch: 4 [40% 140/352]\tLoss: 0.517332\tData (t) 2.035\tBatch (t) 2.071\n",
      "Train Epoch: 4 [45% 160/352]\tLoss: 0.482983\tData (t) 1.874\tBatch (t) 2.072\n",
      "Train Epoch: 4 [51% 180/352]\tLoss: 0.387650\tData (t) 1.873\tBatch (t) 1.904\n",
      "Train Epoch: 4 [57% 200/352]\tLoss: 0.544304\tData (t) 2.034\tBatch (t) 2.065\n",
      "Train Epoch: 4 [62% 220/352]\tLoss: 0.443073\tData (t) 2.024\tBatch (t) 2.055\n",
      "Train Epoch: 4 [68% 240/352]\tLoss: 0.351229\tData (t) 2.033\tBatch (t) 2.065\n",
      "Train Epoch: 4 [74% 260/352]\tLoss: 0.591774\tData (t) 2.036\tBatch (t) 2.080\n",
      "Train Epoch: 4 [80% 280/352]\tLoss: 0.350303\tData (t) 2.033\tBatch (t) 2.075\n",
      "Train Epoch: 4 [85% 300/352]\tLoss: 0.454949\tData (t) 2.011\tBatch (t) 2.049\n",
      "Train Epoch: 4 [91% 320/352]\tLoss: 0.408292\tData (t) 2.027\tBatch (t) 2.059\n",
      "Train Epoch: 4 [97% 340/352]\tLoss: 0.395154\tData (t) 2.021\tBatch (t) 2.052\n",
      "********************************************************************************\n",
      "Starting eval on validation split\n",
      "Val loss: 1.3614   Acc: 79.03: 100% 40/40 [00:36<00:00,  1.10it/s]\n",
      "Val acc at epoch 4: 79.03\n",
      "Saving model to ./config5_5.pt\n",
      "Train Epoch: 5 [0% 0/352]\tLoss: 0.253681\tData (t) 6.137\tBatch (t) 6.603\n",
      "Train Epoch: 5 [6% 20/352]\tLoss: 0.301813\tData (t) 2.061\tBatch (t) 2.108\n",
      "Train Epoch: 5 [11% 40/352]\tLoss: 0.298415\tData (t) 1.994\tBatch (t) 2.037\n",
      "Train Epoch: 5 [17% 60/352]\tLoss: 0.329267\tData (t) 2.051\tBatch (t) 2.099\n",
      "Train Epoch: 5 [23% 80/352]\tLoss: 0.316593\tData (t) 2.028\tBatch (t) 2.060\n",
      "Train Epoch: 5 [28% 100/352]\tLoss: 0.303516\tData (t) 2.037\tBatch (t) 2.069\n",
      "Train Epoch: 5 [34% 120/352]\tLoss: 0.306512\tData (t) 2.044\tBatch (t) 2.075\n",
      "Train Epoch: 5 [40% 140/352]\tLoss: 0.308422\tData (t) 2.034\tBatch (t) 2.065\n",
      "Train Epoch: 5 [45% 160/352]\tLoss: 0.331257\tData (t) 2.031\tBatch (t) 2.063\n",
      "Train Epoch: 5 [51% 180/352]\tLoss: 0.451902\tData (t) 2.034\tBatch (t) 2.065\n",
      "Train Epoch: 5 [57% 200/352]\tLoss: 0.296165\tData (t) 2.033\tBatch (t) 2.066\n",
      "Train Epoch: 5 [62% 220/352]\tLoss: 0.526216\tData (t) 2.031\tBatch (t) 2.063\n",
      "Train Epoch: 5 [68% 240/352]\tLoss: 0.301761\tData (t) 2.031\tBatch (t) 2.064\n",
      "Train Epoch: 5 [74% 260/352]\tLoss: 0.324596\tData (t) 2.038\tBatch (t) 2.070\n",
      "Train Epoch: 5 [80% 280/352]\tLoss: 0.380589\tData (t) 2.017\tBatch (t) 2.048\n",
      "Train Epoch: 5 [85% 300/352]\tLoss: 0.286684\tData (t) 1.786\tBatch (t) 1.837\n",
      "Train Epoch: 5 [91% 320/352]\tLoss: 0.298365\tData (t) 1.936\tBatch (t) 2.269\n",
      "Train Epoch: 5 [97% 340/352]\tLoss: 0.300816\tData (t) 2.004\tBatch (t) 2.188\n",
      "********************************************************************************\n",
      "Starting eval on validation split\n",
      "Val loss: 1.3570   Acc: 79.34: 100% 40/40 [00:40<00:00,  1.02s/it]\n",
      "Val acc at epoch 5: 79.34\n",
      "Saving model to ./config5_6.pt\n",
      "Train Epoch: 6 [0% 0/352]\tLoss: 0.267718\tData (t) 4.828\tBatch (t) 5.309\n",
      "Train Epoch: 6 [6% 20/352]\tLoss: 0.224023\tData (t) 2.062\tBatch (t) 2.106\n",
      "Train Epoch: 6 [11% 40/352]\tLoss: 0.367256\tData (t) 2.023\tBatch (t) 2.055\n",
      "Train Epoch: 6 [17% 60/352]\tLoss: 0.280695\tData (t) 2.041\tBatch (t) 2.073\n",
      "Train Epoch: 6 [23% 80/352]\tLoss: 0.235637\tData (t) 2.042\tBatch (t) 2.073\n",
      "Train Epoch: 6 [28% 100/352]\tLoss: 0.262912\tData (t) 2.036\tBatch (t) 2.067\n",
      "Train Epoch: 6 [34% 120/352]\tLoss: 0.283616\tData (t) 2.035\tBatch (t) 2.066\n",
      "Train Epoch: 6 [40% 140/352]\tLoss: 0.231787\tData (t) 2.035\tBatch (t) 2.066\n",
      "Train Epoch: 6 [45% 160/352]\tLoss: 0.232005\tData (t) 2.034\tBatch (t) 2.083\n",
      "Train Epoch: 6 [51% 180/352]\tLoss: 0.243381\tData (t) 2.035\tBatch (t) 2.094\n",
      "Train Epoch: 6 [57% 200/352]\tLoss: 0.199366\tData (t) 2.036\tBatch (t) 2.076\n",
      "Train Epoch: 6 [62% 220/352]\tLoss: 0.352027\tData (t) 2.030\tBatch (t) 2.061\n",
      "Train Epoch: 6 [68% 240/352]\tLoss: 0.287203\tData (t) 2.036\tBatch (t) 2.333\n",
      "Train Epoch: 6 [74% 260/352]\tLoss: 0.277123\tData (t) 2.035\tBatch (t) 2.066\n",
      "Train Epoch: 6 [80% 280/352]\tLoss: 0.206395\tData (t) 2.039\tBatch (t) 2.090\n",
      "Train Epoch: 6 [85% 300/352]\tLoss: 0.182937\tData (t) 1.875\tBatch (t) 2.202\n",
      "Train Epoch: 6 [91% 320/352]\tLoss: 0.227764\tData (t) 1.876\tBatch (t) 2.191\n",
      "Train Epoch: 6 [97% 340/352]\tLoss: 0.300737\tData (t) 1.879\tBatch (t) 2.149\n",
      "********************************************************************************\n",
      "Starting eval on validation split\n",
      "Val loss: 1.6051   Acc: 79.32: 100% 40/40 [00:39<00:00,  1.00it/s]\n",
      "Val acc at epoch 6: 79.32\n",
      "Saving model to ./config5_7.pt\n",
      "Train Epoch: 7 [0% 0/352]\tLoss: 0.200202\tData (t) 11.272\tBatch (t) 11.724\n",
      "Train Epoch: 7 [6% 20/352]\tLoss: 0.312737\tData (t) 1.971\tBatch (t) 2.158\n",
      "Train Epoch: 7 [11% 40/352]\tLoss: 0.231035\tData (t) 1.646\tBatch (t) 1.781\n",
      "Train Epoch: 7 [17% 60/352]\tLoss: 0.122125\tData (t) 1.890\tBatch (t) 2.219\n",
      "Train Epoch: 7 [23% 80/352]\tLoss: 0.325849\tData (t) 1.874\tBatch (t) 2.065\n",
      "Train Epoch: 7 [28% 100/352]\tLoss: 0.309335\tData (t) 2.037\tBatch (t) 2.219\n",
      "Train Epoch: 7 [34% 120/352]\tLoss: 0.196987\tData (t) 2.036\tBatch (t) 2.067\n",
      "Train Epoch: 7 [40% 140/352]\tLoss: 0.155884\tData (t) 2.037\tBatch (t) 2.068\n",
      "Train Epoch: 7 [45% 160/352]\tLoss: 0.233568\tData (t) 2.039\tBatch (t) 2.071\n",
      "Train Epoch: 7 [51% 180/352]\tLoss: 0.283979\tData (t) 2.036\tBatch (t) 2.071\n",
      "Train Epoch: 7 [57% 200/352]\tLoss: 0.191120\tData (t) 2.034\tBatch (t) 2.065\n",
      "Train Epoch: 7 [62% 220/352]\tLoss: 0.239669\tData (t) 2.036\tBatch (t) 2.068\n",
      "Train Epoch: 7 [68% 240/352]\tLoss: 0.208430\tData (t) 2.037\tBatch (t) 2.069\n",
      "Train Epoch: 7 [74% 260/352]\tLoss: 0.321767\tData (t) 2.040\tBatch (t) 2.073\n",
      "Train Epoch: 7 [80% 280/352]\tLoss: 0.287314\tData (t) 2.036\tBatch (t) 2.067\n",
      "Train Epoch: 7 [85% 300/352]\tLoss: 0.208860\tData (t) 2.035\tBatch (t) 2.066\n",
      "Train Epoch: 7 [91% 320/352]\tLoss: 0.295566\tData (t) 2.023\tBatch (t) 2.055\n",
      "Train Epoch: 7 [97% 340/352]\tLoss: 0.207753\tData (t) 2.032\tBatch (t) 2.068\n",
      "********************************************************************************\n",
      "Starting eval on validation split\n",
      "Val loss: 1.4338   Acc: 79.14: 100% 40/40 [00:41<00:00,  1.04s/it]\n",
      "Val acc at epoch 7: 79.14\n",
      "Saving model to ./config5_8.pt\n",
      "Train Epoch: 8 [0% 0/352]\tLoss: 0.195034\tData (t) 7.124\tBatch (t) 7.371\n",
      "Train Epoch: 8 [6% 20/352]\tLoss: 0.152610\tData (t) 2.075\tBatch (t) 2.118\n",
      "Train Epoch: 8 [11% 40/352]\tLoss: 0.226032\tData (t) 2.016\tBatch (t) 2.047\n",
      "Train Epoch: 8 [17% 60/352]\tLoss: 0.197329\tData (t) 2.047\tBatch (t) 2.079\n",
      "Train Epoch: 8 [23% 80/352]\tLoss: 0.252475\tData (t) 2.033\tBatch (t) 2.064\n",
      "Train Epoch: 8 [28% 100/352]\tLoss: 0.219015\tData (t) 2.027\tBatch (t) 2.059\n",
      "Train Epoch: 8 [34% 120/352]\tLoss: 0.313335\tData (t) 2.034\tBatch (t) 2.065\n",
      "Train Epoch: 8 [40% 140/352]\tLoss: 0.227683\tData (t) 2.013\tBatch (t) 2.045\n",
      "Train Epoch: 8 [45% 160/352]\tLoss: 0.226277\tData (t) 1.821\tBatch (t) 1.854\n",
      "Train Epoch: 8 [51% 180/352]\tLoss: 0.163840\tData (t) 1.791\tBatch (t) 1.822\n",
      "Train Epoch: 8 [57% 200/352]\tLoss: 0.193476\tData (t) 1.825\tBatch (t) 1.865\n",
      "Train Epoch: 8 [62% 220/352]\tLoss: 0.249248\tData (t) 1.667\tBatch (t) 1.776\n",
      "Train Epoch: 8 [68% 240/352]\tLoss: 0.174817\tData (t) 1.797\tBatch (t) 1.884\n",
      "Train Epoch: 8 [74% 260/352]\tLoss: 0.205508\tData (t) 1.843\tBatch (t) 1.973\n",
      "Train Epoch: 8 [80% 280/352]\tLoss: 0.244987\tData (t) 1.826\tBatch (t) 1.948\n",
      "Train Epoch: 8 [85% 300/352]\tLoss: 0.199434\tData (t) 1.710\tBatch (t) 1.771\n",
      "Train Epoch: 8 [91% 320/352]\tLoss: 0.147182\tData (t) 1.808\tBatch (t) 1.848\n",
      "Train Epoch: 8 [97% 340/352]\tLoss: 0.146173\tData (t) 2.031\tBatch (t) 2.077\n",
      "********************************************************************************\n",
      "Starting eval on validation split\n",
      "Val loss: 1.4808   Acc: 79.19: 100% 40/40 [00:37<00:00,  1.07it/s]\n",
      "Val acc at epoch 8: 79.19\n",
      "Saving model to ./config5_9.pt\n",
      "Train Epoch: 9 [0% 0/352]\tLoss: 0.243776\tData (t) 5.230\tBatch (t) 5.540\n",
      "Train Epoch: 9 [6% 20/352]\tLoss: 0.252624\tData (t) 2.078\tBatch (t) 2.115\n",
      "Train Epoch: 9 [11% 40/352]\tLoss: 0.300203\tData (t) 2.010\tBatch (t) 2.041\n",
      "Train Epoch: 9 [17% 60/352]\tLoss: 0.140946\tData (t) 2.052\tBatch (t) 2.084\n",
      "Train Epoch: 9 [23% 80/352]\tLoss: 0.250485\tData (t) 2.028\tBatch (t) 2.064\n",
      "Train Epoch: 9 [28% 100/352]\tLoss: 0.155113\tData (t) 2.031\tBatch (t) 2.063\n",
      "Train Epoch: 9 [34% 120/352]\tLoss: 0.139779\tData (t) 2.042\tBatch (t) 2.075\n",
      "Train Epoch: 9 [40% 140/352]\tLoss: 0.185955\tData (t) 2.043\tBatch (t) 2.076\n",
      "Train Epoch: 9 [45% 160/352]\tLoss: 0.254471\tData (t) 2.041\tBatch (t) 2.072\n",
      "Train Epoch: 9 [51% 180/352]\tLoss: 0.172319\tData (t) 2.036\tBatch (t) 2.067\n",
      "Train Epoch: 9 [57% 200/352]\tLoss: 0.211234\tData (t) 2.028\tBatch (t) 2.059\n",
      "Train Epoch: 9 [62% 220/352]\tLoss: 0.146912\tData (t) 2.035\tBatch (t) 2.068\n",
      "Train Epoch: 9 [68% 240/352]\tLoss: 0.217988\tData (t) 2.038\tBatch (t) 2.070\n",
      "Train Epoch: 9 [74% 260/352]\tLoss: 0.305107\tData (t) 2.039\tBatch (t) 2.073\n",
      "Train Epoch: 9 [80% 280/352]\tLoss: 0.193389\tData (t) 2.035\tBatch (t) 2.079\n",
      "Train Epoch: 9 [85% 300/352]\tLoss: 0.283668\tData (t) 2.036\tBatch (t) 2.077\n",
      "Train Epoch: 9 [91% 320/352]\tLoss: 0.142301\tData (t) 2.024\tBatch (t) 2.067\n",
      "Train Epoch: 9 [97% 340/352]\tLoss: 0.180751\tData (t) 2.028\tBatch (t) 2.060\n",
      "********************************************************************************\n",
      "Starting eval on validation split\n",
      "Val loss: 1.4938   Acc: 79.30: 100% 40/40 [00:37<00:00,  1.06it/s]\n",
      "Val acc at epoch 9: 79.30\n",
      "Saving model to ./config5_10.pt\n",
      "✅ Configuration 5 completed and backed up to Drive!\n"
     ]
    }
   ],
   "source": [
    "# Configuration 5: lr=1e-6, wd=1e-4, epochs=10, batch_size=256, timm_aug=False\n",
    "!python $CODE_DIR/finetune.py --lr 1e-6 --wd 1e-4 --epochs 10 --batch-size 256 --data-location $DATA_DIR --name \"config5\"\n",
    "\n",
    "# Backup model to Google Drive\n",
    "!cp config5_*.pt \"/content/drive/MyDrive/Colab Notebooks/\"\n",
    "print(\"✅ Configuration 5 completed and backed up to Drive!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4j3zhX-GkTwZ"
   },
   "source": [
    "## Load Individual Models\n",
    "Load the models after training all 5 configurations,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "executionInfo": {
     "elapsed": 213,
     "status": "ok",
     "timestamp": 1756051878622,
     "user": {
      "displayName": "Yang Zhou",
      "userId": "09717432835810465973"
     },
     "user_tz": 240
    },
    "id": "ZS_iRE5Ju4y4"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import clip\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import math\n",
    "from tqdm.notebook import tqdm\n",
    "from utils import ModelWrapper,get_model_from_sd, eval_model_on_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 63041,
     "status": "ok",
     "timestamp": 1756051989346,
     "user": {
      "displayName": "Yang Zhou",
      "userId": "09717432835810465973"
     },
     "user_tz": 240
    },
    "id": "yNUi-YrmR4tJ",
    "outputId": "e7ac8d53-8c84-4f6d-c353-a3452c725572"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading /content/drive/MyDrive/Colab Notebooks/config1_3.pt (from Drive)\n",
      "Loading /content/drive/MyDrive/Colab Notebooks/config2_3.pt (from Drive)\n",
      "Loading /content/drive/MyDrive/Colab Notebooks/config3_3.pt (from Drive)\n",
      "Loading /content/drive/MyDrive/Colab Notebooks/config4_3.pt (from Drive)\n",
      "Loading /content/drive/MyDrive/Colab Notebooks/config5_3.pt (from Drive)\n",
      "✅ Loaded 5 models successfully!\n"
     ]
    }
   ],
   "source": [
    "# Load all trained models (check both local and Drive backup)\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model_names = ['config1', 'config2', 'config3', 'config4', 'config5']\n",
    "state_dicts = []\n",
    "checkpoint_step = 3 # FROM 0 TO 10.\n",
    "\n",
    "base_model, preprocess = clip.load('ViT-B/32', device, jit=False)\n",
    "base_model = base_model.float() # Force the base model to stay in float32 to match saved weights\n",
    "\n",
    "for name in model_names:\n",
    "    # Load the final checkpoint (after 10 epochs)\n",
    "    model_path = f'{name}_{checkpoint_step}.pt'\n",
    "    drive_path = f'/content/drive/MyDrive/Colab Notebooks/{model_path}'\n",
    "\n",
    "    # Try local first, then Drive backup\n",
    "    if os.path.exists(model_path):\n",
    "        print(f'Loading {model_path} (local)')\n",
    "        state_dicts.append(torch.load(model_path, map_location=device))\n",
    "    elif os.path.exists(drive_path):\n",
    "        print(f'Loading {drive_path} (from Drive)')\n",
    "        state_dicts.append(torch.load(drive_path, map_location=device))\n",
    "    else:\n",
    "        print(f'⚠️  Model {model_path} not found in local or Drive!')\n",
    "\n",
    "print(f\"✅ Loaded {len(state_dicts)} models successfully!\")\n",
    "print(f\"Checkpoint at step {checkpoint_step}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vajq5XXY9G-9"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YW2hD_S_GcrP"
   },
   "source": [
    "### Diagnostic: uncomment to use\n",
    "- evaluate on samples of the train, val, test split\n",
    "- evaluate one model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 124,
     "status": "ok",
     "timestamp": 1755960584994,
     "user": {
      "displayName": "Yang Zhou",
      "userId": "09717432835810465973"
     },
     "user_tz": 240
    },
    "id": "jmJymNoB9G-9",
    "outputId": "8061e2a3-a740-4082-f394-27d75858a2fd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔍 Testing trained model 0 on training data sample...\n"
     ]
    }
   ],
   "source": [
    "# Choose one model idx\n",
    "i = 0\n",
    "print(f\"\\n🔍 Testing trained model {i} on training data sample...\")\n",
    "\n",
    "# Get number of classes from the dataset\n",
    "num_classes = len(data_tinyImageNet.classnames)\n",
    "debug_model = get_model_from_sd(state_dicts[i], base_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S8LDi0v69G--"
   },
   "outputs": [],
   "source": [
    "from itertools import islice\n",
    "\n",
    "def show_split_samples_model_pred(model, data, split_name, batch_index=0):\n",
    "    \"\"\"Show sample images with label and model prediction\"\"\"\n",
    "\n",
    "    if split_name == \"train\":\n",
    "        loader = data.train_loader\n",
    "    elif split_name == \"validation\":\n",
    "        loader = data.val_loader\n",
    "    elif split_name == \"test\":\n",
    "        loader = data.test_loader\n",
    "\n",
    "    # get one batch\n",
    "    batch = next(islice(loader, batch_index, None))\n",
    "    images = batch['images']\n",
    "    labels = batch['labels']\n",
    "    print(f\"fetching {len(labels)} samples\")\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        logits = model(images.to(device))\n",
    "    pred = logits.argmax(dim=1, keepdim=True).cpu()\n",
    "    print(f\"pred: {pred.flatten()}\")\n",
    "\n",
    "    # Create subplot\n",
    "    fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    for i in range(len(images)):\n",
    "        # Display image (denormalize from CLIP preprocessing)\n",
    "        img = images[i]\n",
    "        # CLIP normalization: mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711]\n",
    "        mean = torch.tensor([0.48145466, 0.4578275, 0.40821073]).view(3, 1, 1)\n",
    "        std = torch.tensor([0.26862954, 0.26130258, 0.27577711]).view(3, 1, 1)\n",
    "        img = img * std + mean  # denormalize\n",
    "        img = torch.clamp(img, 0, 1)  # clamp to [0,1]\n",
    "        img = img.permute(1, 2, 0)\n",
    "\n",
    "        axes[i].imshow(img)\n",
    "        axes[i].axis('off')\n",
    "\n",
    "        # Get label info\n",
    "        pred_idx = int(pred[i])\n",
    "        true_idx = int(labels[i])\n",
    "\n",
    "        pred_class = data.idx_to_class[pred_idx]\n",
    "        true_class = data.idx_to_class[true_idx]\n",
    "        is_correct = (pred_idx == true_idx)\n",
    "\n",
    "        color = 'green' if is_correct else 'red'\n",
    "        axes[i].set_title(f\"True: {data.wordnet_map[true_class][:20]}... \\nPred: {data.wordnet_map[pred_class][:20]}...\",\n",
    "                         color=color, fontsize=10)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.suptitle(f'{split_name.upper()} Split - Images with Labels', fontsize=14, y=1.02)\n",
    "    plt.show()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OxWNGYeL9G--"
   },
   "outputs": [],
   "source": [
    "# show_split_samples_model_pred(debug_model, data_tinyImageNet, \"test\",1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KbXrQrfQ9G--"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5BmoxCKm9G--"
   },
   "source": [
    "### Evaluate individual models performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 415063,
     "status": "ok",
     "timestamp": 1756052404412,
     "user": {
      "displayName": "Yang Zhou",
      "userId": "09717432835810465973"
     },
     "user_tz": 240
    },
    "id": "YeQWWp71N8ba",
    "outputId": "ebf252ca-d6cd-42ef-ddf7-0a96976e1026"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 Evaluating Config 1...\n",
      "[0% 0/1250]\tAcc: 62.50\tData (t) 0.278\tBatch (t) 1.494\n",
      "[2% 20/1250]\tAcc: 77.98\tData (t) 0.003\tBatch (t) 0.029\n",
      "[3% 40/1250]\tAcc: 74.39\tData (t) 0.001\tBatch (t) 0.031\n",
      "[5% 60/1250]\tAcc: 73.98\tData (t) 0.001\tBatch (t) 0.027\n",
      "[6% 80/1250]\tAcc: 74.23\tData (t) 0.001\tBatch (t) 0.025\n",
      "[8% 100/1250]\tAcc: 74.75\tData (t) 0.001\tBatch (t) 0.025\n",
      "[10% 120/1250]\tAcc: 74.79\tData (t) 0.006\tBatch (t) 0.033\n",
      "[11% 140/1250]\tAcc: 75.35\tData (t) 0.001\tBatch (t) 0.025\n",
      "[13% 160/1250]\tAcc: 75.93\tData (t) 0.001\tBatch (t) 0.026\n",
      "[14% 180/1250]\tAcc: 76.59\tData (t) 0.001\tBatch (t) 0.028\n",
      "[16% 200/1250]\tAcc: 76.62\tData (t) 0.001\tBatch (t) 0.026\n",
      "[18% 220/1250]\tAcc: 76.36\tData (t) 0.001\tBatch (t) 0.028\n",
      "[19% 240/1250]\tAcc: 76.50\tData (t) 0.001\tBatch (t) 0.027\n",
      "[21% 260/1250]\tAcc: 76.53\tData (t) 0.001\tBatch (t) 0.025\n",
      "[22% 280/1250]\tAcc: 76.65\tData (t) 0.001\tBatch (t) 0.061\n",
      "[24% 300/1250]\tAcc: 76.70\tData (t) 0.003\tBatch (t) 0.048\n",
      "[26% 320/1250]\tAcc: 76.79\tData (t) 0.026\tBatch (t) 0.108\n",
      "[27% 340/1250]\tAcc: 76.76\tData (t) 0.001\tBatch (t) 0.030\n",
      "[29% 360/1250]\tAcc: 76.84\tData (t) 0.004\tBatch (t) 0.033\n",
      "[30% 380/1250]\tAcc: 76.80\tData (t) 0.003\tBatch (t) 0.030\n",
      "[32% 400/1250]\tAcc: 76.87\tData (t) 0.001\tBatch (t) 0.027\n",
      "[34% 420/1250]\tAcc: 76.72\tData (t) 0.001\tBatch (t) 0.027\n",
      "[35% 440/1250]\tAcc: 76.79\tData (t) 0.001\tBatch (t) 0.027\n",
      "[37% 460/1250]\tAcc: 76.82\tData (t) 0.001\tBatch (t) 0.028\n",
      "[38% 480/1250]\tAcc: 76.92\tData (t) 0.004\tBatch (t) 0.032\n",
      "[40% 500/1250]\tAcc: 77.07\tData (t) 0.002\tBatch (t) 0.028\n",
      "[42% 520/1250]\tAcc: 77.16\tData (t) 0.004\tBatch (t) 0.034\n",
      "[43% 540/1250]\tAcc: 77.15\tData (t) 0.002\tBatch (t) 0.028\n",
      "[45% 560/1250]\tAcc: 77.25\tData (t) 0.001\tBatch (t) 0.025\n",
      "[46% 580/1250]\tAcc: 77.28\tData (t) 0.003\tBatch (t) 0.030\n",
      "[48% 600/1250]\tAcc: 77.16\tData (t) 0.001\tBatch (t) 0.027\n",
      "[50% 620/1250]\tAcc: 77.17\tData (t) 0.001\tBatch (t) 0.027\n",
      "[51% 640/1250]\tAcc: 77.09\tData (t) 0.004\tBatch (t) 0.031\n",
      "[53% 660/1250]\tAcc: 77.02\tData (t) 0.001\tBatch (t) 0.027\n",
      "[54% 680/1250]\tAcc: 76.98\tData (t) 0.003\tBatch (t) 0.085\n",
      "[56% 700/1250]\tAcc: 76.96\tData (t) 0.003\tBatch (t) 0.048\n",
      "[58% 720/1250]\tAcc: 76.94\tData (t) 0.016\tBatch (t) 0.039\n",
      "[59% 740/1250]\tAcc: 76.96\tData (t) 0.002\tBatch (t) 0.034\n",
      "[61% 760/1250]\tAcc: 77.00\tData (t) 0.001\tBatch (t) 0.029\n",
      "[62% 780/1250]\tAcc: 77.03\tData (t) 0.001\tBatch (t) 0.027\n",
      "[64% 800/1250]\tAcc: 76.97\tData (t) 0.001\tBatch (t) 0.026\n",
      "[66% 820/1250]\tAcc: 76.83\tData (t) 0.002\tBatch (t) 0.027\n",
      "[67% 840/1250]\tAcc: 76.69\tData (t) 0.001\tBatch (t) 0.027\n",
      "[69% 860/1250]\tAcc: 76.60\tData (t) 0.001\tBatch (t) 0.026\n",
      "[70% 880/1250]\tAcc: 76.60\tData (t) 0.001\tBatch (t) 0.026\n",
      "[72% 900/1250]\tAcc: 76.60\tData (t) 0.002\tBatch (t) 0.031\n",
      "[74% 920/1250]\tAcc: 76.64\tData (t) 0.003\tBatch (t) 0.027\n",
      "[75% 940/1250]\tAcc: 76.71\tData (t) 0.001\tBatch (t) 0.027\n",
      "[77% 960/1250]\tAcc: 76.61\tData (t) 0.001\tBatch (t) 0.027\n",
      "[78% 980/1250]\tAcc: 76.71\tData (t) 0.006\tBatch (t) 0.038\n",
      "[80% 1000/1250]\tAcc: 76.71\tData (t) 0.007\tBatch (t) 0.058\n",
      "[82% 1020/1250]\tAcc: 76.67\tData (t) 0.006\tBatch (t) 0.080\n",
      "[83% 1040/1250]\tAcc: 76.59\tData (t) 0.001\tBatch (t) 0.041\n",
      "[85% 1060/1250]\tAcc: 76.56\tData (t) 0.001\tBatch (t) 0.060\n",
      "[86% 1080/1250]\tAcc: 76.60\tData (t) 0.001\tBatch (t) 0.089\n",
      "[88% 1100/1250]\tAcc: 76.65\tData (t) 0.004\tBatch (t) 0.030\n",
      "[90% 1120/1250]\tAcc: 76.63\tData (t) 0.001\tBatch (t) 0.048\n",
      "[91% 1140/1250]\tAcc: 76.64\tData (t) 0.009\tBatch (t) 0.037\n",
      "[93% 1160/1250]\tAcc: 76.69\tData (t) 0.001\tBatch (t) 0.027\n",
      "[94% 1180/1250]\tAcc: 76.67\tData (t) 0.001\tBatch (t) 0.029\n",
      "[96% 1200/1250]\tAcc: 76.72\tData (t) 0.002\tBatch (t) 0.029\n",
      "[98% 1220/1250]\tAcc: 76.65\tData (t) 0.001\tBatch (t) 0.027\n",
      "[99% 1240/1250]\tAcc: 76.69\tData (t) 0.001\tBatch (t) 0.028\n",
      "[0% 0/1250]\tAcc: 87.50\tData (t) 0.125\tBatch (t) 0.150\n",
      "[2% 20/1250]\tAcc: 86.31\tData (t) 0.001\tBatch (t) 0.028\n",
      "[3% 40/1250]\tAcc: 77.74\tData (t) 0.004\tBatch (t) 0.033\n",
      "[5% 60/1250]\tAcc: 76.23\tData (t) 0.004\tBatch (t) 0.031\n",
      "[6% 80/1250]\tAcc: 75.62\tData (t) 0.001\tBatch (t) 0.027\n",
      "[8% 100/1250]\tAcc: 74.13\tData (t) 0.001\tBatch (t) 0.026\n",
      "[10% 120/1250]\tAcc: 73.86\tData (t) 0.001\tBatch (t) 0.030\n",
      "[11% 140/1250]\tAcc: 75.53\tData (t) 0.001\tBatch (t) 0.034\n",
      "[13% 160/1250]\tAcc: 76.48\tData (t) 0.001\tBatch (t) 0.024\n",
      "[14% 180/1250]\tAcc: 75.97\tData (t) 0.001\tBatch (t) 0.027\n",
      "[16% 200/1250]\tAcc: 75.68\tData (t) 0.001\tBatch (t) 0.027\n",
      "[18% 220/1250]\tAcc: 76.19\tData (t) 0.001\tBatch (t) 0.032\n",
      "[19% 240/1250]\tAcc: 76.24\tData (t) 0.012\tBatch (t) 0.045\n",
      "[21% 260/1250]\tAcc: 75.67\tData (t) 0.002\tBatch (t) 0.054\n",
      "[22% 280/1250]\tAcc: 75.53\tData (t) 0.001\tBatch (t) 0.075\n",
      "[24% 300/1250]\tAcc: 75.66\tData (t) 0.007\tBatch (t) 0.063\n",
      "[26% 320/1250]\tAcc: 75.43\tData (t) 0.001\tBatch (t) 0.026\n",
      "[27% 340/1250]\tAcc: 75.95\tData (t) 0.001\tBatch (t) 0.028\n",
      "[29% 360/1250]\tAcc: 76.35\tData (t) 0.001\tBatch (t) 0.027\n",
      "[30% 380/1250]\tAcc: 76.87\tData (t) 0.003\tBatch (t) 0.025\n",
      "[32% 400/1250]\tAcc: 77.03\tData (t) 0.005\tBatch (t) 0.052\n",
      "[34% 420/1250]\tAcc: 76.87\tData (t) 0.001\tBatch (t) 0.029\n",
      "[35% 440/1250]\tAcc: 76.56\tData (t) 0.001\tBatch (t) 0.030\n",
      "[37% 460/1250]\tAcc: 76.74\tData (t) 0.001\tBatch (t) 0.027\n",
      "[38% 480/1250]\tAcc: 76.40\tData (t) 0.004\tBatch (t) 0.036\n",
      "[40% 500/1250]\tAcc: 76.17\tData (t) 0.001\tBatch (t) 0.035\n",
      "[42% 520/1250]\tAcc: 76.44\tData (t) 0.004\tBatch (t) 0.061\n",
      "[43% 540/1250]\tAcc: 76.50\tData (t) 0.007\tBatch (t) 0.040\n",
      "[45% 560/1250]\tAcc: 76.49\tData (t) 0.001\tBatch (t) 0.029\n",
      "[46% 580/1250]\tAcc: 76.74\tData (t) 0.001\tBatch (t) 0.026\n",
      "[48% 600/1250]\tAcc: 76.79\tData (t) 0.004\tBatch (t) 0.035\n",
      "[50% 620/1250]\tAcc: 76.77\tData (t) 0.002\tBatch (t) 0.042\n",
      "[51% 640/1250]\tAcc: 76.52\tData (t) 0.001\tBatch (t) 0.033\n",
      "[53% 660/1250]\tAcc: 76.48\tData (t) 0.001\tBatch (t) 0.037\n",
      "[54% 680/1250]\tAcc: 76.71\tData (t) 0.004\tBatch (t) 0.043\n",
      "[56% 700/1250]\tAcc: 76.87\tData (t) 0.001\tBatch (t) 0.031\n",
      "[58% 720/1250]\tAcc: 76.96\tData (t) 0.001\tBatch (t) 0.034\n",
      "[59% 740/1250]\tAcc: 77.11\tData (t) 0.002\tBatch (t) 0.029\n",
      "[61% 760/1250]\tAcc: 77.27\tData (t) 0.001\tBatch (t) 0.031\n",
      "[62% 780/1250]\tAcc: 77.08\tData (t) 0.001\tBatch (t) 0.035\n",
      "[64% 800/1250]\tAcc: 76.95\tData (t) 0.001\tBatch (t) 0.029\n",
      "[66% 820/1250]\tAcc: 76.92\tData (t) 0.002\tBatch (t) 0.031\n",
      "[67% 840/1250]\tAcc: 76.72\tData (t) 0.001\tBatch (t) 0.029\n",
      "[69% 860/1250]\tAcc: 76.70\tData (t) 0.001\tBatch (t) 0.028\n",
      "[70% 880/1250]\tAcc: 76.67\tData (t) 0.001\tBatch (t) 0.031\n",
      "[72% 900/1250]\tAcc: 76.69\tData (t) 0.001\tBatch (t) 0.029\n",
      "[74% 920/1250]\tAcc: 76.93\tData (t) 0.001\tBatch (t) 0.032\n",
      "[75% 940/1250]\tAcc: 76.87\tData (t) 0.001\tBatch (t) 0.032\n",
      "[77% 960/1250]\tAcc: 76.86\tData (t) 0.001\tBatch (t) 0.029\n",
      "[78% 980/1250]\tAcc: 76.75\tData (t) 0.001\tBatch (t) 0.030\n",
      "[80% 1000/1250]\tAcc: 76.54\tData (t) 0.001\tBatch (t) 0.026\n",
      "[82% 1020/1250]\tAcc: 76.51\tData (t) 0.008\tBatch (t) 0.070\n",
      "[83% 1040/1250]\tAcc: 76.65\tData (t) 0.005\tBatch (t) 0.083\n",
      "[85% 1060/1250]\tAcc: 76.66\tData (t) 0.010\tBatch (t) 0.100\n",
      "[86% 1080/1250]\tAcc: 76.75\tData (t) 0.004\tBatch (t) 0.068\n",
      "[88% 1100/1250]\tAcc: 76.58\tData (t) 0.001\tBatch (t) 0.031\n",
      "[90% 1120/1250]\tAcc: 76.56\tData (t) 0.005\tBatch (t) 0.043\n",
      "[91% 1140/1250]\tAcc: 76.54\tData (t) 0.001\tBatch (t) 0.030\n",
      "[93% 1160/1250]\tAcc: 76.59\tData (t) 0.001\tBatch (t) 0.028\n",
      "[94% 1180/1250]\tAcc: 76.66\tData (t) 0.001\tBatch (t) 0.027\n",
      "[96% 1200/1250]\tAcc: 76.68\tData (t) 0.004\tBatch (t) 0.032\n",
      "[98% 1220/1250]\tAcc: 76.80\tData (t) 0.001\tBatch (t) 0.028\n",
      "[99% 1240/1250]\tAcc: 76.73\tData (t) 0.003\tBatch (t) 0.029\n",
      "Config 1 Accuracy: 76.68%\n",
      "\n",
      "📊 Evaluating Config 2...\n",
      "[0% 0/1250]\tAcc: 50.00\tData (t) 0.125\tBatch (t) 0.160\n",
      "[2% 20/1250]\tAcc: 76.79\tData (t) 0.001\tBatch (t) 0.029\n",
      "[3% 40/1250]\tAcc: 77.44\tData (t) 0.001\tBatch (t) 0.028\n",
      "[5% 60/1250]\tAcc: 76.64\tData (t) 0.001\tBatch (t) 0.029\n",
      "[6% 80/1250]\tAcc: 77.47\tData (t) 0.001\tBatch (t) 0.024\n",
      "[8% 100/1250]\tAcc: 77.48\tData (t) 0.001\tBatch (t) 0.029\n",
      "[10% 120/1250]\tAcc: 78.41\tData (t) 0.008\tBatch (t) 0.053\n",
      "[11% 140/1250]\tAcc: 78.28\tData (t) 0.009\tBatch (t) 0.038\n",
      "[13% 160/1250]\tAcc: 78.88\tData (t) 0.001\tBatch (t) 0.030\n",
      "[14% 180/1250]\tAcc: 78.87\tData (t) 0.001\tBatch (t) 0.038\n",
      "[16% 200/1250]\tAcc: 79.04\tData (t) 0.001\tBatch (t) 0.043\n",
      "[18% 220/1250]\tAcc: 78.85\tData (t) 0.007\tBatch (t) 0.049\n",
      "[19% 240/1250]\tAcc: 78.99\tData (t) 0.001\tBatch (t) 0.027\n",
      "[21% 260/1250]\tAcc: 79.36\tData (t) 0.013\tBatch (t) 0.055\n",
      "[22% 280/1250]\tAcc: 79.80\tData (t) 0.001\tBatch (t) 0.028\n",
      "[24% 300/1250]\tAcc: 80.19\tData (t) 0.001\tBatch (t) 0.027\n",
      "[26% 320/1250]\tAcc: 80.41\tData (t) 0.001\tBatch (t) 0.028\n",
      "[27% 340/1250]\tAcc: 80.39\tData (t) 0.001\tBatch (t) 0.028\n",
      "[29% 360/1250]\tAcc: 80.16\tData (t) 0.006\tBatch (t) 0.033\n",
      "[30% 380/1250]\tAcc: 80.12\tData (t) 0.001\tBatch (t) 0.027\n",
      "[32% 400/1250]\tAcc: 80.33\tData (t) 0.001\tBatch (t) 0.028\n",
      "[34% 420/1250]\tAcc: 80.26\tData (t) 0.001\tBatch (t) 0.028\n",
      "[35% 440/1250]\tAcc: 80.33\tData (t) 0.001\tBatch (t) 0.028\n",
      "[37% 460/1250]\tAcc: 80.53\tData (t) 0.001\tBatch (t) 0.028\n",
      "[38% 480/1250]\tAcc: 80.61\tData (t) 0.001\tBatch (t) 0.029\n",
      "[40% 500/1250]\tAcc: 80.54\tData (t) 0.004\tBatch (t) 0.032\n",
      "[42% 520/1250]\tAcc: 80.57\tData (t) 0.001\tBatch (t) 0.027\n",
      "[43% 540/1250]\tAcc: 80.41\tData (t) 0.001\tBatch (t) 0.027\n",
      "[45% 560/1250]\tAcc: 80.48\tData (t) 0.001\tBatch (t) 0.029\n",
      "[46% 580/1250]\tAcc: 80.51\tData (t) 0.001\tBatch (t) 0.047\n",
      "[48% 600/1250]\tAcc: 80.45\tData (t) 0.001\tBatch (t) 0.037\n",
      "[50% 620/1250]\tAcc: 80.37\tData (t) 0.001\tBatch (t) 0.054\n",
      "[51% 640/1250]\tAcc: 80.32\tData (t) 0.009\tBatch (t) 0.075\n",
      "[53% 660/1250]\tAcc: 80.35\tData (t) 0.001\tBatch (t) 0.032\n",
      "[54% 680/1250]\tAcc: 80.43\tData (t) 0.002\tBatch (t) 0.029\n",
      "[56% 700/1250]\tAcc: 80.42\tData (t) 0.001\tBatch (t) 0.029\n",
      "[58% 720/1250]\tAcc: 80.37\tData (t) 0.007\tBatch (t) 0.048\n",
      "[59% 740/1250]\tAcc: 80.50\tData (t) 0.001\tBatch (t) 0.029\n",
      "[61% 760/1250]\tAcc: 80.50\tData (t) 0.001\tBatch (t) 0.027\n",
      "[62% 780/1250]\tAcc: 80.55\tData (t) 0.002\tBatch (t) 0.028\n",
      "[64% 800/1250]\tAcc: 80.46\tData (t) 0.001\tBatch (t) 0.029\n",
      "[66% 820/1250]\tAcc: 80.47\tData (t) 0.001\tBatch (t) 0.037\n",
      "[67% 840/1250]\tAcc: 80.29\tData (t) 0.001\tBatch (t) 0.028\n",
      "[69% 860/1250]\tAcc: 80.21\tData (t) 0.001\tBatch (t) 0.026\n",
      "[70% 880/1250]\tAcc: 80.28\tData (t) 0.001\tBatch (t) 0.028\n",
      "[72% 900/1250]\tAcc: 80.24\tData (t) 0.001\tBatch (t) 0.028\n",
      "[74% 920/1250]\tAcc: 80.13\tData (t) 0.001\tBatch (t) 0.029\n",
      "[75% 940/1250]\tAcc: 80.07\tData (t) 0.002\tBatch (t) 0.029\n",
      "[77% 960/1250]\tAcc: 80.03\tData (t) 0.001\tBatch (t) 0.028\n",
      "[78% 980/1250]\tAcc: 80.05\tData (t) 0.001\tBatch (t) 0.029\n",
      "[80% 1000/1250]\tAcc: 80.01\tData (t) 0.007\tBatch (t) 0.037\n",
      "[82% 1020/1250]\tAcc: 79.98\tData (t) 0.007\tBatch (t) 0.032\n",
      "[83% 1040/1250]\tAcc: 79.90\tData (t) 0.001\tBatch (t) 0.058\n",
      "[85% 1060/1250]\tAcc: 79.92\tData (t) 0.001\tBatch (t) 0.042\n",
      "[86% 1080/1250]\tAcc: 79.91\tData (t) 0.016\tBatch (t) 0.062\n",
      "[88% 1100/1250]\tAcc: 79.95\tData (t) 0.001\tBatch (t) 0.029\n",
      "[90% 1120/1250]\tAcc: 79.93\tData (t) 0.007\tBatch (t) 0.044\n",
      "[91% 1140/1250]\tAcc: 79.94\tData (t) 0.001\tBatch (t) 0.026\n",
      "[93% 1160/1250]\tAcc: 79.92\tData (t) 0.003\tBatch (t) 0.032\n",
      "[94% 1180/1250]\tAcc: 79.87\tData (t) 0.001\tBatch (t) 0.027\n",
      "[96% 1200/1250]\tAcc: 79.91\tData (t) 0.008\tBatch (t) 0.043\n",
      "[98% 1220/1250]\tAcc: 79.98\tData (t) 0.001\tBatch (t) 0.028\n",
      "[99% 1240/1250]\tAcc: 80.03\tData (t) 0.003\tBatch (t) 0.030\n",
      "[0% 0/1250]\tAcc: 87.50\tData (t) 0.118\tBatch (t) 0.151\n",
      "[2% 20/1250]\tAcc: 83.93\tData (t) 0.001\tBatch (t) 0.028\n",
      "[3% 40/1250]\tAcc: 81.40\tData (t) 0.001\tBatch (t) 0.028\n",
      "[5% 60/1250]\tAcc: 80.74\tData (t) 0.005\tBatch (t) 0.032\n",
      "[6% 80/1250]\tAcc: 81.17\tData (t) 0.001\tBatch (t) 0.028\n",
      "[8% 100/1250]\tAcc: 81.68\tData (t) 0.001\tBatch (t) 0.029\n",
      "[10% 120/1250]\tAcc: 79.96\tData (t) 0.002\tBatch (t) 0.030\n",
      "[11% 140/1250]\tAcc: 80.14\tData (t) 0.001\tBatch (t) 0.028\n",
      "[13% 160/1250]\tAcc: 80.51\tData (t) 0.001\tBatch (t) 0.028\n",
      "[14% 180/1250]\tAcc: 79.70\tData (t) 0.003\tBatch (t) 0.044\n",
      "[16% 200/1250]\tAcc: 79.73\tData (t) 0.008\tBatch (t) 0.043\n",
      "[18% 220/1250]\tAcc: 79.07\tData (t) 0.001\tBatch (t) 0.031\n",
      "[19% 240/1250]\tAcc: 79.41\tData (t) 0.001\tBatch (t) 0.042\n",
      "[21% 260/1250]\tAcc: 78.54\tData (t) 0.010\tBatch (t) 0.039\n",
      "[22% 280/1250]\tAcc: 78.91\tData (t) 0.001\tBatch (t) 0.037\n",
      "[24% 300/1250]\tAcc: 78.99\tData (t) 0.001\tBatch (t) 0.028\n",
      "[26% 320/1250]\tAcc: 78.58\tData (t) 0.001\tBatch (t) 0.034\n",
      "[27% 340/1250]\tAcc: 78.89\tData (t) 0.002\tBatch (t) 0.031\n",
      "[29% 360/1250]\tAcc: 79.29\tData (t) 0.020\tBatch (t) 0.052\n",
      "[30% 380/1250]\tAcc: 79.66\tData (t) 0.001\tBatch (t) 0.029\n",
      "[32% 400/1250]\tAcc: 79.64\tData (t) 0.001\tBatch (t) 0.034\n",
      "[34% 420/1250]\tAcc: 79.78\tData (t) 0.001\tBatch (t) 0.027\n",
      "[35% 440/1250]\tAcc: 79.85\tData (t) 0.001\tBatch (t) 0.028\n",
      "[37% 460/1250]\tAcc: 80.02\tData (t) 0.001\tBatch (t) 0.029\n",
      "[38% 480/1250]\tAcc: 79.83\tData (t) 0.010\tBatch (t) 0.078\n",
      "[40% 500/1250]\tAcc: 79.92\tData (t) 0.001\tBatch (t) 0.028\n",
      "[42% 520/1250]\tAcc: 79.99\tData (t) 0.001\tBatch (t) 0.029\n",
      "[43% 540/1250]\tAcc: 80.11\tData (t) 0.007\tBatch (t) 0.040\n",
      "[45% 560/1250]\tAcc: 80.12\tData (t) 0.001\tBatch (t) 0.045\n",
      "[46% 580/1250]\tAcc: 80.19\tData (t) 0.001\tBatch (t) 0.052\n",
      "[48% 600/1250]\tAcc: 80.28\tData (t) 0.009\tBatch (t) 0.052\n",
      "[50% 620/1250]\tAcc: 80.09\tData (t) 0.001\tBatch (t) 0.040\n",
      "[51% 640/1250]\tAcc: 79.91\tData (t) 0.003\tBatch (t) 0.038\n",
      "[53% 660/1250]\tAcc: 79.84\tData (t) 0.002\tBatch (t) 0.023\n",
      "[54% 680/1250]\tAcc: 79.97\tData (t) 0.019\tBatch (t) 0.040\n",
      "[56% 700/1250]\tAcc: 80.12\tData (t) 0.001\tBatch (t) 0.029\n",
      "[58% 720/1250]\tAcc: 80.03\tData (t) 0.001\tBatch (t) 0.030\n",
      "[59% 740/1250]\tAcc: 80.21\tData (t) 0.001\tBatch (t) 0.029\n",
      "[61% 760/1250]\tAcc: 80.24\tData (t) 0.005\tBatch (t) 0.065\n",
      "[62% 780/1250]\tAcc: 80.11\tData (t) 0.001\tBatch (t) 0.036\n",
      "[64% 800/1250]\tAcc: 79.99\tData (t) 0.001\tBatch (t) 0.028\n",
      "[66% 820/1250]\tAcc: 79.96\tData (t) 0.001\tBatch (t) 0.029\n",
      "[67% 840/1250]\tAcc: 79.64\tData (t) 0.002\tBatch (t) 0.030\n",
      "[69% 860/1250]\tAcc: 79.49\tData (t) 0.001\tBatch (t) 0.028\n",
      "[70% 880/1250]\tAcc: 79.26\tData (t) 0.001\tBatch (t) 0.029\n",
      "[72% 900/1250]\tAcc: 79.41\tData (t) 0.001\tBatch (t) 0.025\n",
      "[74% 920/1250]\tAcc: 79.60\tData (t) 0.017\tBatch (t) 0.047\n",
      "[75% 940/1250]\tAcc: 79.58\tData (t) 0.012\tBatch (t) 0.040\n",
      "[77% 960/1250]\tAcc: 79.55\tData (t) 0.001\tBatch (t) 0.048\n",
      "[78% 980/1250]\tAcc: 79.54\tData (t) 0.001\tBatch (t) 0.025\n",
      "[80% 1000/1250]\tAcc: 79.36\tData (t) 0.001\tBatch (t) 0.028\n",
      "[82% 1020/1250]\tAcc: 79.46\tData (t) 0.001\tBatch (t) 0.027\n",
      "[83% 1040/1250]\tAcc: 79.59\tData (t) 0.001\tBatch (t) 0.028\n",
      "[85% 1060/1250]\tAcc: 79.54\tData (t) 0.001\tBatch (t) 0.029\n",
      "[86% 1080/1250]\tAcc: 79.61\tData (t) 0.001\tBatch (t) 0.029\n",
      "[88% 1100/1250]\tAcc: 79.48\tData (t) 0.001\tBatch (t) 0.028\n",
      "[90% 1120/1250]\tAcc: 79.63\tData (t) 0.001\tBatch (t) 0.029\n",
      "[91% 1140/1250]\tAcc: 79.67\tData (t) 0.002\tBatch (t) 0.030\n",
      "[93% 1160/1250]\tAcc: 79.76\tData (t) 0.001\tBatch (t) 0.029\n",
      "[94% 1180/1250]\tAcc: 79.76\tData (t) 0.002\tBatch (t) 0.029\n",
      "[96% 1200/1250]\tAcc: 79.81\tData (t) 0.001\tBatch (t) 0.024\n",
      "[98% 1220/1250]\tAcc: 79.82\tData (t) 0.001\tBatch (t) 0.028\n",
      "[99% 1240/1250]\tAcc: 79.77\tData (t) 0.001\tBatch (t) 0.028\n",
      "Config 2 Accuracy: 80.01%\n",
      "\n",
      "📊 Evaluating Config 3...\n",
      "[0% 0/1250]\tAcc: 50.00\tData (t) 0.135\tBatch (t) 0.166\n",
      "[2% 20/1250]\tAcc: 77.98\tData (t) 0.001\tBatch (t) 0.029\n",
      "[3% 40/1250]\tAcc: 76.83\tData (t) 0.001\tBatch (t) 0.029\n",
      "[5% 60/1250]\tAcc: 77.66\tData (t) 0.001\tBatch (t) 0.028\n",
      "[6% 80/1250]\tAcc: 77.93\tData (t) 0.001\tBatch (t) 0.039\n",
      "[8% 100/1250]\tAcc: 77.97\tData (t) 0.003\tBatch (t) 0.039\n",
      "[10% 120/1250]\tAcc: 78.00\tData (t) 0.010\tBatch (t) 0.045\n",
      "[11% 140/1250]\tAcc: 78.28\tData (t) 0.013\tBatch (t) 0.043\n",
      "[13% 160/1250]\tAcc: 78.73\tData (t) 0.001\tBatch (t) 0.038\n",
      "[14% 180/1250]\tAcc: 78.73\tData (t) 0.001\tBatch (t) 0.028\n",
      "[16% 200/1250]\tAcc: 78.92\tData (t) 0.001\tBatch (t) 0.028\n",
      "[18% 220/1250]\tAcc: 78.90\tData (t) 0.001\tBatch (t) 0.028\n",
      "[19% 240/1250]\tAcc: 79.10\tData (t) 0.003\tBatch (t) 0.030\n",
      "[21% 260/1250]\tAcc: 79.17\tData (t) 0.001\tBatch (t) 0.029\n",
      "[22% 280/1250]\tAcc: 79.49\tData (t) 0.001\tBatch (t) 0.027\n",
      "[24% 300/1250]\tAcc: 79.82\tData (t) 0.002\tBatch (t) 0.031\n",
      "[26% 320/1250]\tAcc: 80.02\tData (t) 0.001\tBatch (t) 0.029\n",
      "[27% 340/1250]\tAcc: 79.99\tData (t) 0.001\tBatch (t) 0.027\n",
      "[29% 360/1250]\tAcc: 79.78\tData (t) 0.002\tBatch (t) 0.030\n",
      "[30% 380/1250]\tAcc: 79.66\tData (t) 0.004\tBatch (t) 0.031\n",
      "[32% 400/1250]\tAcc: 79.80\tData (t) 0.001\tBatch (t) 0.028\n",
      "[34% 420/1250]\tAcc: 79.84\tData (t) 0.001\tBatch (t) 0.029\n",
      "[35% 440/1250]\tAcc: 80.07\tData (t) 0.004\tBatch (t) 0.029\n",
      "[37% 460/1250]\tAcc: 79.91\tData (t) 0.001\tBatch (t) 0.028\n",
      "[38% 480/1250]\tAcc: 79.89\tData (t) 0.001\tBatch (t) 0.028\n",
      "[40% 500/1250]\tAcc: 80.04\tData (t) 0.001\tBatch (t) 0.028\n",
      "[42% 520/1250]\tAcc: 80.09\tData (t) 0.001\tBatch (t) 0.034\n",
      "[43% 540/1250]\tAcc: 80.06\tData (t) 0.001\tBatch (t) 0.059\n",
      "[45% 560/1250]\tAcc: 80.17\tData (t) 0.001\tBatch (t) 0.067\n",
      "[46% 580/1250]\tAcc: 80.14\tData (t) 0.004\tBatch (t) 0.079\n",
      "[48% 600/1250]\tAcc: 80.22\tData (t) 0.001\tBatch (t) 0.029\n",
      "[50% 620/1250]\tAcc: 80.09\tData (t) 0.001\tBatch (t) 0.037\n",
      "[51% 640/1250]\tAcc: 80.01\tData (t) 0.001\tBatch (t) 0.027\n",
      "[53% 660/1250]\tAcc: 80.01\tData (t) 0.001\tBatch (t) 0.029\n",
      "[54% 680/1250]\tAcc: 80.08\tData (t) 0.001\tBatch (t) 0.029\n",
      "[56% 700/1250]\tAcc: 80.10\tData (t) 0.001\tBatch (t) 0.029\n",
      "[58% 720/1250]\tAcc: 80.05\tData (t) 0.001\tBatch (t) 0.028\n",
      "[59% 740/1250]\tAcc: 80.15\tData (t) 0.001\tBatch (t) 0.027\n",
      "[61% 760/1250]\tAcc: 80.01\tData (t) 0.001\tBatch (t) 0.028\n",
      "[62% 780/1250]\tAcc: 79.96\tData (t) 0.001\tBatch (t) 0.029\n",
      "[64% 800/1250]\tAcc: 79.87\tData (t) 0.001\tBatch (t) 0.029\n",
      "[66% 820/1250]\tAcc: 79.87\tData (t) 0.001\tBatch (t) 0.027\n",
      "[67% 840/1250]\tAcc: 79.86\tData (t) 0.006\tBatch (t) 0.032\n",
      "[69% 860/1250]\tAcc: 79.79\tData (t) 0.001\tBatch (t) 0.028\n",
      "[70% 880/1250]\tAcc: 79.82\tData (t) 0.002\tBatch (t) 0.029\n",
      "[72% 900/1250]\tAcc: 79.84\tData (t) 0.001\tBatch (t) 0.029\n",
      "[74% 920/1250]\tAcc: 79.82\tData (t) 0.001\tBatch (t) 0.025\n",
      "[75% 940/1250]\tAcc: 79.80\tData (t) 0.013\tBatch (t) 0.035\n",
      "[77% 960/1250]\tAcc: 79.80\tData (t) 0.015\tBatch (t) 0.041\n",
      "[78% 980/1250]\tAcc: 79.87\tData (t) 0.001\tBatch (t) 0.041\n",
      "[80% 1000/1250]\tAcc: 79.88\tData (t) 0.001\tBatch (t) 0.024\n",
      "[82% 1020/1250]\tAcc: 79.91\tData (t) 0.008\tBatch (t) 0.048\n",
      "[83% 1040/1250]\tAcc: 79.85\tData (t) 0.001\tBatch (t) 0.028\n",
      "[85% 1060/1250]\tAcc: 79.85\tData (t) 0.001\tBatch (t) 0.028\n",
      "[86% 1080/1250]\tAcc: 79.86\tData (t) 0.001\tBatch (t) 0.028\n",
      "[88% 1100/1250]\tAcc: 79.89\tData (t) 0.001\tBatch (t) 0.028\n",
      "[90% 1120/1250]\tAcc: 79.91\tData (t) 0.001\tBatch (t) 0.028\n",
      "[91% 1140/1250]\tAcc: 79.85\tData (t) 0.001\tBatch (t) 0.031\n",
      "[93% 1160/1250]\tAcc: 79.94\tData (t) 0.001\tBatch (t) 0.027\n",
      "[94% 1180/1250]\tAcc: 79.89\tData (t) 0.001\tBatch (t) 0.028\n",
      "[96% 1200/1250]\tAcc: 79.89\tData (t) 0.001\tBatch (t) 0.028\n",
      "[98% 1220/1250]\tAcc: 79.92\tData (t) 0.003\tBatch (t) 0.030\n",
      "[99% 1240/1250]\tAcc: 79.99\tData (t) 0.001\tBatch (t) 0.029\n",
      "[0% 0/1250]\tAcc: 87.50\tData (t) 0.155\tBatch (t) 0.235\n",
      "[2% 20/1250]\tAcc: 85.12\tData (t) 0.006\tBatch (t) 0.076\n",
      "[3% 40/1250]\tAcc: 78.66\tData (t) 0.004\tBatch (t) 0.031\n",
      "[5% 60/1250]\tAcc: 79.51\tData (t) 0.001\tBatch (t) 0.027\n",
      "[6% 80/1250]\tAcc: 79.63\tData (t) 0.001\tBatch (t) 0.028\n",
      "[8% 100/1250]\tAcc: 80.20\tData (t) 0.001\tBatch (t) 0.031\n",
      "[10% 120/1250]\tAcc: 78.93\tData (t) 0.005\tBatch (t) 0.035\n",
      "[11% 140/1250]\tAcc: 79.52\tData (t) 0.001\tBatch (t) 0.059\n",
      "[13% 160/1250]\tAcc: 80.05\tData (t) 0.019\tBatch (t) 0.074\n",
      "[14% 180/1250]\tAcc: 80.39\tData (t) 0.001\tBatch (t) 0.040\n",
      "[16% 200/1250]\tAcc: 79.85\tData (t) 0.001\tBatch (t) 0.033\n",
      "[18% 220/1250]\tAcc: 79.86\tData (t) 0.004\tBatch (t) 0.032\n",
      "[19% 240/1250]\tAcc: 80.24\tData (t) 0.001\tBatch (t) 0.029\n",
      "[21% 260/1250]\tAcc: 79.89\tData (t) 0.001\tBatch (t) 0.027\n",
      "[22% 280/1250]\tAcc: 79.76\tData (t) 0.001\tBatch (t) 0.028\n",
      "[24% 300/1250]\tAcc: 79.61\tData (t) 0.001\tBatch (t) 0.028\n",
      "[26% 320/1250]\tAcc: 79.44\tData (t) 0.002\tBatch (t) 0.030\n",
      "[27% 340/1250]\tAcc: 79.80\tData (t) 0.001\tBatch (t) 0.027\n",
      "[29% 360/1250]\tAcc: 80.02\tData (t) 0.001\tBatch (t) 0.029\n",
      "[30% 380/1250]\tAcc: 80.28\tData (t) 0.001\tBatch (t) 0.029\n",
      "[32% 400/1250]\tAcc: 80.21\tData (t) 0.001\tBatch (t) 0.029\n",
      "[34% 420/1250]\tAcc: 80.34\tData (t) 0.002\tBatch (t) 0.029\n",
      "[35% 440/1250]\tAcc: 80.16\tData (t) 0.007\tBatch (t) 0.039\n",
      "[37% 460/1250]\tAcc: 80.29\tData (t) 0.002\tBatch (t) 0.029\n",
      "[38% 480/1250]\tAcc: 80.02\tData (t) 0.001\tBatch (t) 0.029\n",
      "[40% 500/1250]\tAcc: 80.01\tData (t) 0.001\tBatch (t) 0.029\n",
      "[42% 520/1250]\tAcc: 80.16\tData (t) 0.001\tBatch (t) 0.028\n",
      "[43% 540/1250]\tAcc: 80.24\tData (t) 0.007\tBatch (t) 0.035\n",
      "[45% 560/1250]\tAcc: 80.17\tData (t) 0.002\tBatch (t) 0.031\n",
      "[46% 580/1250]\tAcc: 80.38\tData (t) 0.011\tBatch (t) 0.052\n",
      "[48% 600/1250]\tAcc: 80.39\tData (t) 0.001\tBatch (t) 0.042\n",
      "[50% 620/1250]\tAcc: 80.27\tData (t) 0.002\tBatch (t) 0.029\n",
      "[51% 640/1250]\tAcc: 80.07\tData (t) 0.001\tBatch (t) 0.028\n",
      "[53% 660/1250]\tAcc: 79.90\tData (t) 0.001\tBatch (t) 0.028\n",
      "[54% 680/1250]\tAcc: 80.01\tData (t) 0.006\tBatch (t) 0.029\n",
      "[56% 700/1250]\tAcc: 80.15\tData (t) 0.001\tBatch (t) 0.029\n",
      "[58% 720/1250]\tAcc: 80.03\tData (t) 0.001\tBatch (t) 0.029\n",
      "[59% 740/1250]\tAcc: 80.04\tData (t) 0.002\tBatch (t) 0.030\n",
      "[61% 760/1250]\tAcc: 80.17\tData (t) 0.001\tBatch (t) 0.032\n",
      "[62% 780/1250]\tAcc: 80.12\tData (t) 0.001\tBatch (t) 0.027\n",
      "[64% 800/1250]\tAcc: 80.09\tData (t) 0.001\tBatch (t) 0.027\n",
      "[66% 820/1250]\tAcc: 79.99\tData (t) 0.002\tBatch (t) 0.033\n",
      "[67% 840/1250]\tAcc: 79.62\tData (t) 0.001\tBatch (t) 0.029\n",
      "[69% 860/1250]\tAcc: 79.56\tData (t) 0.001\tBatch (t) 0.028\n",
      "[70% 880/1250]\tAcc: 79.33\tData (t) 0.004\tBatch (t) 0.032\n",
      "[72% 900/1250]\tAcc: 79.41\tData (t) 0.001\tBatch (t) 0.025\n",
      "[74% 920/1250]\tAcc: 79.59\tData (t) 0.001\tBatch (t) 0.029\n",
      "[75% 940/1250]\tAcc: 79.61\tData (t) 0.003\tBatch (t) 0.036\n",
      "[77% 960/1250]\tAcc: 79.64\tData (t) 0.001\tBatch (t) 0.037\n",
      "[78% 980/1250]\tAcc: 79.51\tData (t) 0.001\tBatch (t) 0.041\n",
      "[80% 1000/1250]\tAcc: 79.43\tData (t) 0.003\tBatch (t) 0.056\n",
      "[82% 1020/1250]\tAcc: 79.52\tData (t) 0.001\tBatch (t) 0.028\n",
      "[83% 1040/1250]\tAcc: 79.62\tData (t) 0.001\tBatch (t) 0.028\n",
      "[85% 1060/1250]\tAcc: 79.55\tData (t) 0.002\tBatch (t) 0.028\n",
      "[86% 1080/1250]\tAcc: 79.59\tData (t) 0.001\tBatch (t) 0.030\n",
      "[88% 1100/1250]\tAcc: 79.54\tData (t) 0.001\tBatch (t) 0.028\n",
      "[90% 1120/1250]\tAcc: 79.59\tData (t) 0.014\tBatch (t) 0.061\n",
      "[91% 1140/1250]\tAcc: 79.67\tData (t) 0.012\tBatch (t) 0.053\n",
      "[93% 1160/1250]\tAcc: 79.76\tData (t) 0.001\tBatch (t) 0.022\n",
      "[94% 1180/1250]\tAcc: 79.83\tData (t) 0.002\tBatch (t) 0.035\n",
      "[96% 1200/1250]\tAcc: 79.95\tData (t) 0.002\tBatch (t) 0.027\n",
      "[98% 1220/1250]\tAcc: 80.12\tData (t) 0.001\tBatch (t) 0.047\n",
      "[99% 1240/1250]\tAcc: 80.04\tData (t) 0.008\tBatch (t) 0.041\n",
      "Config 3 Accuracy: 80.01%\n",
      "\n",
      "📊 Evaluating Config 4...\n",
      "[0% 0/1250]\tAcc: 87.50\tData (t) 0.246\tBatch (t) 0.292\n",
      "[2% 20/1250]\tAcc: 77.38\tData (t) 0.001\tBatch (t) 0.028\n",
      "[3% 40/1250]\tAcc: 75.00\tData (t) 0.001\tBatch (t) 0.053\n",
      "[5% 60/1250]\tAcc: 76.43\tData (t) 0.010\tBatch (t) 0.040\n",
      "[6% 80/1250]\tAcc: 76.23\tData (t) 0.001\tBatch (t) 0.028\n",
      "[8% 100/1250]\tAcc: 76.36\tData (t) 0.005\tBatch (t) 0.035\n",
      "[10% 120/1250]\tAcc: 76.34\tData (t) 0.007\tBatch (t) 0.071\n",
      "[11% 140/1250]\tAcc: 76.60\tData (t) 0.001\tBatch (t) 0.028\n",
      "[13% 160/1250]\tAcc: 76.86\tData (t) 0.002\tBatch (t) 0.029\n",
      "[14% 180/1250]\tAcc: 77.28\tData (t) 0.002\tBatch (t) 0.028\n",
      "[16% 200/1250]\tAcc: 77.55\tData (t) 0.001\tBatch (t) 0.029\n",
      "[18% 220/1250]\tAcc: 77.32\tData (t) 0.001\tBatch (t) 0.027\n",
      "[19% 240/1250]\tAcc: 77.39\tData (t) 0.001\tBatch (t) 0.030\n",
      "[21% 260/1250]\tAcc: 77.63\tData (t) 0.004\tBatch (t) 0.031\n",
      "[22% 280/1250]\tAcc: 78.02\tData (t) 0.001\tBatch (t) 0.028\n",
      "[24% 300/1250]\tAcc: 78.03\tData (t) 0.001\tBatch (t) 0.028\n",
      "[26% 320/1250]\tAcc: 78.19\tData (t) 0.001\tBatch (t) 0.028\n",
      "[27% 340/1250]\tAcc: 77.86\tData (t) 0.001\tBatch (t) 0.029\n",
      "[29% 360/1250]\tAcc: 77.87\tData (t) 0.002\tBatch (t) 0.030\n",
      "[30% 380/1250]\tAcc: 77.76\tData (t) 0.001\tBatch (t) 0.029\n",
      "[32% 400/1250]\tAcc: 77.71\tData (t) 0.001\tBatch (t) 0.029\n",
      "[34% 420/1250]\tAcc: 77.82\tData (t) 0.002\tBatch (t) 0.029\n",
      "[35% 440/1250]\tAcc: 78.09\tData (t) 0.001\tBatch (t) 0.028\n",
      "[37% 460/1250]\tAcc: 78.25\tData (t) 0.001\tBatch (t) 0.024\n",
      "[38% 480/1250]\tAcc: 78.43\tData (t) 0.011\tBatch (t) 0.043\n",
      "[40% 500/1250]\tAcc: 78.42\tData (t) 0.009\tBatch (t) 0.039\n",
      "[42% 520/1250]\tAcc: 78.50\tData (t) 0.001\tBatch (t) 0.039\n",
      "[43% 540/1250]\tAcc: 78.49\tData (t) 0.001\tBatch (t) 0.031\n",
      "[45% 560/1250]\tAcc: 78.63\tData (t) 0.001\tBatch (t) 0.033\n",
      "[46% 580/1250]\tAcc: 78.68\tData (t) 0.001\tBatch (t) 0.031\n",
      "[48% 600/1250]\tAcc: 78.66\tData (t) 0.001\tBatch (t) 0.029\n",
      "[50% 620/1250]\tAcc: 78.54\tData (t) 0.001\tBatch (t) 0.027\n",
      "[51% 640/1250]\tAcc: 78.47\tData (t) 0.001\tBatch (t) 0.029\n",
      "[53% 660/1250]\tAcc: 78.48\tData (t) 0.001\tBatch (t) 0.027\n",
      "[54% 680/1250]\tAcc: 78.47\tData (t) 0.001\tBatch (t) 0.028\n",
      "[56% 700/1250]\tAcc: 78.53\tData (t) 0.001\tBatch (t) 0.028\n",
      "[58% 720/1250]\tAcc: 78.54\tData (t) 0.005\tBatch (t) 0.030\n",
      "[59% 740/1250]\tAcc: 78.58\tData (t) 0.001\tBatch (t) 0.027\n",
      "[61% 760/1250]\tAcc: 78.58\tData (t) 0.001\tBatch (t) 0.028\n",
      "[62% 780/1250]\tAcc: 78.49\tData (t) 0.002\tBatch (t) 0.032\n",
      "[64% 800/1250]\tAcc: 78.45\tData (t) 0.003\tBatch (t) 0.031\n",
      "[66% 820/1250]\tAcc: 78.43\tData (t) 0.001\tBatch (t) 0.029\n",
      "[67% 840/1250]\tAcc: 78.43\tData (t) 0.001\tBatch (t) 0.026\n",
      "[69% 860/1250]\tAcc: 78.40\tData (t) 0.001\tBatch (t) 0.030\n",
      "[70% 880/1250]\tAcc: 78.49\tData (t) 0.001\tBatch (t) 0.028\n",
      "[72% 900/1250]\tAcc: 78.57\tData (t) 0.001\tBatch (t) 0.028\n",
      "[74% 920/1250]\tAcc: 78.54\tData (t) 0.001\tBatch (t) 0.030\n",
      "[75% 940/1250]\tAcc: 78.59\tData (t) 0.016\tBatch (t) 0.057\n",
      "[77% 960/1250]\tAcc: 78.51\tData (t) 0.001\tBatch (t) 0.030\n",
      "[78% 980/1250]\tAcc: 78.56\tData (t) 0.001\tBatch (t) 0.043\n",
      "[80% 1000/1250]\tAcc: 78.51\tData (t) 0.001\tBatch (t) 0.036\n",
      "[82% 1020/1250]\tAcc: 78.43\tData (t) 0.002\tBatch (t) 0.028\n",
      "[83% 1040/1250]\tAcc: 78.39\tData (t) 0.001\tBatch (t) 0.028\n",
      "[85% 1060/1250]\tAcc: 78.36\tData (t) 0.001\tBatch (t) 0.038\n",
      "[86% 1080/1250]\tAcc: 78.31\tData (t) 0.001\tBatch (t) 0.028\n",
      "[88% 1100/1250]\tAcc: 78.38\tData (t) 0.005\tBatch (t) 0.032\n",
      "[90% 1120/1250]\tAcc: 78.32\tData (t) 0.001\tBatch (t) 0.027\n",
      "[91% 1140/1250]\tAcc: 78.36\tData (t) 0.001\tBatch (t) 0.029\n",
      "[93% 1160/1250]\tAcc: 78.36\tData (t) 0.001\tBatch (t) 0.028\n",
      "[94% 1180/1250]\tAcc: 78.36\tData (t) 0.002\tBatch (t) 0.030\n",
      "[96% 1200/1250]\tAcc: 78.37\tData (t) 0.001\tBatch (t) 0.029\n",
      "[98% 1220/1250]\tAcc: 78.42\tData (t) 0.001\tBatch (t) 0.029\n",
      "[99% 1240/1250]\tAcc: 78.53\tData (t) 0.004\tBatch (t) 0.028\n",
      "[0% 0/1250]\tAcc: 100.00\tData (t) 0.127\tBatch (t) 0.162\n",
      "[2% 20/1250]\tAcc: 85.12\tData (t) 0.001\tBatch (t) 0.028\n",
      "[3% 40/1250]\tAcc: 79.57\tData (t) 0.002\tBatch (t) 0.028\n",
      "[5% 60/1250]\tAcc: 79.30\tData (t) 0.004\tBatch (t) 0.031\n",
      "[6% 80/1250]\tAcc: 79.32\tData (t) 0.001\tBatch (t) 0.028\n",
      "[8% 100/1250]\tAcc: 78.71\tData (t) 0.001\tBatch (t) 0.029\n",
      "[10% 120/1250]\tAcc: 78.41\tData (t) 0.009\tBatch (t) 0.051\n",
      "[11% 140/1250]\tAcc: 79.52\tData (t) 0.001\tBatch (t) 0.032\n",
      "[13% 160/1250]\tAcc: 79.50\tData (t) 0.007\tBatch (t) 0.041\n",
      "[14% 180/1250]\tAcc: 79.21\tData (t) 0.004\tBatch (t) 0.034\n",
      "[16% 200/1250]\tAcc: 78.86\tData (t) 0.001\tBatch (t) 0.038\n",
      "[18% 220/1250]\tAcc: 78.85\tData (t) 0.001\tBatch (t) 0.029\n",
      "[19% 240/1250]\tAcc: 78.84\tData (t) 0.001\tBatch (t) 0.030\n",
      "[21% 260/1250]\tAcc: 77.54\tData (t) 0.001\tBatch (t) 0.032\n",
      "[22% 280/1250]\tAcc: 78.07\tData (t) 0.001\tBatch (t) 0.028\n",
      "[24% 300/1250]\tAcc: 78.41\tData (t) 0.001\tBatch (t) 0.030\n",
      "[26% 320/1250]\tAcc: 78.12\tData (t) 0.001\tBatch (t) 0.028\n",
      "[27% 340/1250]\tAcc: 78.48\tData (t) 0.001\tBatch (t) 0.028\n",
      "[29% 360/1250]\tAcc: 78.84\tData (t) 0.001\tBatch (t) 0.027\n",
      "[30% 380/1250]\tAcc: 79.10\tData (t) 0.001\tBatch (t) 0.028\n",
      "[32% 400/1250]\tAcc: 78.68\tData (t) 0.001\tBatch (t) 0.028\n",
      "[34% 420/1250]\tAcc: 78.74\tData (t) 0.001\tBatch (t) 0.024\n",
      "[35% 440/1250]\tAcc: 78.74\tData (t) 0.001\tBatch (t) 0.029\n",
      "[37% 460/1250]\tAcc: 78.85\tData (t) 0.001\tBatch (t) 0.029\n",
      "[38% 480/1250]\tAcc: 78.85\tData (t) 0.001\tBatch (t) 0.028\n",
      "[40% 500/1250]\tAcc: 78.67\tData (t) 0.001\tBatch (t) 0.029\n",
      "[42% 520/1250]\tAcc: 78.67\tData (t) 0.001\tBatch (t) 0.028\n",
      "[43% 540/1250]\tAcc: 78.72\tData (t) 0.001\tBatch (t) 0.028\n",
      "[45% 560/1250]\tAcc: 78.81\tData (t) 0.001\tBatch (t) 0.025\n",
      "[46% 580/1250]\tAcc: 78.96\tData (t) 0.001\tBatch (t) 0.034\n",
      "[48% 600/1250]\tAcc: 79.20\tData (t) 0.003\tBatch (t) 0.049\n",
      "[50% 620/1250]\tAcc: 79.25\tData (t) 0.001\tBatch (t) 0.046\n",
      "[51% 640/1250]\tAcc: 79.13\tData (t) 0.001\tBatch (t) 0.022\n",
      "[53% 660/1250]\tAcc: 79.07\tData (t) 0.004\tBatch (t) 0.031\n",
      "[54% 680/1250]\tAcc: 79.11\tData (t) 0.001\tBatch (t) 0.026\n",
      "[56% 700/1250]\tAcc: 79.35\tData (t) 0.001\tBatch (t) 0.028\n",
      "[58% 720/1250]\tAcc: 79.28\tData (t) 0.002\tBatch (t) 0.030\n",
      "[59% 740/1250]\tAcc: 79.32\tData (t) 0.001\tBatch (t) 0.029\n",
      "[61% 760/1250]\tAcc: 79.42\tData (t) 0.012\tBatch (t) 0.038\n",
      "[62% 780/1250]\tAcc: 79.40\tData (t) 0.001\tBatch (t) 0.028\n",
      "[64% 800/1250]\tAcc: 79.34\tData (t) 0.001\tBatch (t) 0.028\n",
      "[66% 820/1250]\tAcc: 79.26\tData (t) 0.002\tBatch (t) 0.029\n",
      "[67% 840/1250]\tAcc: 78.85\tData (t) 0.001\tBatch (t) 0.028\n",
      "[69% 860/1250]\tAcc: 78.88\tData (t) 0.001\tBatch (t) 0.040\n",
      "[70% 880/1250]\tAcc: 78.70\tData (t) 0.001\tBatch (t) 0.029\n",
      "[72% 900/1250]\tAcc: 78.80\tData (t) 0.001\tBatch (t) 0.027\n",
      "[74% 920/1250]\tAcc: 78.90\tData (t) 0.001\tBatch (t) 0.028\n",
      "[75% 940/1250]\tAcc: 78.96\tData (t) 0.001\tBatch (t) 0.028\n",
      "[77% 960/1250]\tAcc: 78.92\tData (t) 0.001\tBatch (t) 0.029\n",
      "[78% 980/1250]\tAcc: 78.81\tData (t) 0.002\tBatch (t) 0.027\n",
      "[80% 1000/1250]\tAcc: 78.58\tData (t) 0.010\tBatch (t) 0.038\n",
      "[82% 1020/1250]\tAcc: 78.65\tData (t) 0.006\tBatch (t) 0.044\n",
      "[83% 1040/1250]\tAcc: 78.78\tData (t) 0.001\tBatch (t) 0.044\n",
      "[85% 1060/1250]\tAcc: 78.73\tData (t) 0.016\tBatch (t) 0.041\n",
      "[86% 1080/1250]\tAcc: 78.64\tData (t) 0.001\tBatch (t) 0.025\n",
      "[88% 1100/1250]\tAcc: 78.50\tData (t) 0.002\tBatch (t) 0.029\n",
      "[90% 1120/1250]\tAcc: 78.45\tData (t) 0.001\tBatch (t) 0.028\n",
      "[91% 1140/1250]\tAcc: 78.37\tData (t) 0.001\tBatch (t) 0.027\n",
      "[93% 1160/1250]\tAcc: 78.35\tData (t) 0.001\tBatch (t) 0.028\n",
      "[94% 1180/1250]\tAcc: 78.47\tData (t) 0.003\tBatch (t) 0.030\n",
      "[96% 1200/1250]\tAcc: 78.55\tData (t) 0.002\tBatch (t) 0.030\n",
      "[98% 1220/1250]\tAcc: 78.67\tData (t) 0.001\tBatch (t) 0.028\n",
      "[99% 1240/1250]\tAcc: 78.64\tData (t) 0.001\tBatch (t) 0.027\n",
      "Config 4 Accuracy: 78.52%\n",
      "\n",
      "📊 Evaluating Config 5...\n",
      "[0% 0/1250]\tAcc: 50.00\tData (t) 0.127\tBatch (t) 0.161\n",
      "[2% 20/1250]\tAcc: 72.02\tData (t) 0.005\tBatch (t) 0.032\n",
      "[3% 40/1250]\tAcc: 73.17\tData (t) 0.001\tBatch (t) 0.027\n",
      "[5% 60/1250]\tAcc: 73.57\tData (t) 0.001\tBatch (t) 0.027\n",
      "[6% 80/1250]\tAcc: 74.38\tData (t) 0.001\tBatch (t) 0.027\n",
      "[8% 100/1250]\tAcc: 75.00\tData (t) 0.002\tBatch (t) 0.030\n",
      "[10% 120/1250]\tAcc: 75.52\tData (t) 0.001\tBatch (t) 0.029\n",
      "[11% 140/1250]\tAcc: 75.62\tData (t) 0.001\tBatch (t) 0.028\n",
      "[13% 160/1250]\tAcc: 76.40\tData (t) 0.001\tBatch (t) 0.027\n",
      "[14% 180/1250]\tAcc: 76.59\tData (t) 0.006\tBatch (t) 0.037\n",
      "[16% 200/1250]\tAcc: 76.68\tData (t) 0.009\tBatch (t) 0.048\n",
      "[18% 220/1250]\tAcc: 76.98\tData (t) 0.003\tBatch (t) 0.037\n",
      "[19% 240/1250]\tAcc: 77.13\tData (t) 0.016\tBatch (t) 0.045\n",
      "[21% 260/1250]\tAcc: 77.20\tData (t) 0.001\tBatch (t) 0.036\n",
      "[22% 280/1250]\tAcc: 77.71\tData (t) 0.001\tBatch (t) 0.028\n",
      "[24% 300/1250]\tAcc: 78.16\tData (t) 0.001\tBatch (t) 0.021\n",
      "[26% 320/1250]\tAcc: 78.31\tData (t) 0.004\tBatch (t) 0.030\n",
      "[27% 340/1250]\tAcc: 78.23\tData (t) 0.001\tBatch (t) 0.029\n",
      "[29% 360/1250]\tAcc: 78.19\tData (t) 0.001\tBatch (t) 0.029\n",
      "[30% 380/1250]\tAcc: 78.12\tData (t) 0.001\tBatch (t) 0.029\n",
      "[32% 400/1250]\tAcc: 78.24\tData (t) 0.010\tBatch (t) 0.036\n",
      "[34% 420/1250]\tAcc: 78.21\tData (t) 0.001\tBatch (t) 0.029\n",
      "[35% 440/1250]\tAcc: 78.37\tData (t) 0.001\tBatch (t) 0.027\n",
      "[37% 460/1250]\tAcc: 78.34\tData (t) 0.002\tBatch (t) 0.028\n",
      "[38% 480/1250]\tAcc: 78.33\tData (t) 0.001\tBatch (t) 0.028\n",
      "[40% 500/1250]\tAcc: 78.49\tData (t) 0.001\tBatch (t) 0.028\n",
      "[42% 520/1250]\tAcc: 78.53\tData (t) 0.001\tBatch (t) 0.029\n",
      "[43% 540/1250]\tAcc: 78.49\tData (t) 0.001\tBatch (t) 0.028\n",
      "[45% 560/1250]\tAcc: 78.65\tData (t) 0.003\tBatch (t) 0.031\n",
      "[46% 580/1250]\tAcc: 78.70\tData (t) 0.004\tBatch (t) 0.032\n",
      "[48% 600/1250]\tAcc: 78.81\tData (t) 0.001\tBatch (t) 0.029\n",
      "[50% 620/1250]\tAcc: 78.74\tData (t) 0.001\tBatch (t) 0.027\n",
      "[51% 640/1250]\tAcc: 78.55\tData (t) 0.001\tBatch (t) 0.038\n",
      "[53% 660/1250]\tAcc: 78.56\tData (t) 0.001\tBatch (t) 0.036\n",
      "[54% 680/1250]\tAcc: 78.60\tData (t) 0.014\tBatch (t) 0.045\n",
      "[56% 700/1250]\tAcc: 78.69\tData (t) 0.001\tBatch (t) 0.030\n",
      "[58% 720/1250]\tAcc: 78.57\tData (t) 0.001\tBatch (t) 0.028\n",
      "[59% 740/1250]\tAcc: 78.69\tData (t) 0.001\tBatch (t) 0.028\n",
      "[61% 760/1250]\tAcc: 78.61\tData (t) 0.001\tBatch (t) 0.032\n",
      "[62% 780/1250]\tAcc: 78.60\tData (t) 0.002\tBatch (t) 0.027\n",
      "[64% 800/1250]\tAcc: 78.43\tData (t) 0.002\tBatch (t) 0.031\n",
      "[66% 820/1250]\tAcc: 78.43\tData (t) 0.001\tBatch (t) 0.028\n",
      "[67% 840/1250]\tAcc: 78.33\tData (t) 0.002\tBatch (t) 0.029\n",
      "[69% 860/1250]\tAcc: 78.27\tData (t) 0.001\tBatch (t) 0.028\n",
      "[70% 880/1250]\tAcc: 78.31\tData (t) 0.009\tBatch (t) 0.073\n",
      "[72% 900/1250]\tAcc: 78.29\tData (t) 0.001\tBatch (t) 0.027\n",
      "[74% 920/1250]\tAcc: 78.23\tData (t) 0.001\tBatch (t) 0.028\n",
      "[75% 940/1250]\tAcc: 78.17\tData (t) 0.001\tBatch (t) 0.028\n",
      "[77% 960/1250]\tAcc: 78.19\tData (t) 0.001\tBatch (t) 0.029\n",
      "[78% 980/1250]\tAcc: 78.27\tData (t) 0.001\tBatch (t) 0.027\n",
      "[80% 1000/1250]\tAcc: 78.21\tData (t) 0.007\tBatch (t) 0.037\n",
      "[82% 1020/1250]\tAcc: 78.22\tData (t) 0.002\tBatch (t) 0.029\n",
      "[83% 1040/1250]\tAcc: 78.12\tData (t) 0.007\tBatch (t) 0.047\n",
      "[85% 1060/1250]\tAcc: 78.13\tData (t) 0.001\tBatch (t) 0.023\n",
      "[86% 1080/1250]\tAcc: 78.17\tData (t) 0.001\tBatch (t) 0.038\n",
      "[88% 1100/1250]\tAcc: 78.14\tData (t) 0.001\tBatch (t) 0.037\n",
      "[90% 1120/1250]\tAcc: 78.14\tData (t) 0.001\tBatch (t) 0.050\n",
      "[91% 1140/1250]\tAcc: 78.12\tData (t) 0.001\tBatch (t) 0.028\n",
      "[93% 1160/1250]\tAcc: 78.15\tData (t) 0.001\tBatch (t) 0.028\n",
      "[94% 1180/1250]\tAcc: 78.07\tData (t) 0.001\tBatch (t) 0.028\n",
      "[96% 1200/1250]\tAcc: 78.04\tData (t) 0.001\tBatch (t) 0.029\n",
      "[98% 1220/1250]\tAcc: 78.06\tData (t) 0.001\tBatch (t) 0.030\n",
      "[99% 1240/1250]\tAcc: 78.14\tData (t) 0.001\tBatch (t) 0.028\n",
      "[0% 0/1250]\tAcc: 87.50\tData (t) 0.125\tBatch (t) 0.158\n",
      "[2% 20/1250]\tAcc: 85.71\tData (t) 0.004\tBatch (t) 0.029\n",
      "[3% 40/1250]\tAcc: 79.27\tData (t) 0.001\tBatch (t) 0.029\n",
      "[5% 60/1250]\tAcc: 78.48\tData (t) 0.004\tBatch (t) 0.030\n",
      "[6% 80/1250]\tAcc: 77.62\tData (t) 0.001\tBatch (t) 0.027\n",
      "[8% 100/1250]\tAcc: 78.59\tData (t) 0.001\tBatch (t) 0.025\n",
      "[10% 120/1250]\tAcc: 77.38\tData (t) 0.001\tBatch (t) 0.027\n",
      "[11% 140/1250]\tAcc: 78.28\tData (t) 0.002\tBatch (t) 0.028\n",
      "[13% 160/1250]\tAcc: 79.04\tData (t) 0.002\tBatch (t) 0.029\n",
      "[14% 180/1250]\tAcc: 79.14\tData (t) 0.003\tBatch (t) 0.032\n",
      "[16% 200/1250]\tAcc: 78.73\tData (t) 0.002\tBatch (t) 0.029\n",
      "[18% 220/1250]\tAcc: 78.39\tData (t) 0.009\tBatch (t) 0.055\n",
      "[19% 240/1250]\tAcc: 78.84\tData (t) 0.007\tBatch (t) 0.043\n",
      "[21% 260/1250]\tAcc: 78.11\tData (t) 0.002\tBatch (t) 0.029\n",
      "[22% 280/1250]\tAcc: 77.98\tData (t) 0.001\tBatch (t) 0.034\n",
      "[24% 300/1250]\tAcc: 77.91\tData (t) 0.014\tBatch (t) 0.051\n",
      "[26% 320/1250]\tAcc: 77.57\tData (t) 0.001\tBatch (t) 0.026\n",
      "[27% 340/1250]\tAcc: 78.04\tData (t) 0.001\tBatch (t) 0.030\n",
      "[29% 360/1250]\tAcc: 78.39\tData (t) 0.001\tBatch (t) 0.027\n",
      "[30% 380/1250]\tAcc: 78.71\tData (t) 0.001\tBatch (t) 0.028\n",
      "[32% 400/1250]\tAcc: 78.62\tData (t) 0.001\tBatch (t) 0.029\n",
      "[34% 420/1250]\tAcc: 78.68\tData (t) 0.002\tBatch (t) 0.028\n",
      "[35% 440/1250]\tAcc: 78.51\tData (t) 0.001\tBatch (t) 0.028\n",
      "[37% 460/1250]\tAcc: 78.66\tData (t) 0.002\tBatch (t) 0.027\n",
      "[38% 480/1250]\tAcc: 78.35\tData (t) 0.001\tBatch (t) 0.024\n",
      "[40% 500/1250]\tAcc: 78.32\tData (t) 0.001\tBatch (t) 0.027\n",
      "[42% 520/1250]\tAcc: 78.36\tData (t) 0.001\tBatch (t) 0.028\n",
      "[43% 540/1250]\tAcc: 78.28\tData (t) 0.001\tBatch (t) 0.028\n",
      "[45% 560/1250]\tAcc: 78.16\tData (t) 0.001\tBatch (t) 0.029\n",
      "[46% 580/1250]\tAcc: 78.38\tData (t) 0.001\tBatch (t) 0.029\n",
      "[48% 600/1250]\tAcc: 78.41\tData (t) 0.001\tBatch (t) 0.029\n",
      "[50% 620/1250]\tAcc: 78.22\tData (t) 0.001\tBatch (t) 0.029\n",
      "[51% 640/1250]\tAcc: 78.12\tData (t) 0.001\tBatch (t) 0.028\n",
      "[53% 660/1250]\tAcc: 78.03\tData (t) 0.001\tBatch (t) 0.041\n",
      "[54% 680/1250]\tAcc: 78.12\tData (t) 0.001\tBatch (t) 0.041\n",
      "[56% 700/1250]\tAcc: 78.28\tData (t) 0.001\tBatch (t) 0.051\n",
      "[58% 720/1250]\tAcc: 78.19\tData (t) 0.007\tBatch (t) 0.047\n",
      "[59% 740/1250]\tAcc: 78.32\tData (t) 0.002\tBatch (t) 0.048\n",
      "[61% 760/1250]\tAcc: 78.52\tData (t) 0.009\tBatch (t) 0.036\n",
      "[62% 780/1250]\tAcc: 78.38\tData (t) 0.001\tBatch (t) 0.029\n",
      "[64% 800/1250]\tAcc: 78.31\tData (t) 0.001\tBatch (t) 0.028\n",
      "[66% 820/1250]\tAcc: 78.29\tData (t) 0.001\tBatch (t) 0.028\n",
      "[67% 840/1250]\tAcc: 78.03\tData (t) 0.001\tBatch (t) 0.029\n",
      "[69% 860/1250]\tAcc: 77.90\tData (t) 0.001\tBatch (t) 0.028\n",
      "[70% 880/1250]\tAcc: 77.77\tData (t) 0.001\tBatch (t) 0.047\n",
      "[72% 900/1250]\tAcc: 77.96\tData (t) 0.002\tBatch (t) 0.029\n",
      "[74% 920/1250]\tAcc: 78.16\tData (t) 0.001\tBatch (t) 0.027\n",
      "[75% 940/1250]\tAcc: 78.16\tData (t) 0.001\tBatch (t) 0.028\n",
      "[77% 960/1250]\tAcc: 78.21\tData (t) 0.004\tBatch (t) 0.031\n",
      "[78% 980/1250]\tAcc: 78.08\tData (t) 0.004\tBatch (t) 0.031\n",
      "[80% 1000/1250]\tAcc: 77.98\tData (t) 0.001\tBatch (t) 0.029\n",
      "[82% 1020/1250]\tAcc: 78.09\tData (t) 0.001\tBatch (t) 0.028\n",
      "[83% 1040/1250]\tAcc: 78.21\tData (t) 0.001\tBatch (t) 0.028\n",
      "[85% 1060/1250]\tAcc: 78.07\tData (t) 0.002\tBatch (t) 0.029\n",
      "[86% 1080/1250]\tAcc: 78.12\tData (t) 0.001\tBatch (t) 0.029\n",
      "[88% 1100/1250]\tAcc: 77.95\tData (t) 0.001\tBatch (t) 0.028\n",
      "[90% 1120/1250]\tAcc: 78.04\tData (t) 0.007\tBatch (t) 0.035\n",
      "[91% 1140/1250]\tAcc: 78.02\tData (t) 0.011\tBatch (t) 0.045\n",
      "[93% 1160/1250]\tAcc: 78.11\tData (t) 0.009\tBatch (t) 0.040\n",
      "[94% 1180/1250]\tAcc: 78.18\tData (t) 0.001\tBatch (t) 0.031\n",
      "[96% 1200/1250]\tAcc: 78.29\tData (t) 0.001\tBatch (t) 0.029\n",
      "[98% 1220/1250]\tAcc: 78.43\tData (t) 0.001\tBatch (t) 0.029\n",
      "[99% 1240/1250]\tAcc: 78.36\tData (t) 0.002\tBatch (t) 0.028\n",
      "Config 5 Accuracy: 78.16%\n",
      "\n",
      "🎯 Individual Model Results:\n",
      "Config 1: 76.68%\n",
      "Config 2: 80.01%\n",
      "Config 3: 80.01%\n",
      "Config 4: 78.52%\n",
      "Config 5: 78.16%\n",
      "Best Individual: 80.01%\n"
     ]
    }
   ],
   "source": [
    "# Use the test dataset from our data object\n",
    "\n",
    "test_loader = data_tinyImageNet.test_loader\n",
    "val_loader = data_tinyImageNet.val_loader\n",
    "\n",
    "num_classes = len(data_tinyImageNet.classnames)\n",
    "individual_results_test = []\n",
    "individual_results_val = []\n",
    "\n",
    "for i, state_dict in enumerate(state_dicts):\n",
    "    print(f\"\\n📊 Evaluating Config {i+1}...\")\n",
    "    model = get_model_from_sd(state_dict, base_model)\n",
    "    test_accuracy = eval_model_on_dataset(model, test_loader)\n",
    "    val_accuracy = eval_model_on_dataset(model, val_loader)\n",
    "    individual_results_test.append(test_accuracy)\n",
    "    individual_results_val.append(val_accuracy)\n",
    "    print(f\"Config {i+1} Accuracy: {100*test_accuracy:.2f}%\")\n",
    "\n",
    "print(f\"\\n🎯 Individual Model Results:\")\n",
    "for i, acc in enumerate(individual_results_test):\n",
    "    print(f\"Config {i+1}: {100*acc:.2f}%\")\n",
    "print(f\"Best Individual: {100*max(individual_results_test):.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kpW7-cLsR4tJ"
   },
   "source": [
    "## Model Soup\n",
    "Create and evaluate uniform and greedy model soups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "executionInfo": {
     "elapsed": 42,
     "status": "ok",
     "timestamp": 1756053264825,
     "user": {
      "displayName": "Yang Zhou",
      "userId": "09717432835810465973"
     },
     "user_tz": 240
    },
    "id": "E1g7oEF99G--"
   },
   "outputs": [],
   "source": [
    "def create_soup(state_dicts, weights=None):\n",
    "    \"\"\"Create a model soup by averaging state dicts with given weights\"\"\"\n",
    "    if weights is None:\n",
    "        weights = [1.0 / len(state_dicts)] * len(state_dicts)\n",
    "\n",
    "    # Start with the first model weighted\n",
    "    soup_state_dict = {k: v.clone() * weights[0] for k, v in state_dicts[0].items()}\n",
    "\n",
    "    # Add remaining models\n",
    "    for i, state_dict in enumerate(state_dicts[1:], 1):\n",
    "        for k, v in state_dict.items():\n",
    "            soup_state_dict[k] += v.clone() * weights[i]\n",
    "\n",
    "    return soup_state_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 39002,
     "status": "ok",
     "timestamp": 1756053304375,
     "user": {
      "displayName": "Yang Zhou",
      "userId": "09717432835810465973"
     },
     "user_tz": 240
    },
    "id": "cGIeRCZ3R4tJ",
    "outputId": "bf6ce110-e62a-44d9-ff45-9903632d0692"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🍲 Creating Uniform Soup...\n",
      "[0% 0/1250]\tAcc: 75.00\tData (t) 0.160\tBatch (t) 0.201\n",
      "[2% 20/1250]\tAcc: 81.55\tData (t) 0.001\tBatch (t) 0.027\n",
      "[3% 40/1250]\tAcc: 79.88\tData (t) 0.001\tBatch (t) 0.028\n",
      "[5% 60/1250]\tAcc: 81.35\tData (t) 0.001\tBatch (t) 0.027\n",
      "[6% 80/1250]\tAcc: 82.25\tData (t) 0.001\tBatch (t) 0.028\n",
      "[8% 100/1250]\tAcc: 82.80\tData (t) 0.001\tBatch (t) 0.029\n",
      "[10% 120/1250]\tAcc: 82.33\tData (t) 0.001\tBatch (t) 0.027\n",
      "[11% 140/1250]\tAcc: 82.27\tData (t) 0.001\tBatch (t) 0.028\n",
      "[13% 160/1250]\tAcc: 83.00\tData (t) 0.001\tBatch (t) 0.027\n",
      "[14% 180/1250]\tAcc: 83.22\tData (t) 0.001\tBatch (t) 0.038\n",
      "[16% 200/1250]\tAcc: 83.15\tData (t) 0.001\tBatch (t) 0.028\n",
      "[18% 220/1250]\tAcc: 83.03\tData (t) 0.001\tBatch (t) 0.024\n",
      "[19% 240/1250]\tAcc: 83.14\tData (t) 0.001\tBatch (t) 0.042\n",
      "[21% 260/1250]\tAcc: 83.14\tData (t) 0.002\tBatch (t) 0.033\n",
      "[22% 280/1250]\tAcc: 83.59\tData (t) 0.007\tBatch (t) 0.041\n",
      "[24% 300/1250]\tAcc: 83.76\tData (t) 0.010\tBatch (t) 0.047\n",
      "[26% 320/1250]\tAcc: 83.72\tData (t) 0.001\tBatch (t) 0.029\n",
      "[27% 340/1250]\tAcc: 83.47\tData (t) 0.001\tBatch (t) 0.029\n",
      "[29% 360/1250]\tAcc: 83.45\tData (t) 0.001\tBatch (t) 0.029\n",
      "[30% 380/1250]\tAcc: 83.40\tData (t) 0.001\tBatch (t) 0.028\n",
      "[32% 400/1250]\tAcc: 83.35\tData (t) 0.001\tBatch (t) 0.028\n",
      "[34% 420/1250]\tAcc: 83.46\tData (t) 0.001\tBatch (t) 0.028\n",
      "[35% 440/1250]\tAcc: 83.56\tData (t) 0.001\tBatch (t) 0.029\n",
      "[37% 460/1250]\tAcc: 83.62\tData (t) 0.001\tBatch (t) 0.028\n",
      "[38% 480/1250]\tAcc: 83.68\tData (t) 0.001\tBatch (t) 0.032\n",
      "[40% 500/1250]\tAcc: 83.71\tData (t) 0.001\tBatch (t) 0.029\n",
      "[42% 520/1250]\tAcc: 83.69\tData (t) 0.001\tBatch (t) 0.029\n",
      "[43% 540/1250]\tAcc: 83.69\tData (t) 0.001\tBatch (t) 0.029\n",
      "[45% 560/1250]\tAcc: 83.80\tData (t) 0.001\tBatch (t) 0.027\n",
      "[46% 580/1250]\tAcc: 83.91\tData (t) 0.001\tBatch (t) 0.027\n",
      "[48% 600/1250]\tAcc: 83.94\tData (t) 0.001\tBatch (t) 0.029\n",
      "[50% 620/1250]\tAcc: 83.88\tData (t) 0.001\tBatch (t) 0.029\n",
      "[51% 640/1250]\tAcc: 83.74\tData (t) 0.001\tBatch (t) 0.029\n",
      "[53% 660/1250]\tAcc: 83.72\tData (t) 0.001\tBatch (t) 0.028\n",
      "[54% 680/1250]\tAcc: 83.77\tData (t) 0.011\tBatch (t) 0.038\n",
      "[56% 700/1250]\tAcc: 83.74\tData (t) 0.007\tBatch (t) 0.038\n",
      "[58% 720/1250]\tAcc: 83.72\tData (t) 0.007\tBatch (t) 0.041\n",
      "[59% 740/1250]\tAcc: 83.77\tData (t) 0.011\tBatch (t) 0.031\n",
      "[61% 760/1250]\tAcc: 83.85\tData (t) 0.001\tBatch (t) 0.027\n",
      "[62% 780/1250]\tAcc: 83.85\tData (t) 0.001\tBatch (t) 0.029\n",
      "[64% 800/1250]\tAcc: 83.74\tData (t) 0.001\tBatch (t) 0.027\n",
      "[66% 820/1250]\tAcc: 83.69\tData (t) 0.001\tBatch (t) 0.027\n",
      "[67% 840/1250]\tAcc: 83.64\tData (t) 0.001\tBatch (t) 0.028\n",
      "[69% 860/1250]\tAcc: 83.57\tData (t) 0.001\tBatch (t) 0.028\n",
      "[70% 880/1250]\tAcc: 83.58\tData (t) 0.001\tBatch (t) 0.028\n",
      "[72% 900/1250]\tAcc: 83.63\tData (t) 0.001\tBatch (t) 0.029\n",
      "[74% 920/1250]\tAcc: 83.54\tData (t) 0.001\tBatch (t) 0.030\n",
      "[75% 940/1250]\tAcc: 83.44\tData (t) 0.001\tBatch (t) 0.029\n",
      "[77% 960/1250]\tAcc: 83.39\tData (t) 0.001\tBatch (t) 0.030\n",
      "[78% 980/1250]\tAcc: 83.44\tData (t) 0.001\tBatch (t) 0.029\n",
      "[80% 1000/1250]\tAcc: 83.42\tData (t) 0.001\tBatch (t) 0.029\n",
      "[82% 1020/1250]\tAcc: 83.40\tData (t) 0.001\tBatch (t) 0.029\n",
      "[83% 1040/1250]\tAcc: 83.31\tData (t) 0.001\tBatch (t) 0.029\n",
      "[85% 1060/1250]\tAcc: 83.27\tData (t) 0.001\tBatch (t) 0.030\n",
      "[86% 1080/1250]\tAcc: 83.30\tData (t) 0.001\tBatch (t) 0.029\n",
      "[88% 1100/1250]\tAcc: 83.31\tData (t) 0.001\tBatch (t) 0.029\n",
      "[90% 1120/1250]\tAcc: 83.33\tData (t) 0.008\tBatch (t) 0.040\n",
      "[91% 1140/1250]\tAcc: 83.29\tData (t) 0.009\tBatch (t) 0.042\n",
      "[93% 1160/1250]\tAcc: 83.31\tData (t) 0.008\tBatch (t) 0.032\n",
      "[94% 1180/1250]\tAcc: 83.35\tData (t) 0.001\tBatch (t) 0.024\n",
      "[96% 1200/1250]\tAcc: 83.38\tData (t) 0.008\tBatch (t) 0.032\n",
      "[98% 1220/1250]\tAcc: 83.39\tData (t) 0.001\tBatch (t) 0.029\n",
      "[99% 1240/1250]\tAcc: 83.45\tData (t) 0.001\tBatch (t) 0.029\n",
      "\n",
      "🍲 Uniform Soup Accuracy: 83.44%\n",
      "Improvement over best individual: 3.43%\n",
      "💾 Soup results saved to Drive!\n"
     ]
    }
   ],
   "source": [
    "# Create and evaluate uniform soup (simple average)\n",
    "print(\"🍲 Creating Uniform Soup...\")\n",
    "uniform_soup_state = create_soup(state_dicts)\n",
    "uniform_soup_model = get_model_from_sd(uniform_soup_state, base_model)\n",
    "uniform_accuracy = eval_model_on_dataset(uniform_soup_model, test_loader)\n",
    "\n",
    "print(f\"\\n🍲 Uniform Soup Accuracy: {100*uniform_accuracy:.2f}%\")\n",
    "print(f\"Improvement over best individual: {100*uniform_accuracy - 100*max(individual_results_test):.2f}%\")\n",
    "\n",
    "\n",
    "# Also backup final soup results to Drive\n",
    "import pickle\n",
    "results = {\n",
    "    'individual_results': individual_results_test,\n",
    "    'uniform_accuracy': uniform_accuracy,\n",
    "    # 'greedy_accuracy': best_accuracy,\n",
    "    # 'greedy_indices': greedy_indices,\n",
    "    'model_configs': [\n",
    "        'lr=3e-5, wd=0.1, timm_aug=False',\n",
    "        'lr=1e-5, wd=0.1, timm_aug=False',\n",
    "        'lr=3e-6, wd=0.1, timm_aug=False',\n",
    "        'lr=2e-5, wd=1e-3, timm_aug=True',\n",
    "        'lr=1e-6, wd=1e-4, timm_aug=False'\n",
    "    ]\n",
    "}\n",
    "\n",
    "with open(f'/content/drive/MyDrive/Colab Notebooks/soup_results_{checkpoint_step}.pkl', 'wb') as f:\n",
    "    pickle.dump(results, f)\n",
    "print(\"💾 Soup results saved to Drive!\")\n",
    "\n",
    "# # Simple greedy soup implementation (add models if they improve performance)\n",
    "# # NOTE: Use val split\n",
    "# print(f\"\\n🧠 Creating Greedy Soup...\")\n",
    "# best_accuracy_val = 0\n",
    "# best_state_dict = None\n",
    "# greedy_indices = []\n",
    "\n",
    "# # Start with the best individual model\n",
    "# best_idx = np.argmax(individual_results_val)\n",
    "# greedy_indices.append(best_idx)\n",
    "# best_state_dict = state_dicts[best_idx]\n",
    "# best_accuracy_val = individual_results_val[best_idx]\n",
    "\n",
    "# print(f\"Starting with Config {best_idx + 1} (val accuracy: {best_accuracy_val:.2f}%)\")\n",
    "\n",
    "# # Try adding each remaining model\n",
    "# for i, state_dict in enumerate(state_dicts):\n",
    "#     if i == best_idx:\n",
    "#         continue\n",
    "\n",
    "#     # Create soup with current best + this model\n",
    "#     temp_soup = create_soup([best_state_dict, state_dict])\n",
    "#     temp_model = get_model_from_sd(temp_soup, base_model)\n",
    "#     temp_accuracy_val = eval_model_on_dataset(temp_model, val_loader)\n",
    "\n",
    "#     print(f\"Adding Config {i+1}:\")\n",
    "#     print(f\"New val accu: {temp_accuracy_val:.2f}%\")\n",
    "\n",
    "#     if temp_accuracy_val > best_accuracy_val:\n",
    "#         print(f\"✅ Improved! Adding Config {i+1}\")\n",
    "#         greedy_indices.append(i)\n",
    "#         best_state_dict = temp_soup\n",
    "#         best_accuracy_val = temp_accuracy_val\n",
    "#     else:\n",
    "#         pass\n",
    "#         # print(f\"❌ No improvement, skipping Config {i+1}\")\n",
    "\n",
    "# # evaluae the greedy soup on test set\n",
    "\n",
    "\n",
    "# print(f\"\\n🧠 Greedy Soup includes configs: {[i+1 for i in greedy_indices]}\")\n",
    "\n",
    "\n",
    "\n",
    "# print(f\"🧠 Greedy Soup Accuracy: {best_accuracy_val:.2f}%\")\n",
    "\n",
    "\n",
    "\n",
    "# print(f\"Improvement over best individual: {best_accuracy_val - max(individual_results):.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 851
    },
    "executionInfo": {
     "elapsed": 585,
     "status": "ok",
     "timestamp": 1756053411848,
     "user": {
      "displayName": "Yang Zhou",
      "userId": "09717432835810465973"
     },
     "user_tz": 240
    },
    "id": "QoVEcNlER4tJ",
    "outputId": "0208eb80-7d05-4b21-94c5-58eb5097c8ba"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Summary Results:\n",
      "         Method  Accuracy (%)\n",
      "       Config 1         76.68\n",
      "       Config 2         80.01\n",
      "       Config 3         80.01\n",
      "       Config 4         78.52\n",
      "       Config 5         78.16\n",
      "Best Individual         80.01\n",
      "   Uniform Soup         83.44\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🎉 Final Results Summary:\n",
      "Checkpoint 3\n",
      "Zero-shot CLIP: ~61.39% (from earlier)\n",
      "Best Individual Model: 0.80%\n",
      "Uniform Soup: 0.83% (+0.03%)\n"
     ]
    }
   ],
   "source": [
    "# Create summary results\n",
    "results_summary = {\n",
    "    'Method': ['Config 1', 'Config 2', 'Config 3', 'Config 4', 'Config 5',\n",
    "               'Best Individual', 'Uniform Soup'], # , 'Greedy Soup'\n",
    "    'Accuracy (%)': np.array(individual_results_test + [max(individual_results_test), uniform_accuracy])*100 #, best_accuracy\n",
    "}\n",
    "\n",
    "# Create DataFrame for easy visualization\n",
    "results_df = pd.DataFrame(results_summary)\n",
    "print(\"📊 Summary Results:\")\n",
    "print(results_df.to_string(index=False))\n",
    "\n",
    "# Plot results\n",
    "plt.figure(figsize=(12, 6))\n",
    "colors = ['lightblue'] * 5 + ['orange', 'red', 'green']\n",
    "bars = plt.bar(results_df['Method'], results_df['Accuracy (%)'], color=colors)\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    plt.text(bar.get_x() + bar.get_width()/2., height + 0.1,\n",
    "             f'{height:.1f}%', ha='center', va='bottom')\n",
    "\n",
    "plt.title(f'Model Soup Results on Tiny ImageNet at step {checkpoint_step}', fontsize=16, fontweight='bold')\n",
    "plt.ylabel('Test Accuracy (%)', fontsize=12)\n",
    "plt.xlabel('Method', fontsize=12)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "\n",
    "# Add legend\n",
    "legend_elements = [plt.Rectangle((0,0),1,1, color='lightblue', label='Individual Models'),\n",
    "                   plt.Rectangle((0,0),1,1, color='orange', label='Best Individual'),\n",
    "                   plt.Rectangle((0,0),1,1, color='red', label='Uniform Soup')] # ,plt.Rectangle((0,0),1,1, color='green', label='Greedy Soup')\n",
    "\n",
    "plt.legend(handles=legend_elements, loc='upper left')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Print final summary\n",
    "print(f\"\\n🎉 Final Results Summary:\")\n",
    "print(f\"Checkpoint {checkpoint_step}\")\n",
    "print(f\"Zero-shot CLIP: ~61.39% (from earlier)\")\n",
    "print(f\"Best Individual Model: {max(individual_results_test):.2f}%\")\n",
    "print(f\"Uniform Soup: {uniform_accuracy:.2f}% (+{uniform_accuracy - max(individual_results_test):.2f}%)\")\n",
    "# print(f\"Greedy Soup: {best_accuracy:.2f}% (+{best_accuracy - max(individual_results):.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1756053305141,
     "user": {
      "displayName": "Yang Zhou",
      "userId": "09717432835810465973"
     },
     "user_tz": 240
    },
    "id": "pJpcvUEa9dv5"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "b_-KAJBYR4tH",
    "TmkIYFQuR4tI"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
