{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f6f76525",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5698d438",
   "metadata": {},
   "source": [
    "### GPU requirements\n",
    "MergeKit's evo evaluators are Ray actors that request 1 GPU each. Even the serial path reserves a GPU in Ray. Running evo fully on CPU would require non-trivial code changes (actor resource specs, evaluation backend). This notebook therefore keeps Part A as a read-only walkthrough and provides Part B as a CPU-only portable template you can run locally."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a31d3953",
   "metadata": {},
   "source": [
    "### Genotype → MergeConfiguration demo (safe to run on CPU)\n",
    "This cell uses MergeKit's public API to: (1) build a `ModelGenome` from a simple definition, (2) generate an initial genotype, (3) convert it to a `MergeConfiguration`, and (4) render the YAML."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "636e8b07",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Yang/Desktop/model-merge/evolutionary-algorithm/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "genotype shape: (12, 2, 1, 2)\n",
      "merge method: dare_ties\n",
      "num referenced models: 2\n",
      "slices: 12\n",
      "Merge configuration YAML:\n",
      "base_model: Qwen/Qwen2.5-0.5B\n",
      "dtype: bfloat16\n",
      "merge_method: dare_ties\n",
      "parameters:\n",
      "  int8_mask: 1.0\n",
      "  normalize: 1.0\n",
      "slices:\n",
      "- sources:\n",
      "  - layer_range: [0, 2]\n",
      "    model: Qwen/Qwen2.5-0.5B\n",
      "    parameters:\n",
      "      density: 1.0\n",
      "      weight: 0.5\n",
      "  - layer_range: [0, 2]\n",
      "    model: Qwen/Qwen2.5-0.5B-Instruct\n",
      "    parameters:\n",
      "      density: 1.0\n",
      "      weight: 0.5\n",
      "- sources:\n",
      "  - layer_range: [2, 4]\n",
      "    model: Qwen/Qwen2.5-0.5B\n",
      "    parameters:\n",
      "      density: 1.0\n",
      "      weight: 0.5\n",
      "  - layer_range: [2, 4]\n",
      "    model: Qwen/Qwen2.5-0.5B-Instruct\n",
      "    parameters:\n",
      "      density: 1.0\n",
      "      weight: 0.5\n",
      "- sources:\n",
      "  - layer_range: [4, 6]\n",
      "    model: Qwen/Qwen2.5-0.5B\n",
      "    parameters:\n",
      "      density: 1.0\n",
      "      weight: 0.5\n",
      "  - layer_range: [4, 6]\n",
      "    model: Qwen/Qwen2.5-0.5B-Instruct\n",
      "    parameters:\n",
      "      density: 1.0\n",
      "      weight: 0.5\n",
      "- sources:\n",
      "  - layer_range: [6, 8]\n",
      "    model: Qwen/Qwen2.5-0.5B\n",
      "    parameters:\n",
      "      density: 1.0\n",
      "      weight: 0.5\n",
      "  - layer_range: [6, 8]\n",
      "    model: Qwen/Qwen2.5-0.5B-Instruct\n",
      "    parameters:\n",
      "      density: 1.0\n",
      "      weight: 0.5\n",
      "- sources:\n",
      "  - layer_range: [8, 10]\n",
      "    model: Qwen/Qwen2.5-0.5B\n",
      "    parameters:\n",
      "      density: 1.0\n",
      "      weight: 0.5\n",
      "  - layer_range: [8, 10]\n",
      "    model: Qwen/Qwen2.5-0.5B-Instruct\n",
      "    parameters:\n",
      "      density: 1.0\n",
      "      weight: 0.5\n",
      "- sources:\n",
      "  - layer_range: [10, 12]\n",
      "    model: Qwen/Qwen2.5-0.5B\n",
      "    parameters:\n",
      "      density: 1.0\n",
      "      weight: 0.5\n",
      "  - layer_range: [10, 12]\n",
      "    model: Qwen/Qwen2.5-0.5B-Instruct\n",
      "    parameters:\n",
      "      density: 1.0\n",
      "      weight: 0.5\n",
      "- sources:\n",
      "  - layer_range: [12, 14]\n",
      "    model: Qwen/Qwen2.5-0.5B\n",
      "    parameters:\n",
      "      density: 1.0\n",
      "      weight: 0.5\n",
      "  - layer_range: [12, 14]\n",
      "    model: Qwen/Qwen2.5-0.5B-Instruct\n",
      "    parameters:\n",
      "      density: 1.0\n",
      "      weight: 0.5\n",
      "- sources:\n",
      "  - layer_range: [14, 16]\n",
      "    model: Qwen/Qwen2.5-0.5B\n",
      "    parameters:\n",
      "      density: 1.0\n",
      "      weight: 0.5\n",
      "  - layer_range: [14, 16]\n",
      "    model: Qwen/Qwen2.5-0.5B-Instruct\n",
      "    parameters:\n",
      "      density: 1.0\n",
      "      weight: 0.5\n",
      "- sources:\n",
      "  - layer_range: [16, 18]\n",
      "    model: Qwen/Qwen2.5-0.5B\n",
      "    parameters:\n",
      "      density: 1.0\n",
      "      weight: 0.5\n",
      "  - layer_range: [16, 18]\n",
      "    model: Qwen/Qwen2.5-0.5B-Instruct\n",
      "    parameters:\n",
      "      density: 1.0\n",
      "      weight: 0.5\n",
      "- sources:\n",
      "  - layer_range: [18, 20]\n",
      "    model: Qwen/Qwen2.5-0.5B\n",
      "    parameters:\n",
      "      density: 1.0\n",
      "      weight: 0.5\n",
      "  - layer_range: [18, 20]\n",
      "    model: Qwen/Qwen2.5-0.5B-Instruct\n",
      "    parameters:\n",
      "      density: 1.0\n",
      "      weight: 0.5\n",
      "- sources:\n",
      "  - layer_range: [20, 22]\n",
      "    model: Qwen/Qwen2.5-0.5B\n",
      "    parameters:\n",
      "      density: 1.0\n",
      "      weight: 0.5\n",
      "  - layer_range: [20, 22]\n",
      "    model: Qwen/Qwen2.5-0.5B-Instruct\n",
      "    parameters:\n",
      "      density: 1.0\n",
      "      weight: 0.5\n",
      "- sources:\n",
      "  - layer_range: [22, 24]\n",
      "    model: Qwen/Qwen2.5-0.5B\n",
      "    parameters:\n",
      "      density: 1.0\n",
      "      weight: 0.5\n",
      "  - layer_range: [22, 24]\n",
      "    model: Qwen/Qwen2.5-0.5B-Instruct\n",
      "    parameters:\n",
      "      density: 1.0\n",
      "      weight: 0.5\n"
     ]
    }
   ],
   "source": [
    "# Minimal, CPU-safe demo: construct genome definition and convert a genotype to MergeConfiguration\n",
    "from mergekit.evo.genome import ModelGenome, ModelGenomeDefinition\n",
    "from mergekit.common import ModelReference\n",
    "import torch\n",
    "\n",
    "# Use two small public models as references (only their configs are read here).\n",
    "# You can replace with your own. This cell does not load weights.\n",
    "genome_def = ModelGenomeDefinition(\n",
    "    models=[\n",
    "        ModelReference(model=\"Qwen/Qwen2.5-0.5B\"),\n",
    "        ModelReference(model=\"Qwen/Qwen2.5-0.5B-Instruct\"),\n",
    "    ],\n",
    "    base_model=ModelReference(model=\"Qwen/Qwen2.5-0.5B\"),\n",
    "    merge_method=\"dare_ties\",\n",
    "    layer_granularity=2,  # group layers for fewer parameters\n",
    "    normalize=None,        # auto for supported methods\n",
    "    allow_negative_weights=True,\n",
    "    filters=None,\n",
    "    smooth=False,\n",
    " )\n",
    "\n",
    "genome = ModelGenome(genome_def, trust_remote_code=False)\n",
    "x0 = genome.initial_genotype(random=False)  # shape: [groups, models, param_sets, params]\n",
    "print('genotype shape:', tuple(x0.shape))\n",
    "\n",
    "merge_cfg = genome.genotype_merge_config(x0)\n",
    "print('merge method:', merge_cfg.merge_method)\n",
    "print('num referenced models:', len(merge_cfg.referenced_models()))\n",
    "print('slices:', len(merge_cfg.slices or []))\n",
    "print('Merge configuration YAML:')\n",
    "print(merge_cfg.to_yaml())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c388e4e",
   "metadata": {},
   "source": [
    "### What happens next (on GPU runs)\n",
    "- The `MergeConfiguration` feeds `MergePlanner`, which builds a tensor DAG per-parameter and per-layer: gather → merge op → save.\n",
    "- The evaluator merges on-disk or in-memory, then calls `lm_eval.simple_evaluate` to compute metrics.\n",
    "- CMA-ES in `mergekit.scripts.evolve` optimizes the genotype by repeatedly evaluating populations and updating the distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc3acba5",
   "metadata": {},
   "source": [
    "## Part B — Portable evolutionary merging for any PyTorch model (CPU)\n",
    "We'll implement a small CPU-only evolutionary search that mirrors MergeKit's ideas:\n",
    "- A genotype controlling per-layer weights (and optional density).\n",
    "- A merge function that combines parent state_dicts using those params.\n",
    "- A fitness function (validation loss/accuracy).\n",
    "- A simple evolutionary optimizer (CMA-ES if available; fallback to random-restart hill climbing)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5dfa8f7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parent A acc: 0.974609375\n",
      "Parent B acc: 0.98046875\n"
     ]
    }
   ],
   "source": [
    "# Toy dataset and model (CPU)\n",
    "import torch, torch.nn as nn, torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import math, random\n",
    "\n",
    "torch.manual_seed(0)\n",
    "device = 'cpu'\n",
    "\n",
    "# Synthetic 2D classification (two moons-like but simpler)\n",
    "def make_data(n=1024):\n",
    "    x = torch.rand(n, 2) * 2 - 1  # [-1,1]^2\n",
    "    y = (x[:,0] * x[:,1] > 0).long()  # XOR-ish\n",
    "    return x, y\n",
    "\n",
    "Xtr, ytr = make_data(2048)\n",
    "Xva, yva = make_data(512)\n",
    "train_loader = DataLoader(TensorDataset(Xtr, ytr), batch_size=128, shuffle=True)\n",
    "val_loader = DataLoader(TensorDataset(Xva, yva), batch_size=256)\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, d=2, h=32, c=2):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(d, h), nn.ReLU(),\n",
    "            nn.Linear(h, h), nn.ReLU(),\n",
    "            nn.Linear(h, c)\n",
    "        )\n",
    "    def forward(self, x): return self.net(x)\n",
    "\n",
    "def train_one(model, steps=200, lr=1e-2):\n",
    "    opt = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "    model.train()\n",
    "    it = iter(train_loader)\n",
    "    for _ in range(steps):\n",
    "        try: xb, yb = next(it)\n",
    "        except StopIteration: it = iter(train_loader); xb, yb = next(it)\n",
    "        opt.zero_grad()\n",
    "        loss = F.cross_entropy(model(xb), yb)\n",
    "        loss.backward(); opt.step()\n",
    "\n",
    "def eval_acc(model):\n",
    "    model.eval(); corr=tot=0\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in val_loader:\n",
    "            pred = model(xb).argmax(-1)\n",
    "            corr += (pred==yb).sum().item(); tot += yb.numel()\n",
    "    return corr/tot\n",
    "\n",
    "# Train two different parents\n",
    "parent_a = MLP(); parent_b = MLP()\n",
    "train_one(parent_a, steps=300)\n",
    "train_one(parent_b, steps=300, lr=8e-3)\n",
    "print('Parent A acc:', eval_acc(parent_a))\n",
    "print('Parent B acc:', eval_acc(parent_b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4def7045",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "too many indices for tensor of dimension 3",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mIndexError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 55\u001b[39m\n\u001b[32m     52\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m eval_acc(model)  \u001b[38;5;66;03m# higher is better\u001b[39;00m\n\u001b[32m     54\u001b[39m g0 = init_genotype()\n\u001b[32m---> \u001b[39m\u001b[32m55\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m'\u001b[39m\u001b[33mInitial fitness:\u001b[39m\u001b[33m'\u001b[39m, \u001b[43mfitness\u001b[49m\u001b[43m(\u001b[49m\u001b[43mg0\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 51\u001b[39m, in \u001b[36mfitness\u001b[39m\u001b[34m(g)\u001b[39m\n\u001b[32m     50\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfitness\u001b[39m(g):\n\u001b[32m---> \u001b[39m\u001b[32m51\u001b[39m     model = \u001b[43mapply_genotype\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparent_a\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparent_b\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     52\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m eval_acc(model)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 31\u001b[39m, in \u001b[36mapply_genotype\u001b[39m\u001b[34m(parent_a, parent_b, g)\u001b[39m\n\u001b[32m     29\u001b[39m out = MLP(); out_sd = out.state_dict()\n\u001b[32m     30\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m gi, names \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(layer_groups):\n\u001b[32m---> \u001b[39m\u001b[32m31\u001b[39m     w_a = \u001b[43mweight\u001b[49m\u001b[43m[\u001b[49m\u001b[43mgi\u001b[49m\u001b[43m,\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m.item(); w_b = weight[gi,\u001b[32m1\u001b[39m,\u001b[32m0\u001b[39m,\u001b[32m0\u001b[39m].item()\n\u001b[32m     32\u001b[39m     d = density[gi,\u001b[32m0\u001b[39m,\u001b[32m0\u001b[39m,\u001b[32m1\u001b[39m].item() \u001b[38;5;28;01mif\u001b[39;00m n_params>\u001b[32m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[32m1.0\u001b[39m\n\u001b[32m     33\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m names:\n",
      "\u001b[31mIndexError\u001b[39m: too many indices for tensor of dimension 3"
     ]
    }
   ],
   "source": [
    "# Genome encoding: [n_layer_groups, n_models, n_param_sets(=1), n_params]\n",
    "# We'll mirror MergeKit's 'dare_ties' shape but implement a simplified op:\n",
    "#  - param 0: weight in [0, +inf) (abs applied);\n",
    "#  - param 1: density in [0,1] (sparsifies the delta merge).\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "layers = [n for n,_ in parent_a.named_parameters()]\n",
    "# Group linear layers by layer index for a coarse \"layer_granularity\"\n",
    "layer_groups = [\n",
    "    [n for n in layers if 'net.0' in n or 'net.1' in n],\n",
    "    [n for n in layers if 'net.2' in n or 'net.3' in n],\n",
    "    [n for n in layers if 'net.4' in n],\n",
    "]\n",
    "n_groups = len(layer_groups); n_models=2; n_sets=1; n_params=2\n",
    "\n",
    "def init_genotype():\n",
    "    g = torch.zeros(n_groups, n_models, n_sets, n_params)\n",
    "    g[:,:,:,0] = 0.5  # equal weights\n",
    "    g[:,:,:,1] = 1.0  # full density\n",
    "    return g\n",
    "\n",
    "def apply_genotype(parent_a, parent_b, g):\n",
    "    # Simplified \"dare_ties-like\": base + sum_i w_i * sparsify(delta_i, density)\n",
    "    # Where delta_i = parent_i - base; base = parent_a here; density in [0,1]\n",
    "    g = g.clone(); g[:,:,:,0] = g[:,:,:,0].abs(); g[:,:,:,1] = g[:,:,:,1].abs().clamp(0,1)\n",
    "    weight = g[:,:,:,0]; density = g[:,:,:,1]\n",
    "    base_sd = dict(parent_a.state_dict()); b_sd = dict(parent_b.state_dict())\n",
    "    out = MLP(); out_sd = out.state_dict()\n",
    "    for gi, names in enumerate(layer_groups):\n",
    "        w_a = weight[gi,0,0,0].item(); w_b = weight[gi,1,0,0].item()\n",
    "        d = density[gi,0,0,1].item() if n_params>1 else 1.0\n",
    "        for name in names:\n",
    "            if name not in out_sd: continue\n",
    "            A = base_sd[name]; B = b_sd[name]\n",
    "            delta = B - A\n",
    "            # Sparsify delta using top-|delta| mask by fraction d\n",
    "            k = max(1, int(delta.numel()*d))\n",
    "            flat = delta.flatten().abs()\n",
    "            if k < flat.numel():\n",
    "                thresh = flat.kthvalue(flat.numel()-k).values\n",
    "                mask = (delta.abs() >= thresh)\n",
    "            else:\n",
    "                mask = torch.ones_like(delta, dtype=torch.bool)\n",
    "            merged = A + w_b * (delta * mask)\n",
    "            out_sd[name] = merged.to(out_sd[name].dtype)\n",
    "    out.load_state_dict(out_sd)\n",
    "    return out\n",
    "\n",
    "def fitness(g):\n",
    "    model = apply_genotype(parent_a, parent_b, g)\n",
    "    return eval_acc(model)  # higher is better\n",
    "\n",
    "g0 = init_genotype()\n",
    "print('Initial fitness:', fitness(g0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ea5e1b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evolutionary search: CMA-ES if available, else simple random-restart hill climbing\n",
    "try:\n",
    "    import cma\n",
    "    use_cma = True\n",
    "except Exception:\n",
    "    use_cma = False\n",
    "\n",
    "def run_cma_es(iters=10, sigma0=0.2, popsize=None):\n",
    "    x0 = g0.view(-1).numpy()\n",
    "    es = cma.CMAEvolutionStrategy(x0, sigma0, {'popsize': popsize} if popsize else {})\n",
    "    best = (-1.0, x0)\n",
    "    for _ in range(iters):\n",
    "        xs = es.ask()\n",
    "        fits = []\n",
    "        for x in xs:\n",
    "            g = torch.tensor(x).view_as(g0)\n",
    "            fits.append(-(-fitness(g)))  # CMA minimizes\n",
    "        es.tell(xs, [-f for f in fits])\n",
    "        bi = int(np.argmax(fits)); bfit = fits[bi]\n",
    "        if bfit > best[0]: best = (bfit, xs[bi])\n",
    "    return best[0], torch.tensor(best[1]).view_as(g0)\n",
    "\n",
    "def run_hill_climb(iters=50, samples=16, step=0.2):\n",
    "    best_g = g0.clone(); best_f = fitness(best_g)\n",
    "    for _ in range(iters):\n",
    "        cand = [best_g + step*torch.randn_like(best_g) for _ in range(samples)]\n",
    "        vals = [fitness(g) for g in cand]\n",
    "        bi = int(np.argmax(vals))\n",
    "        if vals[bi] > best_f: best_f, best_g = vals[bi], cand[bi]\n",
    "    return best_f, best_g\n",
    "\n",
    "best_score, best_g = (run_cma_es(iters=15) if use_cma else run_hill_climb())\n",
    "print('Best fitness:', best_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ffd174d",
   "metadata": {},
   "source": [
    "### Adapting to your own architecture\n",
    "- Extract and align parent `state_dict`s; ensure parameter name and shape match. If not, write mapping code.\n",
    "- Choose layer groups (granularity). You can group by blocks or specific submodules.\n",
    "- Define your merge operator over tensors: e.g., base + w_i * masked deltas; or SLERP-like for normalized vectors.\n",
    "- Encode merge parameters into a genotype tensor consistent with your operator.\n",
    "- Define a fitness function appropriate for your task (loss, accuracy, BLEU, etc.).\n",
    "- Swap the toy optimizer with CMA-ES or another EA for stronger search."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbbfac90",
   "metadata": {},
   "source": [
    "## Appendix — Running MergeKit evo on GPU\n",
    "From this repo, prepare a YAML like in your working Colab and run the CLI. Example flags: `--storage-path`, `--max-fevals`, `--num-gpus`, `--no-vllm`, `--allow-crimes` (if optimizing benchmarks), `--no-wandb`, `--save-final-model`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
