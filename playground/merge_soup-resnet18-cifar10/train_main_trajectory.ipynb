{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Main Trajectory\n",
    "\n",
    "This notebook trains a single ResNet18 model on CIFAR-10 for 10 epochs.\n",
    "This main trajectory will serve as the starting point for branched finetuning experiments.\n",
    "\n",
    "**Outputs:**\n",
    "- `checkpoints/base_model.pt` - Initial model (epoch 0)\n",
    "- `checkpoints/main_epoch{i}.pt` - Checkpoints at epochs 1-10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOCAL = True\n",
    "\n",
    "if LOCAL:\n",
    "    ROOT_DIR = \"/Users/Yang/Desktop/research-model-merge/playground/merge_soup-resnet18-cifar10\"\n",
    "    DATA_DIR = \"/Users/Yang/Desktop/research-model-merge/datasets\"\n",
    "    PROJECT_ROOT = \"/Users/Yang/Desktop/research-model-merge\"\n",
    "else:\n",
    "    # on Colab\n",
    "    ROOT_DIR = \"/content/research-model-merge/playground/merge_soup-resnet18-cifar10\"\n",
    "    DATA_DIR = \"/content/research-model-merge/datasets\"\n",
    "    PROJECT_ROOT = \"/content/research-model-merge\"\n",
    "    DRIVE_DIR = \"/content/drive/MyDrive/research-model_merge-shared/merge_soup-resnet18-cifar10\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mount Google Drive (Colab only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not LOCAL:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clone Repository and Install Dependencies (Colab only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not LOCAL:\n",
    "    # !rm -rf research-model-merge\n",
    "    !git clone https://github.com/nbzy1995/research-model-merge.git /content/research-model-merge\n",
    "    !pip install --quiet --upgrade pip\n",
    "    !pip install -q -r research-model-merge/requirements.txt\n",
    "    print(\"‚úÖ Repository cloned and dependencies installed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import subprocess\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Add project directories to path\n",
    "if PROJECT_ROOT not in sys.path:\n",
    "    sys.path.insert(0, PROJECT_ROOT)\n",
    "if ROOT_DIR not in sys.path:\n",
    "    sys.path.insert(0, ROOT_DIR)\n",
    "\n",
    "from datasets.cifar10 import CIFAR10\n",
    "from utils import create_cifar10_resnet18, train_model, save_checkpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check Device Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç System Information:\n",
      "Python version: Python 3.11.5\n",
      "PyTorch version: 2.3.0\n",
      "CUDA available: False\n",
      "‚ö†Ô∏è No GPU available! Training will be slow on CPU.\n"
     ]
    }
   ],
   "source": [
    "print(\"üîç System Information:\")\n",
    "print(f\"Python version: {subprocess.check_output(['python', '--version']).decode().strip()}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU device: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "    print(f\"CUDA version: {torch.version.cuda}\")\n",
    "    DEVICE = torch.device(\"cuda\")\n",
    "else:\n",
    "    if LOCAL:\n",
    "        print(\"‚ö†Ô∏è No GPU available! Training will be slow on CPU.\")\n",
    "    else:\n",
    "        print(\"‚ùå No GPU available! Please enable GPU runtime in Colab.\")\n",
    "        print(\"Runtime > Change runtime type > Hardware accelerator > GPU\")\n",
    "    DEVICE = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "‚úÖ Dataset loaded:\n",
      "   Train samples: 49000\n",
      "   Val samples: 1000\n",
      "   Test samples: 10000\n"
     ]
    }
   ],
   "source": [
    "dataset = CIFAR10(\n",
    "    data_location=DATA_DIR,\n",
    "    batch_size=256,\n",
    "    num_workers=2\n",
    ")\n",
    "\n",
    "train_loader = dataset.train_loader\n",
    "val_loader = dataset.val_loader\n",
    "test_loader = dataset.test_loader\n",
    "\n",
    "print(f\"‚úÖ Dataset loaded:\")\n",
    "print(f\"   Train samples: {len(dataset.train_sampler)}\")\n",
    "print(f\"   Val samples: {len(dataset.val_sampler)}\")\n",
    "print(f\"   Test samples: {len(dataset.test_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Configuration:\n",
      "  Seed: 42\n",
      "  Epochs: 10\n",
      "  Learning Rate: 0.1\n",
      "  Weight Decay: 0.0001\n",
      "  Momentum: 0.9\n",
      "  Warmup Epochs: 5\n",
      "  Checkpoint Dir: /Users/Yang/Desktop/research-model-merge/playground/merge_soup-resnet18-cifar10/checkpoints\n"
     ]
    }
   ],
   "source": [
    "# Fixed seed for reproducibility\n",
    "SEED = 42\n",
    "\n",
    "# Training hyperparameters\n",
    "EPOCHS = 10\n",
    "LEARNING_RATE = 0.01\n",
    "WEIGHT_DECAY = 1e-4\n",
    "MOMENTUM = 0.9\n",
    "WARMUP_EPOCHS = 5\n",
    "\n",
    "# Checkpoint configuration\n",
    "if LOCAL:\n",
    "    CHECKPOINT_DIR = f\"{ROOT_DIR}/checkpoints\"\n",
    "else:\n",
    "    CHECKPOINT_DIR = f\"{DRIVE_DIR}/checkpoints\"\n",
    "\n",
    "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
    "\n",
    "print(f\"Training Configuration:\")\n",
    "print(f\"  Seed: {SEED}\")\n",
    "print(f\"  Epochs: {EPOCHS}\")\n",
    "print(f\"  Learning Rate: {LEARNING_RATE}\")\n",
    "print(f\"  Weight Decay: {WEIGHT_DECAY}\")\n",
    "print(f\"  Momentum: {MOMENTUM}\")\n",
    "print(f\"  Warmup Epochs: {WARMUP_EPOCHS}\")\n",
    "print(f\"  Checkpoint Dir: {CHECKPOINT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Model created:\n",
      "   Total parameters: 11,173,962\n",
      "   Trainable parameters: 11,173,962\n",
      "\n",
      "‚úÖ Saved base model: /Users/Yang/Desktop/research-model-merge/playground/merge_soup-resnet18-cifar10/checkpoints/base_model.pt\n"
     ]
    }
   ],
   "source": [
    "model = create_cifar10_resnet18(num_classes=10, seed=SEED)\n",
    "model = model.to(DEVICE)\n",
    "\n",
    "print(\"‚úÖ Model created:\")\n",
    "print(f\"   Total parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "print(f\"   Trainable parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")\n",
    "\n",
    "# Save base model (epoch 0)\n",
    "base_model_path = os.path.join(CHECKPOINT_DIR, \"base_model.pt\")\n",
    "save_checkpoint(model, base_model_path)\n",
    "print(f\"\\n‚úÖ Saved base model: {base_model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Optimizer and Scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Optimizer and scheduler configured\n"
     ]
    }
   ],
   "source": [
    "optimizer = optim.SGD(\n",
    "    model.parameters(),\n",
    "    lr=LEARNING_RATE,\n",
    "    momentum=MOMENTUM,\n",
    "    weight_decay=WEIGHT_DECAY\n",
    ")\n",
    "\n",
    "# Step LR: decay by 0.1 after warmup epochs\n",
    "lr_scheduler = StepLR(optimizer, step_size=WARMUP_EPOCHS, gamma=0.1)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "print(\"‚úÖ Optimizer and scheduler configured\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Main Trajectory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Starting Main Trajectory Training\n",
      "================================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 [Train]:   4%|‚ñç         | 8/192 [00:51<19:03,  6.22s/it, loss=2.4792, lr=0.100000]"
     ]
    }
   ],
   "source": [
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"Starting Main Trajectory Training\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "\n",
    "history = train_model(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    optimizer=optimizer,\n",
    "    lr_scheduler=lr_scheduler,\n",
    "    criterion=criterion,\n",
    "    epochs=EPOCHS,\n",
    "    device=DEVICE,\n",
    "    checkpoint_dir=CHECKPOINT_DIR,\n",
    "    checkpoint_name_template=\"main_epoch{epoch}.pt\",\n",
    "    log_interval=20,\n",
    "    save_epoch_0=False  # Already saved as base_model.pt\n",
    ")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"Main Trajectory Training Completed!\")\n",
    "print(f\"{'='*80}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Training Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "epochs_range = range(1, EPOCHS + 1)\n",
    "\n",
    "# Plot 1: Training and Validation Loss (combined)\n",
    "axes[0].plot(epochs_range, history['train_loss'], 'b-o', label='Train Loss')\n",
    "axes[0].plot(epochs_range, history['val_loss'], 'r-o', label='Val Loss')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].set_title('Training and Validation Loss')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True)\n",
    "\n",
    "# Plot 2: Validation Accuracy\n",
    "axes[1].plot(epochs_range, [100*x for x in history['val_acc']], 'g-o', label='Val Accuracy')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Accuracy (%)')\n",
    "axes[1].set_title('Validation Accuracy')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True)\n",
    "\n",
    "# Plot 3: Learning Rate\n",
    "axes[2].plot(epochs_range, history['lr'], 'm-o', label='Learning Rate')\n",
    "axes[2].set_xlabel('Epoch')\n",
    "axes[2].set_ylabel('Learning Rate')\n",
    "axes[2].set_title('Learning Rate Schedule')\n",
    "axes[2].legend()\n",
    "axes[2].grid(True)\n",
    "axes[2].set_yscale('log')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{CHECKPOINT_DIR}/main_trajectory_curves.png\", dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"‚úÖ Training curves saved to {CHECKPOINT_DIR}/main_trajectory_curves.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Main Trajectory Training Summary\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Final Train Loss: {history['train_loss'][-1]:.4f}\")\n",
    "print(f\"Final Val Loss: {history['val_loss'][-1]:.4f}\")\n",
    "print(f\"Final Val Accuracy: {100*history['val_acc'][-1]:.2f}%\")\n",
    "print(f\"Best Val Accuracy: {100*max(history['val_acc']):.2f}% (Epoch {history['val_acc'].index(max(history['val_acc']))+1})\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\n‚úÖ Checkpoints saved:\")\n",
    "print(f\"   - base_model.pt (epoch 0)\")\n",
    "for i in range(1, EPOCHS + 1):\n",
    "    print(f\"   - main_epoch{i}.pt\")\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
