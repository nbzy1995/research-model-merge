{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyze Branching Soups\n",
    "\n",
    "This notebook analyzes model soups created from branched finetuning variants.\n",
    "\n",
    "**Hypothesis to Test:**\n",
    "Early branching (high loss, high in loss landscape) ‚Üí models end up in different basins ‚Üí poor soup performance\n",
    "\n",
    "Late branching (lower loss, models closer) ‚Üí models end up in same basin ‚Üí good soup performance\n",
    "\n",
    "**Analysis:**\n",
    "1. Load models from different branching epochs\n",
    "2. Compute pairwise L2 distances between variants\n",
    "3. Create uniform soups from variants at each branching point\n",
    "4. Evaluate soup performance vs branching epoch\n",
    "5. Visualize the basin transition hypothesis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOCAL = True\n",
    "\n",
    "if LOCAL:\n",
    "    ROOT_DIR = \"/Users/Yang/Desktop/research-model-merge/playground/merge_soup-resnet18-cifar10\"\n",
    "    DATA_DIR = \"/Users/Yang/Desktop/research-model-merge/datasets\"\n",
    "    PROJECT_ROOT = \"/Users/Yang/Desktop/research-model-merge\"\n",
    "else:\n",
    "    # on Colab\n",
    "    ROOT_DIR = \"/content/research-model-merge/playground/merge_soup-resnet18-cifar10\"\n",
    "    DATA_DIR = \"/content/research-model-merge/datasets\"\n",
    "    PROJECT_ROOT = \"/content/research-model-merge\"\n",
    "    DRIVE_DIR = \"/content/drive/MyDrive/research-model_merge-shared/merge_soup-resnet18-cifar10\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mount Google Drive (Colab only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not LOCAL:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clone Repository and Install Dependencies (Colab only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not LOCAL:\n",
    "    !rm -rf research-model-merge\n",
    "    !git clone https://github.com/nbzy1995/research-model-merge.git /content/research-model-merge\n",
    "    !pip install --quiet --upgrade pip\n",
    "    !pip install -q -r research-model-merge/requirements.txt\n",
    "    print(\"‚úÖ Repository cloned and dependencies installed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import subprocess\n",
    "from itertools import combinations\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Add project directories to path\n",
    "if PROJECT_ROOT not in sys.path:\n",
    "    sys.path.insert(0, PROJECT_ROOT)\n",
    "if ROOT_DIR not in sys.path:\n",
    "    sys.path.insert(0, ROOT_DIR)\n",
    "\n",
    "from datasets.cifar10 import CIFAR10\n",
    "from utils import (\n",
    "    create_cifar10_resnet18, \n",
    "    load_checkpoint, \n",
    "    evaluate_model,\n",
    "    create_uniform_soup,\n",
    "    compute_l2_distance\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check Device Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üîç System Information:\")\n",
    "print(f\"Python version: {subprocess.check_output(['python', '--version']).decode().strip()}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU device: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "    print(f\"CUDA version: {torch.version.cuda}\")\n",
    "    DEVICE = torch.device(\"cuda\")\n",
    "else:\n",
    "    if LOCAL:\n",
    "        print(\"‚ö†Ô∏è No GPU available! Evaluation will be slow on CPU.\")\n",
    "    else:\n",
    "        print(\"‚ùå No GPU available! Please enable GPU runtime in Colab.\")\n",
    "    DEVICE = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = CIFAR10(\n",
    "    data_location=DATA_DIR,\n",
    "    batch_size=256,\n",
    "    num_workers=2\n",
    ")\n",
    "\n",
    "test_loader = dataset.test_loader\n",
    "\n",
    "print(f\"‚úÖ Dataset loaded:\")\n",
    "print(f\"   Test samples: {len(dataset.test_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Match the configuration from training notebooks\n",
    "BRANCHING_EPOCHS = [0, 2, 5, 8]\n",
    "LR_VARIANTS = [1, 2, 3, 4, 5]  # lr_ids\n",
    "BRANCH_EPOCHS = 10  # How many epochs were trained after branching\n",
    "\n",
    "# Analysis epochs (which epoch after branching to analyze)\n",
    "ANALYSIS_EPOCHS = [1, 5, 10]  # e.g., early, mid, final\n",
    "\n",
    "if LOCAL:\n",
    "    CHECKPOINT_DIR = f\"{ROOT_DIR}/checkpoints\"\n",
    "    RESULTS_DIR = f\"{ROOT_DIR}/results\"\n",
    "else:\n",
    "    CHECKPOINT_DIR = f\"{DRIVE_DIR}/checkpoints\"\n",
    "    RESULTS_DIR = f\"{DRIVE_DIR}/results\"\n",
    "\n",
    "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
    "\n",
    "print(f\"Analysis Configuration:\")\n",
    "print(f\"  Branching Epochs: {BRANCHING_EPOCHS}\")\n",
    "print(f\"  LR Variants: {LR_VARIANTS}\")\n",
    "print(f\"  Analysis Epochs: {ANALYSIS_EPOCHS}\")\n",
    "print(f\"  Results Dir: {RESULTS_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Evaluate Individual Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "individual_results = []\n",
    "\n",
    "print(\"Evaluating individual models...\\n\")\n",
    "\n",
    "for branch_epoch in BRANCHING_EPOCHS:\n",
    "    for lr_id in LR_VARIANTS:\n",
    "        for analysis_epoch in ANALYSIS_EPOCHS:\n",
    "            checkpoint_name = f\"branch_e{branch_epoch}_lr{lr_id}_e{analysis_epoch}.pt\"\n",
    "            checkpoint_path = os.path.join(CHECKPOINT_DIR, checkpoint_name)\n",
    "            \n",
    "            if not os.path.exists(checkpoint_path):\n",
    "                print(f\"‚ö†Ô∏è  Skipping missing: {checkpoint_name}\")\n",
    "                continue\n",
    "            \n",
    "            # Load model\n",
    "            model = create_cifar10_resnet18()\n",
    "            state_dict = load_checkpoint(checkpoint_path, device=DEVICE)\n",
    "            model.load_state_dict(state_dict)\n",
    "            model = model.to(DEVICE)\n",
    "            \n",
    "            # Evaluate\n",
    "            test_loss, test_acc = evaluate_model(model, test_loader, criterion, DEVICE)\n",
    "            \n",
    "            individual_results.append({\n",
    "                'branch_epoch': branch_epoch,\n",
    "                'lr_id': lr_id,\n",
    "                'analysis_epoch': analysis_epoch,\n",
    "                'test_loss': test_loss,\n",
    "                'test_acc': test_acc,\n",
    "                'checkpoint_name': checkpoint_name\n",
    "            })\n",
    "            \n",
    "            print(f\"‚úì {checkpoint_name}: {100*test_acc:.2f}%\")\n",
    "\n",
    "df_individual = pd.DataFrame(individual_results)\n",
    "print(f\"\\n‚úÖ Evaluated {len(individual_results)} individual models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Pairwise L2 Distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distance_results = []\n",
    "\n",
    "print(\"Computing pairwise L2 distances...\\n\")\n",
    "\n",
    "for branch_epoch in BRANCHING_EPOCHS:\n",
    "    for analysis_epoch in ANALYSIS_EPOCHS:\n",
    "        print(f\"Branch epoch {branch_epoch}, analysis epoch {analysis_epoch}:\")\n",
    "        \n",
    "        # Load all models for this configuration\n",
    "        models_data = []\n",
    "        for lr_id in LR_VARIANTS:\n",
    "            checkpoint_name = f\"branch_e{branch_epoch}_lr{lr_id}_e{analysis_epoch}.pt\"\n",
    "            checkpoint_path = os.path.join(CHECKPOINT_DIR, checkpoint_name)\n",
    "            \n",
    "            if not os.path.exists(checkpoint_path):\n",
    "                continue\n",
    "            \n",
    "            state_dict = load_checkpoint(checkpoint_path, device='cpu')  # Load to CPU for distance computation\n",
    "            models_data.append({\n",
    "                'lr_id': lr_id,\n",
    "                'state_dict': state_dict,\n",
    "                'checkpoint_name': checkpoint_name\n",
    "            })\n",
    "        \n",
    "        # Compute pairwise distances\n",
    "        for i, j in combinations(range(len(models_data)), 2):\n",
    "            model_i = models_data[i]\n",
    "            model_j = models_data[j]\n",
    "            \n",
    "            l2_dist = compute_l2_distance(model_i['state_dict'], model_j['state_dict'])\n",
    "            \n",
    "            distance_results.append({\n",
    "                'branch_epoch': branch_epoch,\n",
    "                'analysis_epoch': analysis_epoch,\n",
    "                'lr_id_1': model_i['lr_id'],\n",
    "                'lr_id_2': model_j['lr_id'],\n",
    "                'l2_distance': l2_dist\n",
    "            })\n",
    "        \n",
    "        print(f\"  Computed {len(list(combinations(range(len(models_data)), 2)))} pairwise distances\")\n",
    "\n",
    "df_distances = pd.DataFrame(distance_results)\n",
    "print(f\"\\n‚úÖ Computed {len(distance_results)} pairwise distances\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create and Evaluate Soups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup_results = []\n",
    "\n",
    "print(\"Creating and evaluating soups...\\n\")\n",
    "\n",
    "for branch_epoch in BRANCHING_EPOCHS:\n",
    "    for analysis_epoch in ANALYSIS_EPOCHS:\n",
    "        print(f\"Creating soup: branch_e{branch_epoch}, analysis_e{analysis_epoch}\")\n",
    "        \n",
    "        # Load all models for this configuration\n",
    "        state_dicts = []\n",
    "        for lr_id in LR_VARIANTS:\n",
    "            checkpoint_name = f\"branch_e{branch_epoch}_lr{lr_id}_e{analysis_epoch}.pt\"\n",
    "            checkpoint_path = os.path.join(CHECKPOINT_DIR, checkpoint_name)\n",
    "            \n",
    "            if not os.path.exists(checkpoint_path):\n",
    "                continue\n",
    "            \n",
    "            state_dict = load_checkpoint(checkpoint_path, device='cpu')\n",
    "            state_dicts.append(state_dict)\n",
    "        \n",
    "        if len(state_dicts) == 0:\n",
    "            print(f\"  ‚ö†Ô∏è  No models found, skipping\")\n",
    "            continue\n",
    "        \n",
    "        # Create uniform soup\n",
    "        soup_state_dict = create_uniform_soup(state_dicts)\n",
    "        \n",
    "        # Load soup into model and evaluate\n",
    "        soup_model = create_cifar10_resnet18()\n",
    "        soup_model.load_state_dict(soup_state_dict)\n",
    "        soup_model = soup_model.to(DEVICE)\n",
    "        \n",
    "        soup_loss, soup_acc = evaluate_model(soup_model, test_loader, criterion, DEVICE)\n",
    "        \n",
    "        # Get individual model accuracies for comparison\n",
    "        individual_accs = df_individual[\n",
    "            (df_individual['branch_epoch'] == branch_epoch) & \n",
    "            (df_individual['analysis_epoch'] == analysis_epoch)\n",
    "        ]['test_acc'].values\n",
    "        \n",
    "        mean_individual_acc = np.mean(individual_accs) if len(individual_accs) > 0 else 0\n",
    "        best_individual_acc = np.max(individual_accs) if len(individual_accs) > 0 else 0\n",
    "        \n",
    "        soup_results.append({\n",
    "            'branch_epoch': branch_epoch,\n",
    "            'analysis_epoch': analysis_epoch,\n",
    "            'num_models': len(state_dicts),\n",
    "            'soup_test_loss': soup_loss,\n",
    "            'soup_test_acc': soup_acc,\n",
    "            'mean_individual_acc': mean_individual_acc,\n",
    "            'best_individual_acc': best_individual_acc,\n",
    "            'soup_improvement_over_mean': soup_acc - mean_individual_acc,\n",
    "            'soup_improvement_over_best': soup_acc - best_individual_acc\n",
    "        })\n",
    "        \n",
    "        print(f\"  ‚úì Soup acc: {100*soup_acc:.2f}% | Mean individual: {100*mean_individual_acc:.2f}% | Best individual: {100*best_individual_acc:.2f}%\")\n",
    "\n",
    "df_soups = pd.DataFrame(soup_results)\n",
    "print(f\"\\n‚úÖ Evaluated {len(soup_results)} soups\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save all results to CSV\n",
    "df_individual.to_csv(os.path.join(RESULTS_DIR, \"individual_models.csv\"), index=False)\n",
    "df_distances.to_csv(os.path.join(RESULTS_DIR, \"pairwise_distances.csv\"), index=False)\n",
    "df_soups.to_csv(os.path.join(RESULTS_DIR, \"soup_results.csv\"), index=False)\n",
    "\n",
    "print(\"‚úÖ Results saved:\")\n",
    "print(f\"   - {RESULTS_DIR}/individual_models.csv\")\n",
    "print(f\"   - {RESULTS_DIR}/pairwise_distances.csv\")\n",
    "print(f\"   - {RESULTS_DIR}/soup_results.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization 1: Average L2 Distance vs Branching Epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute average L2 distance for each branching epoch and analysis epoch\n",
    "avg_distances = df_distances.groupby(['branch_epoch', 'analysis_epoch'])['l2_distance'].mean().reset_index()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "for analysis_epoch in ANALYSIS_EPOCHS:\n",
    "    data = avg_distances[avg_distances['analysis_epoch'] == analysis_epoch]\n",
    "    plt.plot(data['branch_epoch'], data['l2_distance'], \n",
    "             marker='o', linewidth=2, markersize=8, \n",
    "             label=f'After {analysis_epoch} epochs')\n",
    "\n",
    "plt.xlabel('Branching Epoch', fontsize=12)\n",
    "plt.ylabel('Average L2 Distance Between Variants', fontsize=12)\n",
    "plt.title('Model Distance vs Branching Point', fontsize=14, fontweight='bold')\n",
    "plt.legend(fontsize=10)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(RESULTS_DIR, \"distance_vs_branching.png\"), dpi=150)\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úÖ Plot saved: distance_vs_branching.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization 2: Soup Performance vs Branching Epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Plot 1: Soup accuracy vs branching epoch\n",
    "for analysis_epoch in ANALYSIS_EPOCHS:\n",
    "    data = df_soups[df_soups['analysis_epoch'] == analysis_epoch]\n",
    "    axes[0].plot(data['branch_epoch'], 100*data['soup_test_acc'], \n",
    "                marker='o', linewidth=2, markersize=8, \n",
    "                label=f'Soup (after {analysis_epoch} epochs)')\n",
    "    axes[0].plot(data['branch_epoch'], 100*data['best_individual_acc'], \n",
    "                marker='s', linewidth=2, markersize=6, linestyle='--', alpha=0.7,\n",
    "                label=f'Best individual (after {analysis_epoch} epochs)')\n",
    "\n",
    "axes[0].set_xlabel('Branching Epoch', fontsize=12)\n",
    "axes[0].set_ylabel('Test Accuracy (%)', fontsize=12)\n",
    "axes[0].set_title('Soup vs Best Individual Accuracy', fontsize=14, fontweight='bold')\n",
    "axes[0].legend(fontsize=9)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Soup improvement over mean\n",
    "for analysis_epoch in ANALYSIS_EPOCHS:\n",
    "    data = df_soups[df_soups['analysis_epoch'] == analysis_epoch]\n",
    "    axes[1].plot(data['branch_epoch'], 100*data['soup_improvement_over_mean'], \n",
    "                marker='o', linewidth=2, markersize=8, \n",
    "                label=f'After {analysis_epoch} epochs')\n",
    "\n",
    "axes[1].axhline(y=0, color='black', linestyle='-', linewidth=1, alpha=0.5)\n",
    "axes[1].set_xlabel('Branching Epoch', fontsize=12)\n",
    "axes[1].set_ylabel('Soup Improvement Over Mean (%)', fontsize=12)\n",
    "axes[1].set_title('Soup Improvement vs Branching Point', fontsize=14, fontweight='bold')\n",
    "axes[1].legend(fontsize=10)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(RESULTS_DIR, \"soup_performance_vs_branching.png\"), dpi=150)\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úÖ Plot saved: soup_performance_vs_branching.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization 3: Soup Improvement vs L2 Distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge soup results with average distances\n",
    "df_combined = df_soups.merge(\n",
    "    avg_distances,\n",
    "    on=['branch_epoch', 'analysis_epoch'],\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Plot 1: Soup accuracy vs L2 distance\n",
    "for analysis_epoch in ANALYSIS_EPOCHS:\n",
    "    data = df_combined[df_combined['analysis_epoch'] == analysis_epoch]\n",
    "    axes[0].scatter(data['l2_distance'], 100*data['soup_test_acc'], \n",
    "                   s=100, alpha=0.7, label=f'After {analysis_epoch} epochs')\n",
    "\n",
    "axes[0].set_xlabel('Average L2 Distance', fontsize=12)\n",
    "axes[0].set_ylabel('Soup Test Accuracy (%)', fontsize=12)\n",
    "axes[0].set_title('Soup Accuracy vs Model Distance', fontsize=14, fontweight='bold')\n",
    "axes[0].legend(fontsize=10)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Soup improvement vs L2 distance\n",
    "for analysis_epoch in ANALYSIS_EPOCHS:\n",
    "    data = df_combined[df_combined['analysis_epoch'] == analysis_epoch]\n",
    "    axes[1].scatter(data['l2_distance'], 100*data['soup_improvement_over_mean'], \n",
    "                   s=100, alpha=0.7, label=f'After {analysis_epoch} epochs')\n",
    "\n",
    "axes[1].axhline(y=0, color='black', linestyle='-', linewidth=1, alpha=0.5)\n",
    "axes[1].set_xlabel('Average L2 Distance', fontsize=12)\n",
    "axes[1].set_ylabel('Soup Improvement Over Mean (%)', fontsize=12)\n",
    "axes[1].set_title('Soup Improvement vs Model Distance', fontsize=14, fontweight='bold')\n",
    "axes[1].legend(fontsize=10)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(RESULTS_DIR, \"soup_vs_distance.png\"), dpi=150)\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úÖ Plot saved: soup_vs_distance.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SOUP ANALYSIS SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\nSoup Performance by Branching Epoch:\")\n",
    "print(\"-\" * 80)\n",
    "summary = df_soups.groupby('branch_epoch').agg({\n",
    "    'soup_test_acc': 'mean',\n",
    "    'mean_individual_acc': 'mean',\n",
    "    'soup_improvement_over_mean': 'mean'\n",
    "})\n",
    "summary['soup_test_acc'] = summary['soup_test_acc'] * 100\n",
    "summary['mean_individual_acc'] = summary['mean_individual_acc'] * 100\n",
    "summary['soup_improvement_over_mean'] = summary['soup_improvement_over_mean'] * 100\n",
    "summary.columns = ['Soup Acc (%)', 'Mean Individual (%)', 'Improvement (%)']\n",
    "print(summary.to_string())\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"HYPOTHESIS TEST: Basin Transition\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "early_branch = df_soups[df_soups['branch_epoch'].isin([0, 2])]['soup_improvement_over_mean'].mean()\n",
    "late_branch = df_soups[df_soups['branch_epoch'].isin([5, 8])]['soup_improvement_over_mean'].mean()\n",
    "\n",
    "print(f\"\\nEarly Branching (epochs 0, 2):\")\n",
    "print(f\"  Average soup improvement: {100*early_branch:.3f}%\")\n",
    "print(f\"\\nLate Branching (epochs 5, 8):\")\n",
    "print(f\"  Average soup improvement: {100*late_branch:.3f}%\")\n",
    "print(f\"\\nDifference: {100*(late_branch - early_branch):.3f}%\")\n",
    "\n",
    "if late_branch > early_branch:\n",
    "    print(\"\\n‚úÖ Hypothesis SUPPORTED: Late branching shows better soup performance\")\n",
    "else:\n",
    "    print(\"\\n‚ùå Hypothesis NOT supported: Early branching shows equal or better performance\")\n",
    "\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis Complete!\n",
    "\n",
    "All results have been saved to the `results/` directory:\n",
    "- `individual_models.csv` - Individual model performance\n",
    "- `pairwise_distances.csv` - L2 distances between model pairs\n",
    "- `soup_results.csv` - Soup performance metrics\n",
    "- Visualization plots (PNG files)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
