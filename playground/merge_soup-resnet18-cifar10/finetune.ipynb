{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ResNet18 Finetuning on CIFAR-10\n",
    "\n",
    "This notebook:\n",
    "- Takes a ResNet18 network pretrained on ImageNet as base point, then finetune on CIFAR-10\n",
    "- Uses different finetuning hyperparameters to obtain different model checkpoints\n",
    "- Follows heDeepResidualLearning2016 training configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOCAL = True\n",
    "\n",
    "# if run locally:\n",
    "if LOCAL:\n",
    "    ROOT_DIR = \"/Users/Yang/Desktop/research-model-merge/playground/merge_soup-resnet18-cifar10\"\n",
    "    DATA_DIR = \"/Users/Yang/Desktop/research-model-merge/datasets\"\n",
    "    PROJECT_ROOT = \"/Users/Yang/Desktop/research-model-merge\"\n",
    "else:\n",
    "    # on Colab\n",
    "    ROOT_DIR = \"/content\"\n",
    "    DATA_DIR = \"/content/datasets\"\n",
    "    PROJECT_ROOT = \"/content\"\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    DRIVE_DIR = \"drive/MyDrive/research-model_merge\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "from typing import Dict, Any\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.models import resnet18, ResNet18_Weights\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Add project root to path\n",
    "if PROJECT_ROOT not in sys.path:\n",
    "    sys.path.insert(0, PROJECT_ROOT)\n",
    "\n",
    "# Add utils to path\n",
    "if ROOT_DIR not in sys.path:\n",
    "    sys.path.insert(0, ROOT_DIR)\n",
    "\n",
    "from datasets.cifar10 import CIFAR10\n",
    "\n",
    "from datasets.cifar10 import CIFAR10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç System Information:\n",
      "Python version: Python 3.11.5\n",
      "PyTorch version: 2.1.2\n",
      "CUDA available: False\n",
      "‚ö†Ô∏è No GPU available! Training will be slow on CPU.\n"
     ]
    }
   ],
   "source": [
    "# Check GPU availability and system info\n",
    "import subprocess\n",
    "\n",
    "print(\"üîç System Information:\")\n",
    "print(f\"Python version: {subprocess.check_output(['python', '--version']).decode().strip()}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU device: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "    print(f\"CUDA version: {torch.version.cuda}\")\n",
    "    DEVICE = torch.device(\"cuda\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No GPU available! Training will be slow on CPU.\")\n",
    "    DEVICE = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Preparation\n",
    "\n",
    "Using the shared CIFAR10 dataset class from `datasets/cifar10.py`:\n",
    "- Training: 98% of original training set (49,000 images)\n",
    "- Validation: 2% of original training set (1,000 images)  \n",
    "- Test: Official CIFAR-10 test set (10,000 images)\n",
    "- Persistent indices ensure consistent splits across all experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "‚úÖ Dataset loaded:\n",
      "   Train samples: 49000\n",
      "   Val samples: 1000\n",
      "   Test samples: 10000\n",
      "   Classnames: ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n"
     ]
    }
   ],
   "source": [
    "# Create CIFAR-10 dataset using shared dataset class\n",
    "# This uses persistent indices for reproducible splits\n",
    "dataset = CIFAR10(\n",
    "    data_location=DATA_DIR,\n",
    "    batch_size=256,\n",
    "    num_workers=2\n",
    ")\n",
    "\n",
    "train_loader = dataset.train_loader\n",
    "val_loader = dataset.val_loader\n",
    "test_loader = dataset.test_loader\n",
    "\n",
    "print(f\"‚úÖ Dataset loaded:\")\n",
    "print(f\"   Train samples: {len(dataset.train_sampler)}\")\n",
    "print(f\"   Val samples: {len(dataset.val_sampler)}\")\n",
    "print(f\"   Test samples: {len(dataset.test_dataset)}\")\n",
    "print(f\"   Classnames: {dataset.classnames}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finetuning Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_lr_schedule(optimizer, epoch, total_epochs, warmup_epochs, base_lr):\n",
    "    \"\"\"\n",
    "    Cosine learning rate schedule with linear warmup.\n",
    "    Following Git Re-Basin configuration.\n",
    "    \"\"\"\n",
    "    if epoch < warmup_epochs:\n",
    "        # Linear warmup from 1e-6 to base_lr\n",
    "        lr = 1e-6 + (base_lr - 1e-6) * epoch / warmup_epochs\n",
    "    else:\n",
    "        # Cosine decay from base_lr to 0\n",
    "        progress = (epoch - warmup_epochs) / (total_epochs - warmup_epochs)\n",
    "        lr = base_lr * 0.5 * (1 + np.cos(np.pi * progress))\n",
    "    \n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "    \n",
    "    return lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step_lr_schedule(optimizer, epoch, total_epochs, warmup_epochs, base_lr):\n",
    "    \"\"\"\n",
    "    Following heDeepResidualLearning2016. \n",
    "    \"\"\"\n",
    "    if epoch < warmup_epochs:\n",
    "        lr = base_lr\n",
    "    else:\n",
    "        lr = base_lr * 0.1\n",
    "    \n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "    \n",
    "    return lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def finetune_resnet(\n",
    "    train_loader: DataLoader,\n",
    "    val_loader: DataLoader,\n",
    "    model_save_location: str = '.',\n",
    "    batch_size: int = 256,\n",
    "    epochs: int = 10,\n",
    "    warmup_epochs: int = 5,\n",
    "    lr: float = 0.1,\n",
    "    wd: float = 1e-4,\n",
    "    momentum: float = 0.9,\n",
    "    name: str = 'config1',\n",
    "    log_interval: int = 20,\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Finetune ResNet18 (pretrained on ImageNet) on CIFAR-10.\n",
    "    \n",
    "    Following He 2016 training configuration:\n",
    "    - SGD optimizer with momentum=0.9\n",
    "    - Weight decay (default 1e-4)\n",
    "    - Step LR schedule with 5-epoch warmup\n",
    "    - Warmup: 1e-6 -> lr over 5 epochs\n",
    "    - Step decay: lr -> 0.1*lr after warmup\n",
    "    \"\"\"\n",
    "    os.makedirs(model_save_location, exist_ok=True)\n",
    "    \n",
    "    # Load pretrained ResNet18 and modify for CIFAR-10\n",
    "    model = resnet18(weights=ResNet18_Weights.IMAGENET1K_V1)\n",
    "    \n",
    "    # Modify first conv layer for 32x32 input (CIFAR-10)\n",
    "    model.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "    \n",
    "    # Remove maxpool layer (too aggressive for 32x32 images)\n",
    "    model.maxpool = nn.Identity()\n",
    "    \n",
    "    # Replace final FC layer for CIFAR-10 (10 classes)\n",
    "    num_features = model.fc.in_features\n",
    "    model.fc = nn.Linear(num_features, 10)\n",
    "    \n",
    "    model = model.to(DEVICE)\n",
    "    \n",
    "    # Optimizer: SGD with momentum\n",
    "    optimizer = optim.SGD(\n",
    "        model.parameters(), \n",
    "        lr=lr,\n",
    "        momentum=momentum,\n",
    "        weight_decay=wd\n",
    "    )\n",
    "    \n",
    "    # Loss function\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # Training history\n",
    "    history = {\n",
    "        'train_loss': [],\n",
    "        'val_loss': [],\n",
    "        'val_acc': [],\n",
    "        'lr': []\n",
    "    }\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Starting training: {name}\")\n",
    "    print(f\"Config: lr={lr}, wd={wd}, epochs={epochs}, batch_size={batch_size}\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(epochs):\n",
    "        # Update learning rate\n",
    "        current_lr = step_lr_schedule(optimizer, epoch, epochs, warmup_epochs, lr)\n",
    "        history['lr'].append(current_lr)\n",
    "        \n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_loss_accum = 0.0\n",
    "        train_batches = 0\n",
    "        \n",
    "        pbar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{epochs} [Train]')\n",
    "        for i, (inputs, labels) in enumerate(pbar):\n",
    "            inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss_accum += loss.item()\n",
    "            train_batches += 1\n",
    "            \n",
    "            if i % log_interval == 0:\n",
    "                pbar.set_postfix({\n",
    "                    'loss': f'{loss.item():.4f}',\n",
    "                    'lr': f'{current_lr:.6f}'\n",
    "                })\n",
    "        \n",
    "        train_loss = train_loss_accum / train_batches\n",
    "        history['train_loss'].append(train_loss)\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss_accum = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            pbar = tqdm(val_loader, desc=f'Epoch {epoch+1}/{epochs} [Val]')\n",
    "            for inputs, labels in pbar:\n",
    "                inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                \n",
    "                val_loss_accum += loss.item()\n",
    "                _, predicted = outputs.max(1)\n",
    "                total += labels.size(0)\n",
    "                correct += predicted.eq(labels).sum().item()\n",
    "                \n",
    "                pbar.set_postfix({\n",
    "                    'loss': f'{loss.item():.4f}',\n",
    "                    'acc': f'{100.*correct/total:.2f}%'\n",
    "                })\n",
    "        \n",
    "        val_loss = val_loss_accum / len(val_loader)\n",
    "        val_acc = correct / total\n",
    "        \n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['val_acc'].append(val_acc)\n",
    "        \n",
    "        print(f\"\\nEpoch {epoch+1}/{epochs} Summary:\")\n",
    "        print(f\"  Train Loss: {train_loss:.4f}\")\n",
    "        print(f\"  Val Loss:   {val_loss:.4f}\")\n",
    "        print(f\"  Val Acc:    {100*val_acc:.2f}%\")\n",
    "        print(f\"  LR:         {current_lr:.6f}\\n\")\n",
    "        \n",
    "        # Save checkpoint after each epoch\n",
    "        checkpoint_path = os.path.join(model_save_location, f'{name}_epoch{epoch+1}.pt')\n",
    "        torch.save(model.state_dict(), checkpoint_path)\n",
    "        print(f\"‚úÖ Saved checkpoint: {checkpoint_path}\")\n",
    "    \n",
    "    result = {\n",
    "        'history': history,\n",
    "        'config': {\n",
    "            'model_save_location': model_save_location,\n",
    "            'batch_size': batch_size,\n",
    "            'epochs': epochs,\n",
    "            'warmup_epochs': warmup_epochs,\n",
    "            'lr': lr,\n",
    "            'wd': wd,\n",
    "            'momentum': momentum,\n",
    "            'name': name,\n",
    "        },\n",
    "    }\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Training with Multiple Configurations\n",
    "\n",
    "We train 5 different configurations with varying learning rates and weight decay values:\n",
    "\n",
    "1. **Config 1**: lr=0.1, wd=1e-4 (He 2016 baseline)\n",
    "2. **Config 2**: lr=0.05, wd=1e-4\n",
    "3. **Config 3**: lr=0.01, wd=1e-4\n",
    "4. **Config 4**: lr=0.1, wd=1e-3\n",
    "5. **Config 5**: lr=0.1, wd=1e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checkpoint directory\n",
    "if LOCAL:\n",
    "    checkpoint_dir = f\"{ROOT_DIR}/checkpoints\"\n",
    "else:\n",
    "    checkpoint_dir = f\"{DRIVE_DIR}/checkpoints\"\n",
    "\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "\n",
    "# Define configurations\n",
    "configs = [\n",
    "    dict(lr=0.1, wd=1e-4, name='config1'),\n",
    "    dict(lr=0.05, wd=1e-4, name='config2'),\n",
    "    dict(lr=0.01, wd=1e-4, name='config3'),\n",
    "    dict(lr=0.1, wd=1e-3, name='config4'),\n",
    "    dict(lr=0.1, wd=1e-5, name='config5'),\n",
    "]\n",
    "\n",
    "# Common parameters\n",
    "common = dict(\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    model_save_location=checkpoint_dir,\n",
    "    batch_size=258,\n",
    "    epochs=10,\n",
    "    warmup_epochs=5,\n",
    "    momentum=0.9,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "################################################################################\n",
      "Running configuration: config1\n",
      "  LR: 0.1, WD: 0.0001\n",
      "################################################################################\n",
      "\n",
      "\n",
      "================================================================================\n",
      "Starting training: config1\n",
      "Config: lr=0.1, wd=0.0001, epochs=10, batch_size=258\n",
      "================================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 [Train]:   2%|‚ñè         | 3/192 [00:21<21:21,  6.78s/it, loss=2.5593, lr=0.100000]"
     ]
    }
   ],
   "source": [
    "# Run all configurations\n",
    "results = []\n",
    "\n",
    "for config in configs:\n",
    "    run_config = {**common, **config}\n",
    "    print(f\"\\n{'#'*80}\")\n",
    "    print(f\"Running configuration: {config['name']}\")\n",
    "    print(f\"  LR: {config['lr']}, WD: {config['wd']}\")\n",
    "    print(f\"{'#'*80}\\n\")\n",
    "    \n",
    "    result = finetune_resnet(**run_config)\n",
    "    results.append(result)\n",
    "    \n",
    "    print(f\"\\n‚úÖ {config['name']} completed!\\n\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"All configurations completed!\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary of Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create summary table\n",
    "summary = []\n",
    "for r in results:\n",
    "    cfg = r['config']\n",
    "    hist = r['history']\n",
    "    summary.append({\n",
    "        'name': cfg['name'],\n",
    "        'lr': cfg['lr'],\n",
    "        'wd': cfg['wd'],\n",
    "        'final_train_loss': hist['train_loss'][-1],\n",
    "        'final_val_loss': hist['val_loss'][-1],\n",
    "        'final_val_acc': f\"{100*hist['val_acc'][-1]:.2f}%\",\n",
    "        'best_val_acc': f\"{100*max(hist['val_acc']):.2f}%\",\n",
    "    })\n",
    "\n",
    "df = pd.DataFrame(summary)\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Training Summary\")\n",
    "print(\"=\"*80)\n",
    "print(df.to_string(index=False))\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Save summary\n",
    "df.to_csv(f\"{checkpoint_dir}/training_summary.csv\", index=False)\n",
    "print(f\"\\n‚úÖ Summary saved to {checkpoint_dir}/training_summary.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Training Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Plot 1: Training Loss\n",
    "for r in results:\n",
    "    axes[0, 0].plot(r['history']['train_loss'], label=r['config']['name'])\n",
    "axes[0, 0].set_xlabel('Epoch')\n",
    "axes[0, 0].set_ylabel('Train Loss')\n",
    "axes[0, 0].set_title('Training Loss')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True)\n",
    "\n",
    "# Plot 2: Validation Loss\n",
    "for r in results:\n",
    "    axes[0, 1].plot(r['history']['val_loss'], label=r['config']['name'])\n",
    "axes[0, 1].set_xlabel('Epoch')\n",
    "axes[0, 1].set_ylabel('Validation Loss')\n",
    "axes[0, 1].set_title('Validation Loss')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True)\n",
    "\n",
    "# Plot 3: Validation Accuracy\n",
    "for r in results:\n",
    "    axes[1, 0].plot([100*x for x in r['history']['val_acc']], label=r['config']['name'])\n",
    "axes[1, 0].set_xlabel('Epoch')\n",
    "axes[1, 0].set_ylabel('Validation Accuracy (%)')\n",
    "axes[1, 0].set_title('Validation Accuracy')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True)\n",
    "\n",
    "# Plot 4: Learning Rate\n",
    "for r in results:\n",
    "    axes[1, 1].plot(r['history']['lr'], label=r['config']['name'])\n",
    "axes[1, 1].set_xlabel('Epoch')\n",
    "axes[1, 1].set_ylabel('Learning Rate')\n",
    "axes[1, 1].set_title('Learning Rate Schedule')\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(True)\n",
    "axes[1, 1].set_yscale('log')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{checkpoint_dir}/training_curves.png\", dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"‚úÖ Training curves saved to {checkpoint_dir}/training_curves.png\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
