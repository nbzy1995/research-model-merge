{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Branched Finetuning\n",
    "\n",
    "This notebook branches from the main trajectory at specific epochs and continues training with different learning rates.\n",
    "\n",
    "**Experiment Design:**\n",
    "- Load checkpoint from main trajectory at epoch `i`\n",
    "- Train 5 variants with different learning rates\n",
    "- Each variant trains for 10 epochs\n",
    "- Save checkpoints at every epoch\n",
    "\n",
    "**Outputs:**\n",
    "- `checkpoints/branch_e{i}_lr{j}_e{k}.pt` where:\n",
    "  - `i` = branching epoch from main trajectory\n",
    "  - `j` = learning rate variant (1-5)\n",
    "  - `k` = epochs trained after branching (1-10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOCAL = True\n",
    "\n",
    "if LOCAL:\n",
    "    ROOT_DIR = \"/Users/Yang/Desktop/research-model-merge/playground/merge_soup-resnet18-cifar10\"\n",
    "    DATA_DIR = \"/Users/Yang/Desktop/research-model-merge/datasets\"\n",
    "    PROJECT_ROOT = \"/Users/Yang/Desktop/research-model-merge\"\n",
    "else:\n",
    "    # on Colab\n",
    "    ROOT_DIR = \"/content/research-model-merge/playground/merge_soup-resnet18-cifar10\"\n",
    "    DATA_DIR = \"/content/research-model-merge/datasets\"\n",
    "    PROJECT_ROOT = \"/content/research-model-merge\"\n",
    "    DRIVE_DIR = \"/content/drive/MyDrive/research-model_merge-shared/merge_soup-resnet18-cifar10\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mount Google Drive (Colab only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not LOCAL:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clone Repository and Install Dependencies (Colab only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not LOCAL:\n",
    "    !rm -rf research-model-merge\n",
    "    !git clone https://github.com/nbzy1995/research-model-merge.git /content/research-model-merge\n",
    "    !pip install --quiet --upgrade pip\n",
    "    !pip install -q -r research-model-merge/requirements.txt\n",
    "    print(\"âœ… Repository cloned and dependencies installed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import subprocess\n",
    "import json\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Add project directories to path\n",
    "if PROJECT_ROOT not in sys.path:\n",
    "    sys.path.insert(0, PROJECT_ROOT)\n",
    "if ROOT_DIR not in sys.path:\n",
    "    sys.path.insert(0, ROOT_DIR)\n",
    "\n",
    "from datasets.cifar10 import CIFAR10\n",
    "from utils import create_cifar10_resnet18, train_model, load_checkpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check Device Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ðŸ” System Information:\")\n",
    "print(f\"Python version: {subprocess.check_output(['python', '--version']).decode().strip()}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU device: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "    print(f\"CUDA version: {torch.version.cuda}\")\n",
    "    DEVICE = torch.device(\"cuda\")\n",
    "else:\n",
    "    if LOCAL:\n",
    "        print(\"âš ï¸ No GPU available! Training will be slow on CPU.\")\n",
    "    else:\n",
    "        print(\"âŒ No GPU available! Please enable GPU runtime in Colab.\")\n",
    "        print(\"Runtime > Change runtime type > Hardware accelerator > GPU\")\n",
    "    DEVICE = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = CIFAR10(\n",
    "    data_location=DATA_DIR,\n",
    "    batch_size=256,\n",
    "    num_workers=2\n",
    ")\n",
    "\n",
    "train_loader = dataset.train_loader\n",
    "val_loader = dataset.val_loader\n",
    "test_loader = dataset.test_loader\n",
    "\n",
    "print(f\"âœ… Dataset loaded:\")\n",
    "print(f\"   Train samples: {len(dataset.train_sampler)}\")\n",
    "print(f\"   Val samples: {len(dataset.val_sampler)}\")\n",
    "print(f\"   Test samples: {len(dataset.test_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Branching Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Branching points (which epochs from main trajectory to branch from)\n",
    "BRANCHING_EPOCHS = [0, 2, 5, 8]\n",
    "\n",
    "# Learning rate variants for branched training\n",
    "LR_VARIANTS = [\n",
    "    {'id': 1, 'lr': 0.1,   'name': 'lr1_0.1'},\n",
    "    {'id': 2, 'lr': 0.05,  'name': 'lr2_0.05'},\n",
    "    {'id': 3, 'lr': 0.01,  'name': 'lr3_0.01'},\n",
    "    {'id': 4, 'lr': 0.005, 'name': 'lr4_0.005'},\n",
    "    {'id': 5, 'lr': 0.001, 'name': 'lr5_0.001'},\n",
    "]\n",
    "\n",
    "# Training hyperparameters\n",
    "BRANCH_EPOCHS = 10  # How many epochs to train after branching\n",
    "WEIGHT_DECAY = 1e-4\n",
    "MOMENTUM = 0.9\n",
    "\n",
    "# Checkpoint configuration\n",
    "if LOCAL:\n",
    "    CHECKPOINT_DIR = f\"{ROOT_DIR}/checkpoints\"\n",
    "else:\n",
    "    CHECKPOINT_DIR = f\"{DRIVE_DIR}/checkpoints\"\n",
    "\n",
    "print(f\"Branching Configuration:\")\n",
    "print(f\"  Branching Epochs: {BRANCHING_EPOCHS}\")\n",
    "print(f\"  LR Variants: {len(LR_VARIANTS)}\")\n",
    "print(f\"  Epochs per Branch: {BRANCH_EPOCHS}\")\n",
    "print(f\"  Total Models to Train: {len(BRANCHING_EPOCHS) * len(LR_VARIANTS)}\")\n",
    "print(f\"  Checkpoint Dir: {CHECKPOINT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Branched Variants\n",
    "\n",
    "For each branching point, we load the checkpoint from the main trajectory and train 5 variants with different learning rates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store training results\n",
    "all_results = []\n",
    "\n",
    "for branch_epoch in BRANCHING_EPOCHS:\n",
    "    print(f\"\\n{'#'*80}\")\n",
    "    print(f\"BRANCHING FROM EPOCH {branch_epoch}\")\n",
    "    print(f\"{'#'*80}\\n\")\n",
    "    \n",
    "    # Load checkpoint from main trajectory\n",
    "    if branch_epoch == 0:\n",
    "        checkpoint_path = os.path.join(CHECKPOINT_DIR, \"base_model.pt\")\n",
    "    else:\n",
    "        checkpoint_path = os.path.join(CHECKPOINT_DIR, f\"main_epoch{branch_epoch}.pt\")\n",
    "    \n",
    "    print(f\"Loading checkpoint: {checkpoint_path}\")\n",
    "    \n",
    "    if not os.path.exists(checkpoint_path):\n",
    "        print(f\"âŒ Checkpoint not found: {checkpoint_path}\")\n",
    "        print(f\"   Please run train_main_trajectory.ipynb first!\")\n",
    "        continue\n",
    "    \n",
    "    for lr_config in LR_VARIANTS:\n",
    "        lr_id = lr_config['id']\n",
    "        lr = lr_config['lr']\n",
    "        lr_name = lr_config['name']\n",
    "        \n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"Training Branch: epoch{branch_epoch} -> {lr_name}\")\n",
    "        print(f\"{'='*80}\\n\")\n",
    "        \n",
    "        # Create new model and load checkpoint\n",
    "        model = create_cifar10_resnet18(num_classes=10)\n",
    "        state_dict = load_checkpoint(checkpoint_path, device=DEVICE)\n",
    "        model.load_state_dict(state_dict)\n",
    "        model = model.to(DEVICE)\n",
    "        \n",
    "        # Setup optimizer and scheduler for this variant\n",
    "        optimizer = optim.SGD(\n",
    "            model.parameters(),\n",
    "            lr=lr,\n",
    "            momentum=MOMENTUM,\n",
    "            weight_decay=WEIGHT_DECAY\n",
    "        )\n",
    "        \n",
    "        # Constant learning rate (no scheduler)\n",
    "        lr_scheduler = None\n",
    "        \n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        \n",
    "        # Train\n",
    "        checkpoint_template = f\"branch_e{branch_epoch}_lr{lr_id}_e{{epoch}}.pt\"\n",
    "        \n",
    "        history = train_model(\n",
    "            model=model,\n",
    "            train_loader=train_loader,\n",
    "            val_loader=val_loader,\n",
    "            optimizer=optimizer,\n",
    "            lr_scheduler=lr_scheduler,\n",
    "            criterion=criterion,\n",
    "            epochs=BRANCH_EPOCHS,\n",
    "            device=DEVICE,\n",
    "            checkpoint_dir=CHECKPOINT_DIR,\n",
    "            checkpoint_name_template=checkpoint_template,\n",
    "            log_interval=20,\n",
    "            save_epoch_0=False\n",
    "        )\n",
    "        \n",
    "        # Store results\n",
    "        result = {\n",
    "            'branch_epoch': branch_epoch,\n",
    "            'lr_id': lr_id,\n",
    "            'lr': lr,\n",
    "            'lr_name': lr_name,\n",
    "            'history': history,\n",
    "            'final_val_acc': history['val_acc'][-1],\n",
    "            'best_val_acc': max(history['val_acc'])\n",
    "        }\n",
    "        all_results.append(result)\n",
    "        \n",
    "        print(f\"\\nâœ… Completed: epoch{branch_epoch} -> {lr_name}\")\n",
    "        print(f\"   Final Val Acc: {100*result['final_val_acc']:.2f}%\")\n",
    "        print(f\"   Best Val Acc: {100*result['best_val_acc']:.2f}%\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"ALL BRANCHED TRAINING COMPLETED!\")\n",
    "print(f\"{'='*80}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Training Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create summary DataFrame\n",
    "summary_data = []\n",
    "for r in all_results:\n",
    "    summary_data.append({\n",
    "        'branch_epoch': r['branch_epoch'],\n",
    "        'lr_id': r['lr_id'],\n",
    "        'lr': r['lr'],\n",
    "        'lr_name': r['lr_name'],\n",
    "        'final_val_acc': f\"{100*r['final_val_acc']:.2f}%\",\n",
    "        'best_val_acc': f\"{100*r['best_val_acc']:.2f}%\"\n",
    "    })\n",
    "\n",
    "df_summary = pd.DataFrame(summary_data)\n",
    "\n",
    "# Save to CSV\n",
    "summary_path = os.path.join(CHECKPOINT_DIR, \"branched_training_summary.csv\")\n",
    "df_summary.to_csv(summary_path, index=False)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Branched Training Summary\")\n",
    "print(\"=\"*80)\n",
    "print(df_summary.to_string(index=False))\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nâœ… Summary saved to {summary_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Training Curves by Branching Epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Create a plot for each branching epoch\nfor branch_epoch in BRANCHING_EPOCHS:\n    # Filter results for this branching epoch\n    branch_results = [r for r in all_results if r['branch_epoch'] == branch_epoch]\n    \n    if not branch_results:\n        continue\n    \n    fig, axes = plt.subplots(1, 2, figsize=(16, 5))\n    \n    epochs_range = range(1, BRANCH_EPOCHS + 1)\n    \n    # Plot 1: Training and Validation Loss (combined)\n    for r in branch_results:\n        # Plot train loss with solid line\n        axes[0].plot(epochs_range, r['history']['train_loss'], \n                    linestyle='-', marker='o', alpha=0.7, label=f\"{r['lr_name']} (train)\")\n        # Plot val loss with dashed line\n        axes[0].plot(epochs_range, r['history']['val_loss'], \n                    linestyle='--', marker='s', alpha=0.7, label=f\"{r['lr_name']} (val)\")\n    axes[0].set_xlabel('Epoch')\n    axes[0].set_ylabel('Loss')\n    axes[0].set_title(f'Training and Validation Loss (Branched from Epoch {branch_epoch})')\n    axes[0].legend(fontsize=8, ncol=2)\n    axes[0].grid(True)\n    \n    # Plot 2: Validation Accuracy\n    for r in branch_results:\n        axes[1].plot(epochs_range, [100*x for x in r['history']['val_acc']], \n                    marker='o', label=r['lr_name'])\n    axes[1].set_xlabel('Epoch')\n    axes[1].set_ylabel('Validation Accuracy (%)')\n    axes[1].set_title(f'Validation Accuracy (Branched from Epoch {branch_epoch})')\n    axes[1].legend()\n    axes[1].grid(True)\n    \n    plt.tight_layout()\n    plot_path = os.path.join(CHECKPOINT_DIR, f\"branch_e{branch_epoch}_curves.png\")\n    plt.savefig(plot_path, dpi=150, bbox_inches='tight')\n    plt.show()\n    \n    print(f\"âœ… Plot saved: {plot_path}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Branched Finetuning Experiment Complete!\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nTotal models trained: {len(all_results)}\")\n",
    "print(f\"Branching epochs: {BRANCHING_EPOCHS}\")\n",
    "print(f\"LR variants per branch: {len(LR_VARIANTS)}\")\n",
    "print(f\"\\nCheckpoints saved in: {CHECKPOINT_DIR}\")\n",
    "print(f\"\\nCheckpoint naming format: branch_e{{branch_epoch}}_lr{{lr_id}}_e{{epoch}}.pt\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nâœ… Ready for soup analysis!\")\n",
    "print(f\"   Next: Run analyze_branching_soups.ipynb\")\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}